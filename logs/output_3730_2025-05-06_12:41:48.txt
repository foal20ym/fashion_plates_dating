Using model: InceptionV3.
TensorFlow Version: 2.19.0
Num GPUs Available: 1
Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')
Test fold: 0
Epoch 1/100
707/707 - 183s - 259ms/step - accuracy: 0.3818 - loss: 4.9544 - val_accuracy: 0.2444 - val_loss: 7.5702 - learning_rate: 0.0100
Epoch 2/100
707/707 - 130s - 183ms/step - accuracy: 0.4762 - loss: 4.4172 - val_accuracy: 0.3201 - val_loss: 6.0766 - learning_rate: 0.0100
Epoch 3/100
707/707 - 131s - 185ms/step - accuracy: 0.5046 - loss: 4.3741 - val_accuracy: 0.3893 - val_loss: 5.7267 - learning_rate: 0.0100
Epoch 4/100
707/707 - 123s - 174ms/step - accuracy: 0.5377 - loss: 4.3212 - val_accuracy: 0.3901 - val_loss: 6.5979 - learning_rate: 0.0100
Epoch 5/100
707/707 - 121s - 171ms/step - accuracy: 0.5468 - loss: 4.3599 - val_accuracy: 0.4610 - val_loss: 5.1227 - learning_rate: 0.0100
Epoch 6/100
707/707 - 120s - 170ms/step - accuracy: 0.5673 - loss: 4.1895 - val_accuracy: 0.4140 - val_loss: 6.5039 - learning_rate: 0.0100
Epoch 7/100
707/707 - 127s - 179ms/step - accuracy: 0.5831 - loss: 4.1786 - val_accuracy: 0.3551 - val_loss: 9.0071 - learning_rate: 0.0100
Epoch 8/100

Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.
707/707 - 132s - 187ms/step - accuracy: 0.5817 - loss: 4.0120 - val_accuracy: 0.4403 - val_loss: 6.0140 - learning_rate: 0.0100
Epoch 9/100
707/707 - 137s - 193ms/step - accuracy: 0.7103 - loss: 1.7907 - val_accuracy: 0.5510 - val_loss: 3.6800 - learning_rate: 1.0000e-03
Epoch 10/100
707/707 - 130s - 184ms/step - accuracy: 0.7148 - loss: 1.6103 - val_accuracy: 0.5255 - val_loss: 3.5002 - learning_rate: 1.0000e-03
Epoch 11/100
707/707 - 125s - 176ms/step - accuracy: 0.7288 - loss: 1.4909 - val_accuracy: 0.5406 - val_loss: 3.4185 - learning_rate: 1.0000e-03
Epoch 12/100
707/707 - 123s - 174ms/step - accuracy: 0.7264 - loss: 1.4486 - val_accuracy: 0.5422 - val_loss: 3.2145 - learning_rate: 1.0000e-03
Epoch 13/100
707/707 - 128s - 181ms/step - accuracy: 0.7336 - loss: 1.3497 - val_accuracy: 0.5565 - val_loss: 3.1537 - learning_rate: 1.0000e-03
Epoch 14/100
707/707 - 136s - 193ms/step - accuracy: 0.7297 - loss: 1.3549 - val_accuracy: 0.5430 - val_loss: 3.0733 - learning_rate: 1.0000e-03
Epoch 15/100
707/707 - 133s - 188ms/step - accuracy: 0.7375 - loss: 1.2717 - val_accuracy: 0.5462 - val_loss: 2.9913 - learning_rate: 1.0000e-03
Epoch 16/100
707/707 - 137s - 194ms/step - accuracy: 0.7358 - loss: 1.2141 - val_accuracy: 0.5271 - val_loss: 2.9825 - learning_rate: 1.0000e-03
Epoch 17/100
707/707 - 130s - 184ms/step - accuracy: 0.7449 - loss: 1.1968 - val_accuracy: 0.5693 - val_loss: 2.8108 - learning_rate: 1.0000e-03
Epoch 18/100
707/707 - 139s - 197ms/step - accuracy: 0.7503 - loss: 1.1006 - val_accuracy: 0.5573 - val_loss: 2.8955 - learning_rate: 1.0000e-03
Epoch 19/100
707/707 - 130s - 184ms/step - accuracy: 0.7532 - loss: 1.0777 - val_accuracy: 0.5502 - val_loss: 2.9904 - learning_rate: 1.0000e-03
Epoch 20/100

Epoch 20: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.
707/707 - 131s - 186ms/step - accuracy: 0.7455 - loss: 1.0810 - val_accuracy: 0.5183 - val_loss: 3.2472 - learning_rate: 1.0000e-03
Epoch 21/100
707/707 - 134s - 189ms/step - accuracy: 0.7655 - loss: 0.9813 - val_accuracy: 0.5581 - val_loss: 2.7474 - learning_rate: 1.0000e-04
Epoch 22/100
707/707 - 134s - 189ms/step - accuracy: 0.7683 - loss: 0.9525 - val_accuracy: 0.5430 - val_loss: 2.8041 - learning_rate: 1.0000e-04
Epoch 23/100
707/707 - 134s - 190ms/step - accuracy: 0.7705 - loss: 0.9232 - val_accuracy: 0.5557 - val_loss: 2.7144 - learning_rate: 1.0000e-04
Epoch 24/100
707/707 - 132s - 187ms/step - accuracy: 0.7724 - loss: 0.9202 - val_accuracy: 0.5557 - val_loss: 2.7350 - learning_rate: 1.0000e-04
Epoch 25/100
707/707 - 133s - 188ms/step - accuracy: 0.7822 - loss: 0.8583 - val_accuracy: 0.5573 - val_loss: 2.7087 - learning_rate: 1.0000e-04
Epoch 26/100
707/707 - 132s - 187ms/step - accuracy: 0.7673 - loss: 0.9384 - val_accuracy: 0.5653 - val_loss: 2.6250 - learning_rate: 1.0000e-04
Epoch 27/100
707/707 - 126s - 178ms/step - accuracy: 0.7747 - loss: 0.9049 - val_accuracy: 0.5565 - val_loss: 2.5944 - learning_rate: 1.0000e-04
Epoch 28/100
707/707 - 132s - 186ms/step - accuracy: 0.7744 - loss: 0.9038 - val_accuracy: 0.5541 - val_loss: 2.6215 - learning_rate: 1.0000e-04
Epoch 29/100
707/707 - 130s - 185ms/step - accuracy: 0.7803 - loss: 0.8759 - val_accuracy: 0.5669 - val_loss: 2.6294 - learning_rate: 1.0000e-04
Epoch 30/100

Epoch 30: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.
707/707 - 134s - 189ms/step - accuracy: 0.7794 - loss: 0.8751 - val_accuracy: 0.5589 - val_loss: 2.6090 - learning_rate: 1.0000e-04
Epoch 31/100
707/707 - 136s - 192ms/step - accuracy: 0.7838 - loss: 0.8611 - val_accuracy: 0.5565 - val_loss: 2.5905 - learning_rate: 1.0000e-05
Epoch 32/100
707/707 - 135s - 191ms/step - accuracy: 0.7825 - loss: 0.8819 - val_accuracy: 0.5637 - val_loss: 2.5908 - learning_rate: 1.0000e-05
Epoch 33/100
707/707 - 135s - 190ms/step - accuracy: 0.7763 - loss: 0.9014 - val_accuracy: 0.5629 - val_loss: 2.5859 - learning_rate: 1.0000e-05
Epoch 34/100
707/707 - 136s - 192ms/step - accuracy: 0.7618 - loss: 0.9365 - val_accuracy: 0.5629 - val_loss: 2.5736 - learning_rate: 1.0000e-05
Epoch 35/100
707/707 - 135s - 191ms/step - accuracy: 0.7715 - loss: 0.8999 - val_accuracy: 0.5661 - val_loss: 2.5749 - learning_rate: 1.0000e-05
Epoch 36/100
707/707 - 128s - 181ms/step - accuracy: 0.7765 - loss: 0.8729 - val_accuracy: 0.5653 - val_loss: 2.5813 - learning_rate: 1.0000e-05
Epoch 37/100

Epoch 37: ReduceLROnPlateau reducing learning rate to 1e-06.
707/707 - 131s - 185ms/step - accuracy: 0.7817 - loss: 0.8388 - val_accuracy: 0.5645 - val_loss: 2.5763 - learning_rate: 1.0000e-05
Epoch 38/100
707/707 - 127s - 179ms/step - accuracy: 0.7712 - loss: 0.8817 - val_accuracy: 0.5653 - val_loss: 2.5767 - learning_rate: 1.0000e-06
Epoch 38: early stopping
Restoring model weights from the end of the best epoch: 34.
Fold 0 Evaluation results: [2.5735487937927246, 0.5628980994224548]
              precision    recall  f1-score   support

        1820       0.59      0.76      0.67        62
        1821       0.96      0.88      0.92        57
        1822       0.00      0.00      0.00         1
        1823       0.50      1.00      0.67         1
        1824       0.00      0.00      0.00         1
        1825       1.00      0.67      0.80         3
        1826       0.00      0.00      0.00         2
        1827       0.65      0.52      0.58        25
        1828       0.00      0.00      0.00         1
        1829       1.00      0.40      0.57         5
        1830       0.50      0.64      0.56        56
        1831       0.87      0.72      0.79       134
        1832       0.75      0.76      0.76        67
        1833       1.00      0.84      0.91        19
        1834       0.51      0.62      0.56        29
        1835       0.00      0.00      0.00         2
        1836       1.00      0.25      0.40         4
        1837       0.60      0.50      0.55         6
        1838       0.33      0.33      0.33         3
        1839       0.00      0.00      0.00         1
        1840       0.51      0.42      0.46        43
        1841       0.54      0.67      0.60       108
        1842       0.00      0.00      0.00         6
        1843       0.20      0.33      0.25         6
        1844       0.00      0.00      0.00         1
        1845       0.00      0.00      0.00         1
        1846       0.00      0.00      0.00         6
        1847       0.00      0.00      0.00         2
        1848       0.50      0.20      0.29         5
        1849       0.50      0.33      0.40         6
        1850       0.33      0.38      0.35        48
        1851       0.60      0.77      0.67        77
        1852       0.00      0.00      0.00         7
        1853       0.00      0.00      0.00         7
        1854       0.00      0.00      0.00         3
        1855       0.56      0.39      0.46        23
        1856       0.89      0.67      0.76        12
        1857       0.45      0.33      0.38        30
        1858       0.00      0.00      0.00         2
        1859       0.00      0.00      0.00         2
        1860       0.23      0.26      0.24        65
        1861       0.71      0.73      0.72        85
        1862       0.26      0.32      0.29        19
        1863       0.38      0.42      0.40        19
        1864       0.25      0.18      0.21        17
        1865       0.17      0.14      0.15         7
        1866       0.17      0.20      0.18         5
        1867       0.40      0.18      0.25        11
        1868       0.09      0.14      0.11         7
        1869       0.12      0.20      0.15         5
        1870       0.44      0.48      0.46        31
        1871       0.72      0.69      0.71        49
        1872       0.43      0.43      0.43         7
        1873       0.14      0.10      0.12        10
        1874       0.25      0.20      0.22         5
        1875       0.31      0.36      0.33        14
        1876       1.00      0.80      0.89        10
        1877       0.00      0.00      0.00         5
        1878       0.50      0.22      0.31         9
        1879       0.00      0.00      0.00         2

    accuracy                           0.56      1256
   macro avg       0.37      0.32      0.33      1256
weighted avg       0.57      0.56      0.56      1256

Macro avg F1: 0.331
Weighted avg F1: 0.558
Micro avg F1: 0.563
Top-3 Accuracy: 0.796
Top-5 Accuracy: 0.868
Classification MAE (in years): 4.44
Micro ROC AUC  = 0.97
Macro ROC AUC (present classes) = 0.94
Metrics: {'accuracy': 0.5628980994224548}
Total running time: 1 hours, 24 minutes, 43 seconds
