TensorFlow Version: 2.6.0
Num GPUs Available: 1
Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')
Using model: InceptionV3.
RUN ID: 2025-05-06_14:58:19
Test fold: 0
Fine Tuning!
Epoch 1/100
707/707 - 149s - loss: 7.6812 - accuracy: 0.3688 - val_loss: 7.1875 - val_accuracy: 0.3925
Epoch 2/100
707/707 - 133s - loss: 7.1131 - accuracy: 0.4632 - val_loss: 8.8370 - val_accuracy: 0.3447
Epoch 3/100
707/707 - 136s - loss: 6.7457 - accuracy: 0.5036 - val_loss: 9.5828 - val_accuracy: 0.3336
Epoch 4/100
707/707 - 130s - loss: 6.7204 - accuracy: 0.5238 - val_loss: 8.3174 - val_accuracy: 0.4427
Epoch 5/100
707/707 - 131s - loss: 6.7701 - accuracy: 0.5374 - val_loss: 10.2907 - val_accuracy: 0.3901
Epoch 6/100
707/707 - 132s - loss: 6.6341 - accuracy: 0.5459 - val_loss: 8.6057 - val_accuracy: 0.4268

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.
Epoch 7/100
707/707 - 132s - loss: 3.0004 - accuracy: 0.6834 - val_loss: 4.8402 - val_accuracy: 0.5263
Epoch 8/100
707/707 - 132s - loss: 2.5239 - accuracy: 0.6980 - val_loss: 4.6742 - val_accuracy: 0.5279
Epoch 9/100
707/707 - 129s - loss: 2.2995 - accuracy: 0.7092 - val_loss: 4.5314 - val_accuracy: 0.5382
Epoch 10/100
707/707 - 128s - loss: 2.1636 - accuracy: 0.7130 - val_loss: 4.4387 - val_accuracy: 0.5494
Epoch 11/100
707/707 - 119s - loss: 2.1329 - accuracy: 0.7129 - val_loss: 4.3436 - val_accuracy: 0.5446
Epoch 12/100
707/707 - 120s - loss: 2.0534 - accuracy: 0.7149 - val_loss: 4.0950 - val_accuracy: 0.5406
Epoch 13/100
707/707 - 120s - loss: 1.9451 - accuracy: 0.7195 - val_loss: 4.0876 - val_accuracy: 0.5287
Epoch 14/100
707/707 - 118s - loss: 1.8481 - accuracy: 0.7262 - val_loss: 4.0843 - val_accuracy: 0.5231
Epoch 15/100
707/707 - 117s - loss: 1.8055 - accuracy: 0.7249 - val_loss: 3.9117 - val_accuracy: 0.5478
Epoch 16/100
707/707 - 117s - loss: 1.7859 - accuracy: 0.7272 - val_loss: 3.9068 - val_accuracy: 0.5446
Epoch 17/100
707/707 - 119s - loss: 1.6392 - accuracy: 0.7365 - val_loss: 3.9738 - val_accuracy: 0.5311
Epoch 18/100
707/707 - 119s - loss: 1.7074 - accuracy: 0.7281 - val_loss: 3.7166 - val_accuracy: 0.5549
Epoch 19/100
707/707 - 119s - loss: 1.5924 - accuracy: 0.7369 - val_loss: 3.8009 - val_accuracy: 0.5398
Epoch 20/100
707/707 - 119s - loss: 1.5756 - accuracy: 0.7390 - val_loss: 3.7629 - val_accuracy: 0.5605
Epoch 21/100
707/707 - 117s - loss: 1.5571 - accuracy: 0.7447 - val_loss: 3.6221 - val_accuracy: 0.5589
Epoch 22/100
707/707 - 119s - loss: 1.5043 - accuracy: 0.7411 - val_loss: 3.5498 - val_accuracy: 0.5462
Epoch 23/100
707/707 - 128s - loss: 1.4606 - accuracy: 0.7462 - val_loss: 3.5188 - val_accuracy: 0.5438
Epoch 24/100
707/707 - 129s - loss: 1.4505 - accuracy: 0.7399 - val_loss: 3.6958 - val_accuracy: 0.5366
Epoch 25/100
707/707 - 131s - loss: 1.4537 - accuracy: 0.7463 - val_loss: 3.5727 - val_accuracy: 0.5454
Epoch 26/100
707/707 - 130s - loss: 1.4437 - accuracy: 0.7475 - val_loss: 3.5840 - val_accuracy: 0.5422
Epoch 27/100
707/707 - 127s - loss: 1.3979 - accuracy: 0.7496 - val_loss: 3.7615 - val_accuracy: 0.5119
Epoch 28/100
707/707 - 127s - loss: 1.3599 - accuracy: 0.7571 - val_loss: 3.4396 - val_accuracy: 0.5494
Epoch 29/100
707/707 - 129s - loss: 1.3628 - accuracy: 0.7539 - val_loss: 3.6206 - val_accuracy: 0.5390
Epoch 30/100
707/707 - 128s - loss: 1.3536 - accuracy: 0.7551 - val_loss: 3.6219 - val_accuracy: 0.5462
Epoch 31/100
707/707 - 131s - loss: 1.3105 - accuracy: 0.7554 - val_loss: 3.4194 - val_accuracy: 0.5533
Epoch 32/100
707/707 - 129s - loss: 1.3166 - accuracy: 0.7513 - val_loss: 3.3483 - val_accuracy: 0.5502
Epoch 33/100
707/707 - 127s - loss: 1.2882 - accuracy: 0.7538 - val_loss: 3.3138 - val_accuracy: 0.5565
Epoch 34/100
707/707 - 129s - loss: 1.2859 - accuracy: 0.7566 - val_loss: 3.4609 - val_accuracy: 0.5565
Epoch 35/100
707/707 - 127s - loss: 1.2484 - accuracy: 0.7632 - val_loss: 3.3317 - val_accuracy: 0.5518
Epoch 36/100
707/707 - 133s - loss: 1.2296 - accuracy: 0.7661 - val_loss: 3.3469 - val_accuracy: 0.5518
Epoch 37/100
707/707 - 140s - loss: 1.2573 - accuracy: 0.7559 - val_loss: 3.2954 - val_accuracy: 0.5533
Epoch 38/100
707/707 - 145s - loss: 1.2393 - accuracy: 0.7624 - val_loss: 3.3835 - val_accuracy: 0.5533
Epoch 39/100
707/707 - 138s - loss: 1.2429 - accuracy: 0.7648 - val_loss: 3.5136 - val_accuracy: 0.5287
Epoch 40/100
707/707 - 125s - loss: 1.2275 - accuracy: 0.7636 - val_loss: 3.2977 - val_accuracy: 0.5454
Epoch 41/100
707/707 - 123s - loss: 1.1996 - accuracy: 0.7670 - val_loss: 3.2291 - val_accuracy: 0.5549
Epoch 42/100
707/707 - 123s - loss: 1.1897 - accuracy: 0.7640 - val_loss: 3.1707 - val_accuracy: 0.5541
Epoch 43/100
707/707 - 123s - loss: 1.2220 - accuracy: 0.7631 - val_loss: 3.2434 - val_accuracy: 0.5565
Epoch 44/100
707/707 - 124s - loss: 1.1488 - accuracy: 0.7671 - val_loss: 3.0834 - val_accuracy: 0.5613
Epoch 45/100
707/707 - 121s - loss: 1.1800 - accuracy: 0.7671 - val_loss: 3.2994 - val_accuracy: 0.5581
Epoch 46/100
707/707 - 122s - loss: 1.1412 - accuracy: 0.7701 - val_loss: 3.3936 - val_accuracy: 0.5494
Epoch 47/100
707/707 - 123s - loss: 1.1807 - accuracy: 0.7632 - val_loss: 3.2723 - val_accuracy: 0.5398
Epoch 48/100
707/707 - 125s - loss: 1.1050 - accuracy: 0.7745 - val_loss: 3.0728 - val_accuracy: 0.5637
Epoch 49/100
707/707 - 123s - loss: 1.0960 - accuracy: 0.7787 - val_loss: 3.1446 - val_accuracy: 0.5494
Epoch 50/100
707/707 - 124s - loss: 1.1310 - accuracy: 0.7753 - val_loss: 3.1143 - val_accuracy: 0.5701
Epoch 51/100
707/707 - 124s - loss: 1.1167 - accuracy: 0.7706 - val_loss: 3.0936 - val_accuracy: 0.5693
Epoch 52/100
707/707 - 124s - loss: 1.1278 - accuracy: 0.7705 - val_loss: 3.4378 - val_accuracy: 0.5414
Epoch 53/100
707/707 - 124s - loss: 1.1250 - accuracy: 0.7737 - val_loss: 3.1441 - val_accuracy: 0.5470

Epoch 00053: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.
Epoch 54/100
707/707 - 122s - loss: 0.8850 - accuracy: 0.8102 - val_loss: 2.8933 - val_accuracy: 0.5812
Epoch 55/100
707/707 - 121s - loss: 0.8759 - accuracy: 0.8121 - val_loss: 2.8485 - val_accuracy: 0.5756
Epoch 56/100
707/707 - 123s - loss: 0.8581 - accuracy: 0.8130 - val_loss: 2.8594 - val_accuracy: 0.5796
Epoch 57/100
707/707 - 122s - loss: 0.8797 - accuracy: 0.8083 - val_loss: 2.8582 - val_accuracy: 0.5756
Epoch 58/100
707/707 - 126s - loss: 0.8472 - accuracy: 0.8136 - val_loss: 2.8550 - val_accuracy: 0.5836
Epoch 59/100
707/707 - 123s - loss: 0.8566 - accuracy: 0.8144 - val_loss: 2.8563 - val_accuracy: 0.5812
Epoch 60/100
707/707 - 123s - loss: 0.8479 - accuracy: 0.8133 - val_loss: 2.8097 - val_accuracy: 0.5860
Epoch 61/100
707/707 - 124s - loss: 0.8595 - accuracy: 0.8124 - val_loss: 2.8611 - val_accuracy: 0.5844
Epoch 62/100
707/707 - 124s - loss: 0.8557 - accuracy: 0.8126 - val_loss: 2.8472 - val_accuracy: 0.5812
Epoch 63/100
707/707 - 122s - loss: 0.8645 - accuracy: 0.8132 - val_loss: 2.8304 - val_accuracy: 0.5796
Epoch 64/100
707/707 - 126s - loss: 0.8221 - accuracy: 0.8187 - val_loss: 2.8479 - val_accuracy: 0.5820
Epoch 65/100
707/707 - 121s - loss: 0.8678 - accuracy: 0.8101 - val_loss: 2.8324 - val_accuracy: 0.5788

Epoch 00065: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.
Epoch 66/100
707/707 - 124s - loss: 0.8103 - accuracy: 0.8235 - val_loss: 2.8225 - val_accuracy: 0.5796
Epoch 67/100
707/707 - 123s - loss: 0.8364 - accuracy: 0.8193 - val_loss: 2.8226 - val_accuracy: 0.5828
Epoch 68/100
707/707 - 122s - loss: 0.8147 - accuracy: 0.8250 - val_loss: 2.8186 - val_accuracy: 0.5788
Epoch 69/100
707/707 - 122s - loss: 0.8148 - accuracy: 0.8182 - val_loss: 2.8093 - val_accuracy: 0.5796
Epoch 70/100
707/707 - 123s - loss: 0.8269 - accuracy: 0.8177 - val_loss: 2.8075 - val_accuracy: 0.5788
Epoch 71/100
707/707 - 123s - loss: 0.8104 - accuracy: 0.8230 - val_loss: 2.8023 - val_accuracy: 0.5828
Epoch 72/100
707/707 - 124s - loss: 0.8149 - accuracy: 0.8231 - val_loss: 2.8010 - val_accuracy: 0.5804
Epoch 73/100
707/707 - 124s - loss: 0.7837 - accuracy: 0.8237 - val_loss: 2.7943 - val_accuracy: 0.5828
Epoch 74/100
707/707 - 123s - loss: 0.8040 - accuracy: 0.8223 - val_loss: 2.7967 - val_accuracy: 0.5812
Epoch 75/100
707/707 - 123s - loss: 0.8316 - accuracy: 0.8199 - val_loss: 2.7950 - val_accuracy: 0.5820
Epoch 76/100
707/707 - 124s - loss: 0.8152 - accuracy: 0.8177 - val_loss: 2.7999 - val_accuracy: 0.5804
Epoch 77/100
707/707 - 122s - loss: 0.8050 - accuracy: 0.8232 - val_loss: 2.8027 - val_accuracy: 0.5780
Epoch 78/100
707/707 - 123s - loss: 0.8144 - accuracy: 0.8227 - val_loss: 2.8039 - val_accuracy: 0.5820

Epoch 00078: ReduceLROnPlateau reducing learning rate to 1e-06.
Epoch 79/100
707/707 - 123s - loss: 0.8083 - accuracy: 0.8211 - val_loss: 2.8038 - val_accuracy: 0.5828
Epoch 80/100
707/707 - 125s - loss: 0.8309 - accuracy: 0.8143 - val_loss: 2.8023 - val_accuracy: 0.5804
Epoch 81/100
707/707 - 121s - loss: 0.8135 - accuracy: 0.8205 - val_loss: 2.8017 - val_accuracy: 0.5804
Epoch 82/100
707/707 - 126s - loss: 0.7891 - accuracy: 0.8254 - val_loss: 2.8006 - val_accuracy: 0.5812
Epoch 83/100
707/707 - 124s - loss: 0.8044 - accuracy: 0.8198 - val_loss: 2.8004 - val_accuracy: 0.5812
Restoring model weights from the end of the best epoch.
Epoch 00083: early stopping
Fold 0 Evaluation results: [2.794269323348999, 0.5828025341033936]
              precision    recall  f1-score   support

        1820       0.64      0.69      0.67        62
        1821       0.90      0.91      0.90        57
        1822       0.00      0.00      0.00         1
        1823       0.00      0.00      0.00         1
        1824       0.00      0.00      0.00         1
        1825       0.67      0.67      0.67         3
        1826       0.00      0.00      0.00         2
        1827       0.63      0.68      0.65        25
        1828       0.00      0.00      0.00         1
        1829       0.00      0.00      0.00         5
        1830       0.54      0.64      0.59        56
        1831       0.87      0.78      0.82       134
        1832       0.79      0.78      0.78        67
        1833       1.00      0.84      0.91        19
        1834       0.49      0.76      0.59        29
        1835       0.00      0.00      0.00         2
        1836       0.50      0.25      0.33         4
        1837       0.57      0.67      0.62         6
        1838       0.50      0.33      0.40         3
        1839       0.00      0.00      0.00         1
        1840       0.59      0.56      0.57        43
        1841       0.64      0.58      0.61       108
        1842       0.67      0.67      0.67         6
        1843       0.29      0.33      0.31         6
        1844       0.00      0.00      0.00         1
        1845       0.00      0.00      0.00         1
        1846       0.33      0.17      0.22         6
        1847       0.00      0.00      0.00         2
        1848       0.67      0.40      0.50         5
        1849       0.67      0.33      0.44         6
        1850       0.33      0.42      0.37        48
        1851       0.62      0.73      0.67        77
        1852       0.25      0.14      0.18         7
        1853       0.00      0.00      0.00         7
        1854       0.00      0.00      0.00         3
        1855       0.65      0.48      0.55        23
        1856       0.75      0.75      0.75        12
        1857       0.39      0.37      0.38        30
        1858       0.00      0.00      0.00         2
        1859       0.00      0.00      0.00         2
        1860       0.19      0.26      0.22        65
        1861       0.70      0.75      0.73        85
        1862       0.10      0.11      0.10        19
        1863       0.60      0.47      0.53        19
        1864       0.11      0.12      0.11        17
        1865       0.60      0.43      0.50         7
        1866       0.12      0.20      0.15         5
        1867       1.00      0.18      0.31        11
        1868       0.17      0.14      0.15         7
        1869       0.20      0.20      0.20         5
        1870       0.39      0.42      0.41        31
        1871       0.75      0.73      0.74        49
        1872       0.29      0.29      0.29         7
        1873       0.20      0.10      0.13        10
        1874       1.00      0.40      0.57         5
        1875       0.31      0.36      0.33        14
        1876       1.00      0.80      0.89        10
        1877       0.50      0.40      0.44         5
        1878       0.42      0.56      0.48         9
        1879       0.00      0.00      0.00         2

    accuracy                           0.58      1256
   macro avg       0.39      0.35      0.36      1256
weighted avg       0.59      0.58      0.58      1256

Macro avg F1: 0.358
Weighted avg F1: 0.580
Micro avg F1: 0.583
Top-3 Accuracy: 0.815
Top-5 Accuracy: 0.882
Micro ROC AUC  = 0.97
Macro ROC AUC (present classes) = 0.94
Classification MAE (in years): 4.09
Metrics: {'accuracy': 0.5828025341033936, 'mae_years': 4.094745222929936}
Total running time: 3 hours, 0 minutes, 21 seconds
