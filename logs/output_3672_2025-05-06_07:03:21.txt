Using model: NASNetMobile.
TensorFlow Version: 2.19.0
Num GPUs Available: 1
Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')
Test fold: 6
model_name: NASNetMobile
input_shape: (224, 224, 3)
Epoch 1/100
707/707 - 170s - 240ms/step - accuracy: 0.3972 - loss: 3.1720 - val_accuracy: 0.3248 - val_loss: 4.0937 - learning_rate: 0.0100
Epoch 2/100
707/707 - 111s - 157ms/step - accuracy: 0.4722 - loss: 2.8396 - val_accuracy: 0.3432 - val_loss: 4.1848 - learning_rate: 0.0100
Epoch 3/100
707/707 - 105s - 149ms/step - accuracy: 0.5044 - loss: 2.7624 - val_accuracy: 0.3527 - val_loss: 4.4242 - learning_rate: 0.0100
Epoch 4/100
707/707 - 100s - 142ms/step - accuracy: 0.5269 - loss: 2.6698 - val_accuracy: 0.3583 - val_loss: 5.1263 - learning_rate: 0.0100
Epoch 5/100
707/707 - 100s - 141ms/step - accuracy: 0.5445 - loss: 2.5677 - val_accuracy: 0.3766 - val_loss: 4.2068 - learning_rate: 0.0100
Epoch 6/100

Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.
707/707 - 96s - 136ms/step - accuracy: 0.5392 - loss: 2.7628 - val_accuracy: 0.3225 - val_loss: 5.1836 - learning_rate: 0.0100
Epoch 7/100
707/707 - 99s - 140ms/step - accuracy: 0.6333 - loss: 1.6615 - val_accuracy: 0.4570 - val_loss: 3.1208 - learning_rate: 1.0000e-03
Epoch 8/100
707/707 - 97s - 138ms/step - accuracy: 0.6637 - loss: 1.3741 - val_accuracy: 0.4610 - val_loss: 2.9992 - learning_rate: 1.0000e-03
Epoch 9/100
707/707 - 99s - 140ms/step - accuracy: 0.6705 - loss: 1.2905 - val_accuracy: 0.4642 - val_loss: 2.9952 - learning_rate: 1.0000e-03
Epoch 10/100
707/707 - 103s - 145ms/step - accuracy: 0.6626 - loss: 1.3201 - val_accuracy: 0.4705 - val_loss: 2.7931 - learning_rate: 1.0000e-03
Epoch 11/100
707/707 - 101s - 143ms/step - accuracy: 0.6754 - loss: 1.2091 - val_accuracy: 0.4610 - val_loss: 2.7255 - learning_rate: 1.0000e-03
Epoch 12/100
707/707 - 107s - 151ms/step - accuracy: 0.6756 - loss: 1.1896 - val_accuracy: 0.4602 - val_loss: 2.7131 - learning_rate: 1.0000e-03
Epoch 13/100
707/707 - 107s - 151ms/step - accuracy: 0.6887 - loss: 1.1282 - val_accuracy: 0.4411 - val_loss: 2.7758 - learning_rate: 1.0000e-03
Epoch 14/100
707/707 - 103s - 146ms/step - accuracy: 0.6708 - loss: 1.1637 - val_accuracy: 0.4713 - val_loss: 2.5817 - learning_rate: 1.0000e-03
Epoch 15/100
707/707 - 99s - 141ms/step - accuracy: 0.6797 - loss: 1.1381 - val_accuracy: 0.4658 - val_loss: 2.5910 - learning_rate: 1.0000e-03
Epoch 16/100
707/707 - 101s - 142ms/step - accuracy: 0.6944 - loss: 1.0592 - val_accuracy: 0.4578 - val_loss: 2.6723 - learning_rate: 1.0000e-03
Epoch 17/100
707/707 - 144s - 204ms/step - accuracy: 0.6867 - loss: 1.0734 - val_accuracy: 0.4594 - val_loss: 2.5049 - learning_rate: 1.0000e-03
Epoch 18/100
707/707 - 102s - 144ms/step - accuracy: 0.6877 - loss: 1.0801 - val_accuracy: 0.4650 - val_loss: 2.5232 - learning_rate: 1.0000e-03
Epoch 19/100
707/707 - 111s - 156ms/step - accuracy: 0.6916 - loss: 1.0598 - val_accuracy: 0.4570 - val_loss: 2.4572 - learning_rate: 1.0000e-03
Epoch 20/100
707/707 - 106s - 150ms/step - accuracy: 0.6888 - loss: 1.0687 - val_accuracy: 0.4769 - val_loss: 2.3676 - learning_rate: 1.0000e-03
Epoch 21/100
707/707 - 109s - 154ms/step - accuracy: 0.6945 - loss: 1.0544 - val_accuracy: 0.4610 - val_loss: 2.3747 - learning_rate: 1.0000e-03
Epoch 22/100
707/707 - 107s - 152ms/step - accuracy: 0.6914 - loss: 1.0044 - val_accuracy: 0.4435 - val_loss: 2.5229 - learning_rate: 1.0000e-03
Epoch 23/100
707/707 - 103s - 145ms/step - accuracy: 0.6850 - loss: 1.0681 - val_accuracy: 0.4713 - val_loss: 2.3741 - learning_rate: 1.0000e-03
Epoch 24/100
707/707 - 99s - 140ms/step - accuracy: 0.6907 - loss: 1.0235 - val_accuracy: 0.4817 - val_loss: 2.3941 - learning_rate: 1.0000e-03
Epoch 25/100

Epoch 25: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.
707/707 - 100s - 142ms/step - accuracy: 0.7074 - loss: 0.9856 - val_accuracy: 0.4586 - val_loss: 2.3827 - learning_rate: 1.0000e-03
Epoch 26/100
707/707 - 100s - 141ms/step - accuracy: 0.7010 - loss: 0.9688 - val_accuracy: 0.4896 - val_loss: 2.2611 - learning_rate: 1.0000e-04
Epoch 27/100
707/707 - 100s - 141ms/step - accuracy: 0.7120 - loss: 0.9146 - val_accuracy: 0.4857 - val_loss: 2.2488 - learning_rate: 1.0000e-04
Epoch 28/100
707/707 - 114s - 161ms/step - accuracy: 0.7271 - loss: 0.8605 - val_accuracy: 0.4857 - val_loss: 2.2583 - learning_rate: 1.0000e-04
Epoch 29/100
707/707 - 107s - 151ms/step - accuracy: 0.7157 - loss: 0.9087 - val_accuracy: 0.4849 - val_loss: 2.2362 - learning_rate: 1.0000e-04
Epoch 30/100
707/707 - 136s - 192ms/step - accuracy: 0.7175 - loss: 0.9028 - val_accuracy: 0.4889 - val_loss: 2.2307 - learning_rate: 1.0000e-04
Epoch 31/100
707/707 - 126s - 179ms/step - accuracy: 0.7130 - loss: 0.9156 - val_accuracy: 0.4976 - val_loss: 2.2261 - learning_rate: 1.0000e-04
Epoch 32/100
707/707 - 115s - 163ms/step - accuracy: 0.7211 - loss: 0.8994 - val_accuracy: 0.4896 - val_loss: 2.2309 - learning_rate: 1.0000e-04
Epoch 33/100
707/707 - 121s - 171ms/step - accuracy: 0.7221 - loss: 0.8700 - val_accuracy: 0.4920 - val_loss: 2.2208 - learning_rate: 1.0000e-04
Epoch 34/100
707/707 - 119s - 169ms/step - accuracy: 0.7207 - loss: 0.8928 - val_accuracy: 0.4912 - val_loss: 2.2162 - learning_rate: 1.0000e-04
Epoch 35/100
707/707 - 126s - 178ms/step - accuracy: 0.7292 - loss: 0.8664 - val_accuracy: 0.4968 - val_loss: 2.2223 - learning_rate: 1.0000e-04
Epoch 36/100
707/707 - 129s - 183ms/step - accuracy: 0.7226 - loss: 0.8651 - val_accuracy: 0.4928 - val_loss: 2.2292 - learning_rate: 1.0000e-04
Epoch 37/100
707/707 - 123s - 174ms/step - accuracy: 0.7289 - loss: 0.8522 - val_accuracy: 0.5000 - val_loss: 2.2241 - learning_rate: 1.0000e-04
Epoch 38/100
707/707 - 119s - 168ms/step - accuracy: 0.7132 - loss: 0.9386 - val_accuracy: 0.4912 - val_loss: 2.2139 - learning_rate: 1.0000e-04
Epoch 39/100
707/707 - 105s - 149ms/step - accuracy: 0.7234 - loss: 0.8626 - val_accuracy: 0.4944 - val_loss: 2.2193 - learning_rate: 1.0000e-04
Epoch 40/100
707/707 - 114s - 161ms/step - accuracy: 0.7270 - loss: 0.8570 - val_accuracy: 0.4992 - val_loss: 2.2122 - learning_rate: 1.0000e-04
Epoch 41/100
707/707 - 116s - 165ms/step - accuracy: 0.7144 - loss: 0.9112 - val_accuracy: 0.4952 - val_loss: 2.1915 - learning_rate: 1.0000e-04
Epoch 42/100
707/707 - 112s - 158ms/step - accuracy: 0.7252 - loss: 0.8617 - val_accuracy: 0.4968 - val_loss: 2.2004 - learning_rate: 1.0000e-04
Epoch 43/100
707/707 - 122s - 172ms/step - accuracy: 0.7203 - loss: 0.8789 - val_accuracy: 0.4936 - val_loss: 2.1978 - learning_rate: 1.0000e-04
Epoch 44/100
707/707 - 121s - 171ms/step - accuracy: 0.7225 - loss: 0.8762 - val_accuracy: 0.4936 - val_loss: 2.1871 - learning_rate: 1.0000e-04
Epoch 45/100
707/707 - 122s - 173ms/step - accuracy: 0.7322 - loss: 0.8437 - val_accuracy: 0.5008 - val_loss: 2.2009 - learning_rate: 1.0000e-04
Epoch 46/100
707/707 - 121s - 171ms/step - accuracy: 0.7296 - loss: 0.8454 - val_accuracy: 0.4976 - val_loss: 2.1911 - learning_rate: 1.0000e-04
Epoch 47/100
707/707 - 111s - 157ms/step - accuracy: 0.7307 - loss: 0.8475 - val_accuracy: 0.4904 - val_loss: 2.1937 - learning_rate: 1.0000e-04
Epoch 48/100
707/707 - 110s - 156ms/step - accuracy: 0.7166 - loss: 0.8908 - val_accuracy: 0.4984 - val_loss: 2.1941 - learning_rate: 1.0000e-04
Epoch 49/100

Epoch 49: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.
707/707 - 112s - 159ms/step - accuracy: 0.7249 - loss: 0.8543 - val_accuracy: 0.4920 - val_loss: 2.1943 - learning_rate: 1.0000e-04
Epoch 50/100
707/707 - 117s - 166ms/step - accuracy: 0.7245 - loss: 0.8481 - val_accuracy: 0.4881 - val_loss: 2.1954 - learning_rate: 1.0000e-05
Epoch 51/100
707/707 - 120s - 170ms/step - accuracy: 0.7190 - loss: 0.8805 - val_accuracy: 0.4889 - val_loss: 2.1953 - learning_rate: 1.0000e-05
Epoch 52/100
707/707 - 119s - 169ms/step - accuracy: 0.7395 - loss: 0.8201 - val_accuracy: 0.4873 - val_loss: 2.1936 - learning_rate: 1.0000e-05
Epoch 53/100
707/707 - 122s - 172ms/step - accuracy: 0.7284 - loss: 0.8681 - val_accuracy: 0.4896 - val_loss: 2.1913 - learning_rate: 1.0000e-05
Epoch 54/100

Epoch 54: ReduceLROnPlateau reducing learning rate to 1e-06.
707/707 - 120s - 169ms/step - accuracy: 0.7226 - loss: 0.8570 - val_accuracy: 0.4912 - val_loss: 2.1933 - learning_rate: 1.0000e-05
Epoch 54: early stopping
Restoring model weights from the end of the best epoch: 44.
Fold 6 Evaluation results: [2.1871337890625, 0.493630588054657]
1/1 - 11s - 11s/step
1/1 - 0s - 131ms/step
1/1 - 0s - 142ms/step
1/1 - 0s - 134ms/step
1/1 - 0s - 135ms/step
1/1 - 0s - 138ms/step
1/1 - 0s - 135ms/step
1/1 - 0s - 132ms/step
1/1 - 0s - 128ms/step
1/1 - 0s - 134ms/step
1/1 - 0s - 132ms/step
1/1 - 0s - 132ms/step
1/1 - 0s - 129ms/step
1/1 - 0s - 134ms/step
1/1 - 0s - 149ms/step
1/1 - 0s - 143ms/step
1/1 - 0s - 135ms/step
1/1 - 0s - 135ms/step
1/1 - 0s - 131ms/step
1/1 - 0s - 133ms/step
1/1 - 0s - 142ms/step
1/1 - 0s - 140ms/step
1/1 - 0s - 131ms/step
1/1 - 0s - 140ms/step
1/1 - 0s - 137ms/step
1/1 - 0s - 149ms/step
1/1 - 0s - 142ms/step
1/1 - 0s - 132ms/step
1/1 - 0s - 140ms/step
1/1 - 0s - 127ms/step
1/1 - 0s - 129ms/step
1/1 - 0s - 129ms/step
1/1 - 0s - 135ms/step
1/1 - 0s - 139ms/step
1/1 - 0s - 132ms/step
1/1 - 0s - 140ms/step
1/1 - 0s - 136ms/step
1/1 - 0s - 126ms/step
1/1 - 0s - 127ms/step
1/1 - 0s - 135ms/step
1/1 - 0s - 135ms/step
1/1 - 0s - 126ms/step
1/1 - 0s - 132ms/step
1/1 - 0s - 133ms/step
1/1 - 0s - 135ms/step
1/1 - 0s - 137ms/step
1/1 - 0s - 131ms/step
1/1 - 0s - 127ms/step
1/1 - 0s - 131ms/step
1/1 - 0s - 126ms/step
1/1 - 0s - 135ms/step
1/1 - 0s - 144ms/step
1/1 - 0s - 133ms/step
1/1 - 0s - 136ms/step
1/1 - 0s - 142ms/step
1/1 - 0s - 154ms/step
1/1 - 0s - 131ms/step
1/1 - 0s - 138ms/step
1/1 - 0s - 135ms/step
1/1 - 0s - 132ms/step
1/1 - 0s - 144ms/step
1/1 - 0s - 122ms/step
1/1 - 0s - 133ms/step
1/1 - 0s - 128ms/step
1/1 - 0s - 135ms/step
1/1 - 0s - 128ms/step
1/1 - 0s - 134ms/step
1/1 - 0s - 125ms/step
1/1 - 0s - 129ms/step
1/1 - 0s - 135ms/step
1/1 - 0s - 139ms/step
1/1 - 0s - 132ms/step
1/1 - 0s - 129ms/step
1/1 - 0s - 135ms/step
1/1 - 0s - 141ms/step
1/1 - 0s - 137ms/step
1/1 - 0s - 123ms/step
1/1 - 0s - 129ms/step
1/1 - 10s - 10s/step
              precision    recall  f1-score   support

        1820       0.59      0.59      0.59        61
        1821       0.74      0.74      0.74        58
        1822       0.00      0.00      0.00         2
        1823       0.00      0.00      0.00         1
        1824       0.00      0.00      0.00         1
        1825       0.00      0.00      0.00         2
        1826       0.00      0.00      0.00         2
        1827       0.58      0.60      0.59        25
        1828       1.00      0.50      0.67         2
        1829       0.00      0.00      0.00         4
        1830       0.47      0.39      0.43        56
        1831       0.76      0.73      0.74       135
        1832       0.69      0.81      0.74        68
        1833       0.86      0.63      0.73        19
        1834       0.45      0.66      0.54        29
        1835       0.00      0.00      0.00         2
        1836       0.00      0.00      0.00         3
        1837       0.40      0.33      0.36         6
        1838       0.25      0.25      0.25         4
        1839       0.00      0.00      0.00         1
        1840       0.41      0.55      0.47        42
        1841       0.45      0.42      0.44       107
        1842       0.43      0.50      0.46         6
        1843       0.50      0.50      0.50         6
        1844       0.00      0.00      0.00         0
        1845       0.00      0.00      0.00         1
        1846       0.25      0.17      0.20         6
        1847       0.00      0.00      0.00         2
        1848       0.40      0.40      0.40         5
        1849       0.00      0.00      0.00         5
        1850       0.35      0.38      0.36        48
        1851       0.44      0.40      0.42        77
        1852       0.00      0.00      0.00         8
        1853       0.00      0.00      0.00         6
        1854       0.00      0.00      0.00         2
        1855       0.42      0.22      0.29        23
        1856       0.62      0.67      0.64        12
        1857       0.27      0.42      0.33        31
        1858       0.00      0.00      0.00         3
        1859       0.00      0.00      0.00         3
        1860       0.28      0.37      0.32        65
        1861       0.61      0.59      0.60        85
        1862       0.33      0.16      0.21        19
        1863       0.40      0.44      0.42        18
        1864       0.25      0.24      0.24        17
        1865       0.12      0.14      0.13         7
        1866       0.00      0.00      0.00         6
        1867       0.25      0.10      0.14        10
        1868       0.17      0.14      0.15         7
        1869       0.00      0.00      0.00         6
        1870       0.36      0.52      0.43        31
        1871       0.53      0.57      0.55        49
        1872       0.00      0.00      0.00         7
        1873       0.71      0.45      0.56        11
        1874       0.25      0.20      0.22         5
        1875       0.35      0.43      0.39        14
        1876       0.62      1.00      0.77        10
        1877       1.00      0.20      0.33         5
        1878       0.33      0.44      0.38         9
        1879       0.00      0.00      0.00         1

    accuracy                           0.49      1256
   macro avg       0.30      0.28      0.28      1256
weighted avg       0.49      0.49      0.48      1256

Macro avg F1: 0.279
Weighted avg F1: 0.484
Micro avg F1: 0.494
Top-3 Accuracy: 0.736
Top-5 Accuracy: 0.824
Classification MAE (in years): 5.36
Micro ROC AUC  = 0.96
Macro ROC AUC (present classes) = 0.91
Metrics: {'mae': 2.1871337890625, 'accuracy': 0.493630588054657}
Total running time: 1 hours, 42 minutes, 26 seconds
