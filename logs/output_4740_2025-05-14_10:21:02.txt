TensorFlow Version: 2.20.0-dev20250425
Num GPUs Available: 1
Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')

=== Using model: InceptionV3. ===
RUN ID: 2025-05-14_10:21:07

===== Running 5x2 Cross-Validation for Null Hypothesis Testing =====
Using models specified in config: InceptionV3 vs NASNetMobile
Comparing: Base model (InceptionV3) vs. Alternative model (NASNetMobile)

===== Iteration 1/5 =====
=== Training Base Model ===

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 125s - 160ms/step - accuracy: 0.1245 - loss: 4.4548 - val_accuracy: 0.1876 - val_loss: 3.8150 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 81s - 103ms/step - accuracy: 0.1752 - loss: 4.1667 - val_accuracy: 0.2476 - val_loss: 3.7949 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 81s - 103ms/step - accuracy: 0.2110 - loss: 4.0057 - val_accuracy: 0.3088 - val_loss: 3.7850 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 68s - 86ms/step - accuracy: 0.2289 - loss: 3.8943 - val_accuracy: 0.3451 - val_loss: 3.7193 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 65s - 83ms/step - accuracy: 0.2438 - loss: 3.8058 - val_accuracy: 0.3463 - val_loss: 3.4932 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 80s - 102ms/step - accuracy: 0.2647 - loss: 3.7134 - val_accuracy: 0.3658 - val_loss: 3.3475 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 79s - 101ms/step - accuracy: 0.2735 - loss: 3.6507 - val_accuracy: 0.3976 - val_loss: 3.2427 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 80s - 101ms/step - accuracy: 0.2803 - loss: 3.6032 - val_accuracy: 0.4067 - val_loss: 3.1413 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 79s - 101ms/step - accuracy: 0.2929 - loss: 3.5468 - val_accuracy: 0.4143 - val_loss: 3.0240 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 79s - 100ms/step - accuracy: 0.2956 - loss: 3.5099 - val_accuracy: 0.4202 - val_loss: 3.0015 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 72s - 92ms/step - accuracy: 0.2973 - loss: 3.4810 - val_accuracy: 0.4311 - val_loss: 2.9427 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 66s - 84ms/step - accuracy: 0.3037 - loss: 3.4564 - val_accuracy: 0.4459 - val_loss: 2.9301 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 75s - 96ms/step - accuracy: 0.3179 - loss: 3.3885 - val_accuracy: 0.4564 - val_loss: 2.8505 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 74s - 95ms/step - accuracy: 0.3155 - loss: 3.3827 - val_accuracy: 0.4533 - val_loss: 2.8401 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 74s - 94ms/step - accuracy: 0.3268 - loss: 3.3233 - val_accuracy: 0.4502 - val_loss: 2.7837 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 15.
Fold 0_1 Evaluation results: [2.7889702320098877, 0.4501592218875885]
              precision    recall  f1-score   support

        1820       0.52      0.65      0.58       311
        1821       0.75      0.55      0.63       278
        1822       0.00      0.00      0.00         4
        1823       0.00      0.00      0.00         6
        1824       0.00      0.00      0.00         5
        1825       0.00      0.00      0.00        12
        1826       0.00      0.00      0.00        10
        1827       0.77      0.42      0.54       130
        1828       0.00      0.00      0.00         4
        1829       0.00      0.00      0.00        19
        1830       0.37      0.51      0.43       283
        1831       0.56      0.89      0.69       664
        1832       0.60      0.61      0.60       310
        1833       0.80      0.66      0.72        90
        1834       0.50      0.31      0.38       143
        1835       0.00      0.00      0.00        10
        1836       0.00      0.00      0.00        17
        1837       0.00      0.00      0.00        29
        1838       0.00      0.00      0.00        17
        1839       0.00      0.00      0.00         5
        1840       0.61      0.32      0.42       229
        1841       0.37      0.42      0.40       560
        1842       0.00      0.00      0.00        29
        1843       0.00      0.00      0.00        33
        1844       0.00      0.00      0.00         2
        1845       0.00      0.00      0.00         9
        1846       0.00      0.00      0.00        21
        1847       0.00      0.00      0.00         8
        1848       0.00      0.00      0.00        32
        1849       0.00      0.00      0.00        24
        1850       0.35      0.31      0.33       231
        1851       0.43      0.69      0.53       379
        1852       0.00      0.00      0.00        34
        1853       0.00      0.00      0.00        31
        1854       0.00      0.00      0.00        11
        1855       0.38      0.05      0.09       123
        1856       0.75      0.05      0.09        64
        1857       0.31      0.70      0.43       150
        1858       0.00      0.00      0.00        10
        1859       0.00      0.00      0.00        10
        1860       0.24      0.31      0.27       311
        1861       0.47      0.74      0.57       423
        1862       0.00      0.00      0.00        93
        1863       0.29      0.04      0.08        90
        1864       0.17      0.01      0.02        94
        1865       0.00      0.00      0.00        38
        1866       0.00      0.00      0.00        30
        1867       0.00      0.00      0.00        51
        1868       0.00      0.00      0.00        33
        1869       0.00      0.00      0.00        27
        1870       0.54      0.13      0.21       162
        1871       0.31      0.73      0.44       249
        1872       0.00      0.00      0.00        40
        1873       0.00      0.00      0.00        57
        1874       0.00      0.00      0.00        25
        1875       0.35      0.14      0.20        78
        1876       1.00      0.02      0.04        52
        1877       0.00      0.00      0.00        32
        1878       0.00      0.00      0.00        51
        1879       0.00      0.00      0.00         7

    accuracy                           0.45      6280
   macro avg       0.19      0.15      0.14      6280
weighted avg       0.41      0.45      0.40      6280

Matthews Correlation Coefficient: 0.418
Macro avg F1: 0.145
Weighted avg F1: 0.395
Micro avg F1: 0.450
Top-3 Accuracy: 0.706
Top-5 Accuracy: 0.781
Micro ROC AUC  = 0.95
Macro ROC AUC (present classes) = 0.92
Classification MAE (in years): 5.48

Fold 0_1 Misclassification Analysis:
Near misses (within 2 years): 632 out of 3453 misclassifications (18.30%)
MAE with outliers: 5.48
MAE without outliers: 4.06 (improvement: 1.41)

5 Worst misclassifications:
Image: data/datasets/public/1870/1877_033met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1877_035met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1877_034met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1876_53washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_449vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_428vna.jpg, True: 1876, Predicted: 1821, Error: 55
Image: data/datasets/private/1870/1871_415etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/public/1820/1820_023met.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_115_001wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1870/1871_114etsy.jpg, True: 1871, Predicted: 1820, Error: 51

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 96s - 123ms/step - accuracy: 0.1105 - loss: 4.5211 - val_accuracy: 0.2164 - val_loss: 3.9022 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 72s - 92ms/step - accuracy: 0.1578 - loss: 4.2124 - val_accuracy: 0.2484 - val_loss: 4.2311 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 74s - 95ms/step - accuracy: 0.1960 - loss: 4.0743 - val_accuracy: 0.3324 - val_loss: 3.6921 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 73s - 93ms/step - accuracy: 0.2156 - loss: 3.9567 - val_accuracy: 0.3440 - val_loss: 3.5253 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 73s - 93ms/step - accuracy: 0.2301 - loss: 3.8631 - val_accuracy: 0.3776 - val_loss: 3.4367 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 67s - 85ms/step - accuracy: 0.2486 - loss: 3.7799 - val_accuracy: 0.3969 - val_loss: 3.2396 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 64s - 82ms/step - accuracy: 0.2610 - loss: 3.7209 - val_accuracy: 0.4141 - val_loss: 3.2229 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 66s - 84ms/step - accuracy: 0.2697 - loss: 3.6631 - val_accuracy: 0.4289 - val_loss: 3.2208 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 71s - 91ms/step - accuracy: 0.2809 - loss: 3.5820 - val_accuracy: 0.4318 - val_loss: 3.0491 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 72s - 91ms/step - accuracy: 0.2879 - loss: 3.5625 - val_accuracy: 0.4130 - val_loss: 2.9644 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 72s - 92ms/step - accuracy: 0.2885 - loss: 3.5235 - val_accuracy: 0.4580 - val_loss: 2.9487 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 72s - 91ms/step - accuracy: 0.2909 - loss: 3.4806 - val_accuracy: 0.4822 - val_loss: 2.8940 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 72s - 92ms/step - accuracy: 0.3043 - loss: 3.4571 - val_accuracy: 0.4843 - val_loss: 2.8922 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 72s - 92ms/step - accuracy: 0.3057 - loss: 3.4153 - val_accuracy: 0.4619 - val_loss: 2.7222 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 72s - 91ms/step - accuracy: 0.3140 - loss: 3.3758 - val_accuracy: 0.4674 - val_loss: 2.7869 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 14.
Fold 0_2 Evaluation results: [2.715920925140381, 0.46185699105262756]
              precision    recall  f1-score   support

        1820       0.62      0.59      0.60       306
        1821       0.87      0.65      0.74       296
        1822       0.00      0.00      0.00        10
        1823       0.00      0.00      0.00         7
        1824       0.00      0.00      0.00         5
        1825       0.00      0.00      0.00         9
        1826       0.00      0.00      0.00        13
        1827       0.73      0.31      0.43       120
        1828       0.00      0.00      0.00        13
        1829       0.00      0.00      0.00        25
        1830       0.32      0.71      0.44       277
        1831       0.63      0.85      0.72       680
        1832       0.82      0.59      0.68       369
        1833       0.84      0.81      0.83       100
        1834       0.83      0.03      0.06       149
        1835       0.00      0.00      0.00        11
        1836       0.00      0.00      0.00        18
        1837       0.00      0.00      0.00        35
        1838       0.00      0.00      0.00        21
        1839       0.00      0.00      0.00         3
        1840       0.24      0.54      0.33       198
        1841       0.33      0.60      0.43       515
        1842       0.00      0.00      0.00        26
        1843       0.00      0.00      0.00        25
        1844       0.00      0.00      0.00         1
        1845       0.00      0.00      0.00         2
        1846       0.00      0.00      0.00        36
        1847       0.00      0.00      0.00        12
        1848       0.00      0.00      0.00        23
        1849       0.00      0.00      0.00        29
        1850       0.25      0.41      0.31       246
        1851       0.47      0.71      0.57       391
        1852       0.00      0.00      0.00        40
        1853       0.00      0.00      0.00        32
        1854       0.00      0.00      0.00        13
        1855       0.00      0.00      0.00       109
        1856       0.67      0.14      0.24        56
        1857       0.33      0.50      0.40       157
        1858       0.00      0.00      0.00        13
        1859       0.00      0.00      0.00        15
        1860       0.25      0.26      0.26       335
        1861       0.60      0.58      0.59       427
        1862       0.00      0.00      0.00        96
        1863       0.67      0.02      0.04        94
        1864       0.00      0.00      0.00        76
        1865       0.00      0.00      0.00        28
        1866       0.00      0.00      0.00        27
        1867       0.00      0.00      0.00        53
        1868       0.00      0.00      0.00        40
        1869       0.00      0.00      0.00        26
        1870       0.30      0.12      0.17       145
        1871       0.47      0.65      0.55       242
        1872       0.00      0.00      0.00        31
        1873       0.00      0.00      0.00        49
        1874       0.00      0.00      0.00        27
        1875       0.47      0.39      0.43        61
        1876       0.00      0.00      0.00        48
        1877       0.00      0.00      0.00        22
        1878       0.00      0.00      0.00        39
        1879       0.00      0.00      0.00         7

    accuracy                           0.46      6279
   macro avg       0.18      0.16      0.15      6279
weighted avg       0.43      0.46      0.41      6279

Matthews Correlation Coefficient: 0.431
Macro avg F1: 0.147
Weighted avg F1: 0.414
Micro avg F1: 0.462
Top-3 Accuracy: 0.690
Top-5 Accuracy: 0.771
Micro ROC AUC  = 0.95
Macro ROC AUC (present classes) = 0.91
Classification MAE (in years): 5.71

Fold 0_2 Misclassification Analysis:
Near misses (within 2 years): 515 out of 3379 misclassifications (15.24%)
MAE with outliers: 5.71
MAE without outliers: 4.31 (improvement: 1.40)

5 Worst misclassifications:
Image: data/datasets/public/1870/1879_9wikimedia2.jpg, True: 1879, Predicted: 1820, Error: 59
Image: data/datasets/public/1870/1876_70washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/private/1870/1871_177etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/private/1870/1871_259etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/private/1820/1820_031_Zrzut ekranu 2022-07-26 195637.png, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_13wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1820/1821_580etsy.jpg, True: 1821, Predicted: 1871, Error: 50
Image: data/datasets/private/1870/1871_391etsy.jpg, True: 1871, Predicted: 1821, Error: 50
Image: data/datasets/public/1870/1870_041met.jpg, True: 1870, Predicted: 1820, Error: 50
Image: data/datasets/private/1870/1871_404etsy.jpg, True: 1871, Predicted: 1821, Error: 50
=== Training Alternative Model ===

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 127s - 162ms/step - accuracy: 0.1093 - loss: 4.7559 - val_accuracy: 0.2021 - val_loss: 4.0210 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 69s - 88ms/step - accuracy: 0.1669 - loss: 4.2229 - val_accuracy: 0.2390 - val_loss: 3.8824 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 69s - 88ms/step - accuracy: 0.1980 - loss: 4.0223 - val_accuracy: 0.2712 - val_loss: 3.7211 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 70s - 89ms/step - accuracy: 0.2234 - loss: 3.8913 - val_accuracy: 0.3205 - val_loss: 3.4462 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 69s - 88ms/step - accuracy: 0.2413 - loss: 3.7992 - val_accuracy: 0.3626 - val_loss: 3.5255 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 81s - 103ms/step - accuracy: 0.2515 - loss: 3.7123 - val_accuracy: 0.3521 - val_loss: 3.4054 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 69s - 88ms/step - accuracy: 0.2719 - loss: 3.6633 - val_accuracy: 0.3826 - val_loss: 3.2223 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 69s - 88ms/step - accuracy: 0.2719 - loss: 3.5978 - val_accuracy: 0.3965 - val_loss: 3.0936 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 70s - 89ms/step - accuracy: 0.2817 - loss: 3.5327 - val_accuracy: 0.4145 - val_loss: 3.1032 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 68s - 87ms/step - accuracy: 0.2932 - loss: 3.4964 - val_accuracy: 0.4303 - val_loss: 2.9971 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 69s - 88ms/step - accuracy: 0.3063 - loss: 3.4655 - val_accuracy: 0.4363 - val_loss: 2.9210 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 69s - 88ms/step - accuracy: 0.3023 - loss: 3.4487 - val_accuracy: 0.4330 - val_loss: 2.9310 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 70s - 89ms/step - accuracy: 0.3157 - loss: 3.3901 - val_accuracy: 0.4497 - val_loss: 2.9046 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 69s - 88ms/step - accuracy: 0.3099 - loss: 3.3649 - val_accuracy: 0.4398 - val_loss: 2.8723 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 68s - 87ms/step - accuracy: 0.3136 - loss: 3.3231 - val_accuracy: 0.4408 - val_loss: 2.7683 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 15.
Fold 0_1 Evaluation results: [2.7663872241973877, 0.44076433777809143]
              precision    recall  f1-score   support

        1820       0.54      0.60      0.57       311
        1821       0.84      0.44      0.58       278
        1822       0.00      0.00      0.00         4
        1823       0.00      0.00      0.00         6
        1824       0.00      0.00      0.00         5
        1825       0.00      0.00      0.00        12
        1826       0.00      0.00      0.00        10
        1827       0.61      0.30      0.40       130
        1828       0.00      0.00      0.00         4
        1829       0.00      0.00      0.00        19
        1830       0.39      0.44      0.41       283
        1831       0.49      0.96      0.65       664
        1832       0.56      0.67      0.61       310
        1833       0.85      0.57      0.68        90
        1834       0.38      0.43      0.40       143
        1835       0.00      0.00      0.00        10
        1836       0.00      0.00      0.00        17
        1837       0.00      0.00      0.00        29
        1838       0.00      0.00      0.00        17
        1839       0.00      0.00      0.00         5
        1840       0.49      0.38      0.43       229
        1841       0.39      0.35      0.37       560
        1842       0.00      0.00      0.00        29
        1843       0.00      0.00      0.00        33
        1844       0.00      0.00      0.00         2
        1845       0.00      0.00      0.00         9
        1846       0.00      0.00      0.00        21
        1847       0.00      0.00      0.00         8
        1848       0.00      0.00      0.00        32
        1849       0.00      0.00      0.00        24
        1850       0.32      0.46      0.38       231
        1851       0.54      0.65      0.59       379
        1852       0.00      0.00      0.00        34
        1853       0.00      0.00      0.00        31
        1854       0.00      0.00      0.00        11
        1855       0.09      0.01      0.01       123
        1856       1.00      0.02      0.03        64
        1857       0.28      0.63      0.39       150
        1858       0.00      0.00      0.00        10
        1859       0.00      0.00      0.00        10
        1860       0.19      0.27      0.22       311
        1861       0.48      0.58      0.52       423
        1862       0.26      0.10      0.14        93
        1863       0.43      0.14      0.22        90
        1864       0.02      0.01      0.01        94
        1865       0.00      0.00      0.00        38
        1866       0.00      0.00      0.00        30
        1867       0.00      0.00      0.00        51
        1868       0.00      0.00      0.00        33
        1869       0.00      0.00      0.00        27
        1870       0.35      0.20      0.25       162
        1871       0.44      0.72      0.55       249
        1872       0.00      0.00      0.00        40
        1873       0.00      0.00      0.00        57
        1874       0.00      0.00      0.00        25
        1875       0.33      0.59      0.42        78
        1876       0.00      0.00      0.00        52
        1877       0.00      0.00      0.00        32
        1878       0.00      0.00      0.00        51
        1879       0.00      0.00      0.00         7

    accuracy                           0.44      6280
   macro avg       0.17      0.16      0.15      6280
weighted avg       0.39      0.44      0.39      6280

Matthews Correlation Coefficient: 0.408
Macro avg F1: 0.147
Weighted avg F1: 0.391
Micro avg F1: 0.441
Top-3 Accuracy: 0.700
Top-5 Accuracy: 0.783
Micro ROC AUC  = 0.96
Macro ROC AUC (present classes) = 0.92
Classification MAE (in years): 5.93

Fold 0_1 Misclassification Analysis:
Near misses (within 2 years): 642 out of 3512 misclassifications (18.28%)
MAE with outliers: 5.93
MAE without outliers: 4.23 (improvement: 1.69)

5 Worst misclassifications:
Image: data/datasets/public/1870/1878_030met.jpg, True: 1878, Predicted: 1820, Error: 58
Image: data/datasets/public/1870/1877_033met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1877_035met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1876_463vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_381vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_53washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_409vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_410vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_432vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_408vna.jpg, True: 1876, Predicted: 1820, Error: 56

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 111s - 142ms/step - accuracy: 0.1124 - loss: 4.4585 - val_accuracy: 0.1715 - val_loss: 4.0327 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 70s - 89ms/step - accuracy: 0.1519 - loss: 4.2496 - val_accuracy: 0.2391 - val_loss: 3.9172 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 71s - 91ms/step - accuracy: 0.1844 - loss: 4.0885 - val_accuracy: 0.3053 - val_loss: 3.8366 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 70s - 89ms/step - accuracy: 0.2213 - loss: 3.9239 - val_accuracy: 0.3163 - val_loss: 3.6161 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 69s - 88ms/step - accuracy: 0.2354 - loss: 3.8420 - val_accuracy: 0.3542 - val_loss: 3.5111 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 69s - 88ms/step - accuracy: 0.2441 - loss: 3.7372 - val_accuracy: 0.3875 - val_loss: 3.3333 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 69s - 88ms/step - accuracy: 0.2578 - loss: 3.6779 - val_accuracy: 0.3845 - val_loss: 3.2818 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 70s - 89ms/step - accuracy: 0.2693 - loss: 3.6078 - val_accuracy: 0.4330 - val_loss: 3.1375 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 69s - 88ms/step - accuracy: 0.2780 - loss: 3.5819 - val_accuracy: 0.4303 - val_loss: 3.0627 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 69s - 88ms/step - accuracy: 0.2919 - loss: 3.5142 - val_accuracy: 0.4246 - val_loss: 3.0875 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 70s - 90ms/step - accuracy: 0.2914 - loss: 3.4946 - val_accuracy: 0.4359 - val_loss: 2.9461 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 69s - 88ms/step - accuracy: 0.3025 - loss: 3.4780 - val_accuracy: 0.4361 - val_loss: 2.9050 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 68s - 87ms/step - accuracy: 0.3059 - loss: 3.4262 - val_accuracy: 0.4512 - val_loss: 2.8137 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 68s - 87ms/step - accuracy: 0.3022 - loss: 3.4172 - val_accuracy: 0.4564 - val_loss: 2.8318 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 82s - 105ms/step - accuracy: 0.3158 - loss: 3.3931 - val_accuracy: 0.4362 - val_loss: 2.9037 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 13.
Fold 0_2 Evaluation results: [2.8144142627716064, 0.45118650794029236]
              precision    recall  f1-score   support

        1820       0.48      0.64      0.54       306
        1821       0.86      0.45      0.59       296
        1822       0.00      0.00      0.00        10
        1823       0.00      0.00      0.00         7
        1824       0.00      0.00      0.00         5
        1825       0.00      0.00      0.00         9
        1826       0.00      0.00      0.00        13
        1827       0.50      0.24      0.33       120
        1828       0.00      0.00      0.00        13
        1829       0.00      0.00      0.00        25
        1830       0.34      0.65      0.45       277
        1831       0.58      0.91      0.71       680
        1832       0.79      0.60      0.68       369
        1833       0.84      0.81      0.82       100
        1834       0.46      0.29      0.36       149
        1835       0.00      0.00      0.00        11
        1836       0.00      0.00      0.00        18
        1837       0.00      0.00      0.00        35
        1838       0.00      0.00      0.00        21
        1839       0.00      0.00      0.00         3
        1840       0.50      0.35      0.42       198
        1841       0.35      0.41      0.38       515
        1842       0.00      0.00      0.00        26
        1843       0.00      0.00      0.00        25
        1844       0.00      0.00      0.00         1
        1845       0.00      0.00      0.00         2
        1846       0.00      0.00      0.00        36
        1847       0.00      0.00      0.00        12
        1848       0.00      0.00      0.00        23
        1849       0.00      0.00      0.00        29
        1850       0.42      0.33      0.37       246
        1851       0.53      0.58      0.56       391
        1852       0.00      0.00      0.00        40
        1853       0.00      0.00      0.00        32
        1854       0.00      0.00      0.00        13
        1855       0.11      0.01      0.02       109
        1856       0.00      0.00      0.00        56
        1857       0.34      0.59      0.43       157
        1858       0.00      0.00      0.00        13
        1859       0.00      0.00      0.00        15
        1860       0.19      0.35      0.24       335
        1861       0.48      0.66      0.55       427
        1862       0.00      0.00      0.00        96
        1863       0.30      0.03      0.06        94
        1864       0.00      0.00      0.00        76
        1865       0.00      0.00      0.00        28
        1866       0.00      0.00      0.00        27
        1867       0.00      0.00      0.00        53
        1868       0.00      0.00      0.00        40
        1869       0.00      0.00      0.00        26
        1870       0.27      0.42      0.33       145
        1871       0.42      0.66      0.51       242
        1872       0.00      0.00      0.00        31
        1873       0.00      0.00      0.00        49
        1874       0.00      0.00      0.00        27
        1875       0.30      0.51      0.38        61
        1876       0.00      0.00      0.00        48
        1877       0.00      0.00      0.00        22
        1878       1.00      0.03      0.05        39
        1879       0.00      0.00      0.00         7

    accuracy                           0.45      6279
   macro avg       0.17      0.16      0.15      6279
weighted avg       0.40      0.45      0.40      6279

Matthews Correlation Coefficient: 0.419
Macro avg F1: 0.146
Weighted avg F1: 0.405
Micro avg F1: 0.451
Top-3 Accuracy: 0.699
Top-5 Accuracy: 0.790
Micro ROC AUC  = 0.95
Macro ROC AUC (present classes) = 0.92
Classification MAE (in years): 5.64

Fold 0_2 Misclassification Analysis:
Near misses (within 2 years): 583 out of 3446 misclassifications (16.92%)
MAE with outliers: 5.64
MAE without outliers: 4.23 (improvement: 1.41)

5 Worst misclassifications:
Image: data/datasets/public/1870/1878_034met.jpg, True: 1878, Predicted: 1820, Error: 58
Image: data/datasets/public/1870/1876_442vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_71washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_379vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_407vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_419vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_431vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_72washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_398vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_70washington.jpg, True: 1876, Predicted: 1820, Error: 56

===== Iteration 2/5 =====
=== Training Base Model ===

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 94s - 119ms/step - accuracy: 0.1215 - loss: 4.4903 - val_accuracy: 0.2307 - val_loss: 3.9402 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 72s - 92ms/step - accuracy: 0.1766 - loss: 4.1957 - val_accuracy: 0.2766 - val_loss: 3.8009 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 71s - 90ms/step - accuracy: 0.2072 - loss: 4.0624 - val_accuracy: 0.3088 - val_loss: 3.8082 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 72s - 91ms/step - accuracy: 0.2201 - loss: 3.9503 - val_accuracy: 0.3570 - val_loss: 3.3934 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 71s - 91ms/step - accuracy: 0.2414 - loss: 3.8177 - val_accuracy: 0.3804 - val_loss: 3.3997 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 72s - 91ms/step - accuracy: 0.2709 - loss: 3.7240 - val_accuracy: 0.3900 - val_loss: 3.2570 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 71s - 90ms/step - accuracy: 0.2730 - loss: 3.6632 - val_accuracy: 0.4193 - val_loss: 3.1559 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 71s - 90ms/step - accuracy: 0.2832 - loss: 3.6381 - val_accuracy: 0.4331 - val_loss: 3.1303 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 71s - 91ms/step - accuracy: 0.2945 - loss: 3.5924 - val_accuracy: 0.4296 - val_loss: 2.9895 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 72s - 91ms/step - accuracy: 0.2954 - loss: 3.5151 - val_accuracy: 0.4323 - val_loss: 2.9016 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 69s - 88ms/step - accuracy: 0.3013 - loss: 3.4661 - val_accuracy: 0.4441 - val_loss: 2.8831 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 68s - 87ms/step - accuracy: 0.3142 - loss: 3.4236 - val_accuracy: 0.4400 - val_loss: 2.8567 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 72s - 92ms/step - accuracy: 0.3126 - loss: 3.4001 - val_accuracy: 0.4616 - val_loss: 2.8119 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 72s - 92ms/step - accuracy: 0.3150 - loss: 3.4095 - val_accuracy: 0.4637 - val_loss: 2.7359 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 72s - 91ms/step - accuracy: 0.3160 - loss: 3.3694 - val_accuracy: 0.4779 - val_loss: 2.6999 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 15.
Fold 1_1 Evaluation results: [2.696460008621216, 0.4778662323951721]
              precision    recall  f1-score   support

        1820       0.56      0.66      0.61       296
        1821       0.87      0.58      0.69       289
        1822       0.00      0.00      0.00         6
        1823       0.00      0.00      0.00         9
        1824       0.00      0.00      0.00         6
        1825       0.00      0.00      0.00        11
        1826       0.00      0.00      0.00         8
        1827       0.77      0.29      0.42       118
        1828       0.00      0.00      0.00         6
        1829       0.00      0.00      0.00        21
        1830       0.35      0.48      0.40       282
        1831       0.60      0.91      0.73       673
        1832       0.62      0.68      0.65       338
        1833       0.70      0.85      0.77        88
        1834       0.64      0.13      0.21       143
        1835       0.00      0.00      0.00         8
        1836       0.00      0.00      0.00        18
        1837       0.00      0.00      0.00        38
        1838       0.00      0.00      0.00        23
        1839       0.00      0.00      0.00         3
        1840       0.34      0.57      0.43       220
        1841       0.47      0.51      0.49       552
        1842       0.00      0.00      0.00        21
        1843       0.00      0.00      0.00        37
        1844       0.00      0.00      0.00         1
        1845       0.00      0.00      0.00         4
        1846       0.00      0.00      0.00        27
        1847       0.00      0.00      0.00        11
        1848       0.00      0.00      0.00        23
        1849       0.00      0.00      0.00        26
        1850       0.26      0.56      0.35       227
        1851       0.53      0.69      0.60       380
        1852       0.00      0.00      0.00        40
        1853       0.00      0.00      0.00        25
        1854       0.00      0.00      0.00        14
        1855       0.62      0.04      0.08       119
        1856       0.00      0.00      0.00        59
        1857       0.36      0.50      0.42       149
        1858       0.00      0.00      0.00        13
        1859       0.00      0.00      0.00        16
        1860       0.23      0.35      0.28       328
        1861       0.62      0.64      0.63       436
        1862       0.00      0.00      0.00        98
        1863       0.41      0.08      0.13        90
        1864       0.00      0.00      0.00        86
        1865       0.00      0.00      0.00        31
        1866       0.00      0.00      0.00        33
        1867       0.00      0.00      0.00        56
        1868       0.00      0.00      0.00        37
        1869       0.00      0.00      0.00        27
        1870       0.41      0.28      0.33       162
        1871       0.38      0.73      0.50       239
        1872       0.00      0.00      0.00        36
        1873       0.00      0.00      0.00        51
        1874       0.00      0.00      0.00        22
        1875       0.44      0.31      0.36        77
        1876       1.00      0.16      0.28        49
        1877       0.00      0.00      0.00        27
        1878       0.00      0.00      0.00        40
        1879       0.00      0.00      0.00         7

    accuracy                           0.48      6280
   macro avg       0.19      0.17      0.16      6280
weighted avg       0.43      0.48      0.43      6280

Matthews Correlation Coefficient: 0.448
Macro avg F1: 0.156
Weighted avg F1: 0.428
Micro avg F1: 0.478
Top-3 Accuracy: 0.718
Top-5 Accuracy: 0.792
Micro ROC AUC  = 0.96
Macro ROC AUC (present classes) = 0.93
Classification MAE (in years): 5.13

Fold 1_1 Misclassification Analysis:
Near misses (within 2 years): 605 out of 3279 misclassifications (18.45%)
MAE with outliers: 5.13
MAE without outliers: 3.77 (improvement: 1.36)

5 Worst misclassifications:
Image: data/datasets/public/1870/1876_427vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/private/1870/1871_114etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/private/1820/1820_179etsy.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1820/1820_81etsy.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1870/1871_192etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/private/1820/1821_368etsy.jpg, True: 1821, Predicted: 1871, Error: 50
Image: data/datasets/private/1870/1871_217etsy.jpg, True: 1871, Predicted: 1821, Error: 50
Image: data/datasets/public/1820/1820_16wikimedia2.jpg, True: 1820, Predicted: 1870, Error: 50
Image: data/datasets/public/1820/1820_044met.jpg, True: 1820, Predicted: 1870, Error: 50
Image: data/datasets/public/1870/1870_79wikimedia2.jpg, True: 1870, Predicted: 1820, Error: 50

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 92s - 117ms/step - accuracy: 0.1180 - loss: 4.5960 - val_accuracy: 0.1661 - val_loss: 3.7485 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 87s - 110ms/step - accuracy: 0.1779 - loss: 4.2255 - val_accuracy: 0.2615 - val_loss: 3.7365 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 72s - 91ms/step - accuracy: 0.2019 - loss: 4.0314 - val_accuracy: 0.3026 - val_loss: 3.7891 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 72s - 92ms/step - accuracy: 0.2268 - loss: 3.9467 - val_accuracy: 0.3196 - val_loss: 3.6119 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 82s - 104ms/step - accuracy: 0.2419 - loss: 3.8387 - val_accuracy: 0.3677 - val_loss: 3.4668 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 73s - 93ms/step - accuracy: 0.2618 - loss: 3.7656 - val_accuracy: 0.3953 - val_loss: 3.3037 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 73s - 93ms/step - accuracy: 0.2683 - loss: 3.6835 - val_accuracy: 0.4249 - val_loss: 3.2197 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 71s - 91ms/step - accuracy: 0.2728 - loss: 3.6361 - val_accuracy: 0.4168 - val_loss: 3.1435 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 72s - 92ms/step - accuracy: 0.2874 - loss: 3.5446 - val_accuracy: 0.4462 - val_loss: 3.0183 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 72s - 92ms/step - accuracy: 0.2898 - loss: 3.5259 - val_accuracy: 0.4576 - val_loss: 2.9711 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 72s - 91ms/step - accuracy: 0.3038 - loss: 3.4434 - val_accuracy: 0.4478 - val_loss: 2.9427 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 72s - 91ms/step - accuracy: 0.3054 - loss: 3.4386 - val_accuracy: 0.4634 - val_loss: 2.8195 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 72s - 91ms/step - accuracy: 0.3166 - loss: 3.4055 - val_accuracy: 0.4711 - val_loss: 2.8254 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 72s - 92ms/step - accuracy: 0.3231 - loss: 3.3780 - val_accuracy: 0.4856 - val_loss: 2.7454 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 71s - 90ms/step - accuracy: 0.3150 - loss: 3.3504 - val_accuracy: 0.4502 - val_loss: 2.7624 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 14.
Fold 1_2 Evaluation results: [2.7434072494506836, 0.48558688163757324]
              precision    recall  f1-score   support

        1820       0.60      0.55      0.58       321
        1821       0.81      0.56      0.66       285
        1822       0.00      0.00      0.00         8
        1823       0.00      0.00      0.00         4
        1824       0.00      0.00      0.00         4
        1825       0.00      0.00      0.00        10
        1826       0.00      0.00      0.00        15
        1827       0.67      0.62      0.64       132
        1828       0.00      0.00      0.00        11
        1829       0.00      0.00      0.00        23
        1830       0.42      0.53      0.47       278
        1831       0.61      0.89      0.72       671
        1832       0.70      0.64      0.67       341
        1833       0.65      0.84      0.73       102
        1834       0.45      0.18      0.26       149
        1835       0.00      0.00      0.00        13
        1836       0.00      0.00      0.00        17
        1837       0.00      0.00      0.00        26
        1838       0.00      0.00      0.00        15
        1839       0.00      0.00      0.00         5
        1840       0.36      0.51      0.43       207
        1841       0.48      0.50      0.49       523
        1842       0.00      0.00      0.00        34
        1843       0.00      0.00      0.00        21
        1844       0.00      0.00      0.00         2
        1845       0.00      0.00      0.00         7
        1846       0.00      0.00      0.00        30
        1847       0.00      0.00      0.00         9
        1848       0.00      0.00      0.00        32
        1849       0.00      0.00      0.00        27
        1850       0.34      0.43      0.38       250
        1851       0.57      0.65      0.61       390
        1852       0.00      0.00      0.00        34
        1853       0.00      0.00      0.00        38
        1854       0.00      0.00      0.00        10
        1855       0.30      0.03      0.05       113
        1856       0.00      0.00      0.00        61
        1857       0.32      0.60      0.42       158
        1858       0.00      0.00      0.00        10
        1859       0.00      0.00      0.00         9
        1860       0.23      0.44      0.31       318
        1861       0.51      0.78      0.62       414
        1862       0.00      0.00      0.00        91
        1863       0.83      0.11      0.19        94
        1864       0.00      0.00      0.00        84
        1865       0.00      0.00      0.00        35
        1866       0.00      0.00      0.00        24
        1867       0.00      0.00      0.00        48
        1868       0.00      0.00      0.00        36
        1869       0.00      0.00      0.00        26
        1870       0.28      0.32      0.30       145
        1871       0.42      0.74      0.53       252
        1872       0.00      0.00      0.00        35
        1873       0.00      0.00      0.00        55
        1874       0.00      0.00      0.00        30
        1875       0.33      0.27      0.30        62
        1876       0.78      0.14      0.23        51
        1877       0.00      0.00      0.00        27
        1878       0.00      0.00      0.00        50
        1879       0.00      0.00      0.00         7

    accuracy                           0.49      6279
   macro avg       0.18      0.17      0.16      6279
weighted avg       0.43      0.49      0.44      6279

Matthews Correlation Coefficient: 0.456
Macro avg F1: 0.159
Weighted avg F1: 0.436
Micro avg F1: 0.486
Top-3 Accuracy: 0.720
Top-5 Accuracy: 0.792
Micro ROC AUC  = 0.96
Macro ROC AUC (present classes) = 0.92
Classification MAE (in years): 5.03

Fold 1_2 Misclassification Analysis:
Near misses (within 2 years): 552 out of 3230 misclassifications (17.09%)
MAE with outliers: 5.03
MAE without outliers: 3.75 (improvement: 1.28)

5 Worst misclassifications:
Image: data/datasets/public/1870/1879_39wikimedia2.jpg, True: 1879, Predicted: 1820, Error: 59
Image: data/datasets/public/1870/1877_035met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1877_034met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1876_447vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_53washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_70washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1820/1820_2wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_40_001wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1820/1820_037_Zrzut ekranu 2022-07-26 210308.png, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1870/1871_154etsy.jpg, True: 1871, Predicted: 1820, Error: 51
=== Training Alternative Model ===

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 116s - 147ms/step - accuracy: 0.1085 - loss: 4.6930 - val_accuracy: 0.1844 - val_loss: 3.9056 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 69s - 88ms/step - accuracy: 0.1495 - loss: 4.2876 - val_accuracy: 0.2283 - val_loss: 4.0850 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 68s - 86ms/step - accuracy: 0.1855 - loss: 4.1387 - val_accuracy: 0.2750 - val_loss: 3.7855 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 71s - 90ms/step - accuracy: 0.2176 - loss: 3.9818 - val_accuracy: 0.3344 - val_loss: 3.7034 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 69s - 87ms/step - accuracy: 0.2400 - loss: 3.8367 - val_accuracy: 0.3516 - val_loss: 3.4505 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 70s - 89ms/step - accuracy: 0.2462 - loss: 3.7545 - val_accuracy: 0.3588 - val_loss: 3.3532 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 68s - 87ms/step - accuracy: 0.2706 - loss: 3.6562 - val_accuracy: 0.4068 - val_loss: 3.1642 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 77s - 98ms/step - accuracy: 0.2715 - loss: 3.6091 - val_accuracy: 0.4038 - val_loss: 3.1713 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 78s - 99ms/step - accuracy: 0.2873 - loss: 3.5436 - val_accuracy: 0.4170 - val_loss: 3.0988 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 76s - 97ms/step - accuracy: 0.2956 - loss: 3.5034 - val_accuracy: 0.3978 - val_loss: 3.0630 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 76s - 96ms/step - accuracy: 0.2972 - loss: 3.4656 - val_accuracy: 0.4322 - val_loss: 2.9598 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 77s - 98ms/step - accuracy: 0.3079 - loss: 3.4070 - val_accuracy: 0.4361 - val_loss: 2.9646 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 77s - 99ms/step - accuracy: 0.3101 - loss: 3.3680 - val_accuracy: 0.4349 - val_loss: 2.8885 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 77s - 98ms/step - accuracy: 0.3112 - loss: 3.3561 - val_accuracy: 0.4379 - val_loss: 2.9630 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 76s - 96ms/step - accuracy: 0.3258 - loss: 3.3110 - val_accuracy: 0.4213 - val_loss: 2.9589 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 13.
Fold 1_1 Evaluation results: [2.886788845062256, 0.4348725974559784]
              precision    recall  f1-score   support

        1820       0.45      0.58      0.50       296
        1821       0.84      0.41      0.55       289
        1822       0.00      0.00      0.00         6
        1823       0.00      0.00      0.00         9
        1824       0.00      0.00      0.00         6
        1825       0.00      0.00      0.00        11
        1826       0.00      0.00      0.00         8
        1827       0.56      0.12      0.20       118
        1828       0.00      0.00      0.00         6
        1829       0.00      0.00      0.00        21
        1830       0.39      0.41      0.40       282
        1831       0.55      0.89      0.68       673
        1832       0.58      0.64      0.61       338
        1833       0.87      0.53      0.66        88
        1834       0.41      0.57      0.48       143
        1835       0.00      0.00      0.00         8
        1836       0.00      0.00      0.00        18
        1837       0.00      0.00      0.00        38
        1838       0.00      0.00      0.00        23
        1839       0.00      0.00      0.00         3
        1840       0.56      0.30      0.40       220
        1841       0.46      0.36      0.40       552
        1842       0.00      0.00      0.00        21
        1843       0.00      0.00      0.00        37
        1844       0.00      0.00      0.00         1
        1845       0.00      0.00      0.00         4
        1846       0.00      0.00      0.00        27
        1847       0.00      0.00      0.00        11
        1848       0.00      0.00      0.00        23
        1849       0.00      0.00      0.00        26
        1850       0.29      0.43      0.35       227
        1851       0.52      0.65      0.58       380
        1852       0.00      0.00      0.00        40
        1853       0.00      0.00      0.00        25
        1854       0.00      0.00      0.00        14
        1855       0.24      0.05      0.08       119
        1856       0.00      0.00      0.00        59
        1857       0.33      0.56      0.42       149
        1858       0.00      0.00      0.00        13
        1859       0.00      0.00      0.00        16
        1860       0.18      0.24      0.21       328
        1861       0.37      0.74      0.49       436
        1862       0.00      0.00      0.00        98
        1863       0.67      0.02      0.04        90
        1864       0.21      0.12      0.15        86
        1865       0.00      0.00      0.00        31
        1866       0.00      0.00      0.00        33
        1867       0.00      0.00      0.00        56
        1868       0.00      0.00      0.00        37
        1869       0.00      0.00      0.00        27
        1870       0.36      0.30      0.33       162
        1871       0.35      0.75      0.48       239
        1872       0.00      0.00      0.00        36
        1873       1.00      0.02      0.04        51
        1874       0.00      0.00      0.00        22
        1875       0.36      0.27      0.31        77
        1876       0.38      0.06      0.11        49
        1877       0.00      0.00      0.00        27
        1878       0.00      0.00      0.00        40
        1879       0.00      0.00      0.00         7

    accuracy                           0.43      6280
   macro avg       0.18      0.15      0.14      6280
weighted avg       0.40      0.43      0.38      6280

Matthews Correlation Coefficient: 0.402
Macro avg F1: 0.141
Weighted avg F1: 0.384
Micro avg F1: 0.435
Top-3 Accuracy: 0.694
Top-5 Accuracy: 0.778
Micro ROC AUC  = 0.95
Macro ROC AUC (present classes) = 0.92
Classification MAE (in years): 6.11

Fold 1_1 Misclassification Analysis:
Near misses (within 2 years): 629 out of 3549 misclassifications (17.72%)
MAE with outliers: 6.11
MAE without outliers: 4.18 (improvement: 1.93)

5 Worst misclassifications:
Image: data/datasets/public/1870/1877_033met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1876_72washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_431vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1873_012_001met.jpg, True: 1873, Predicted: 1820, Error: 53
Image: data/datasets/public/1870/1873_039met.jpg, True: 1873, Predicted: 1820, Error: 53
Image: data/datasets/public/1820/1820_22wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1820/1820_034_Zrzut ekranu 2022-07-26 203655.png, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_003_001met.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1820/1820_036_Zrzut ekranu 2022-07-26 205312.png, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1820/1820_159etsy.jpg, True: 1820, Predicted: 1871, Error: 51

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 119s - 152ms/step - accuracy: 0.1146 - loss: 4.5985 - val_accuracy: 0.1937 - val_loss: 4.0563 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 75s - 96ms/step - accuracy: 0.1645 - loss: 4.2501 - val_accuracy: 0.2500 - val_loss: 3.7449 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 75s - 96ms/step - accuracy: 0.1963 - loss: 4.0465 - val_accuracy: 0.2709 - val_loss: 3.7603 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 79s - 100ms/step - accuracy: 0.2215 - loss: 3.9055 - val_accuracy: 0.3513 - val_loss: 3.4776 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 78s - 100ms/step - accuracy: 0.2366 - loss: 3.8242 - val_accuracy: 0.3731 - val_loss: 3.3813 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 77s - 98ms/step - accuracy: 0.2553 - loss: 3.7030 - val_accuracy: 0.3856 - val_loss: 3.2293 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 77s - 98ms/step - accuracy: 0.2624 - loss: 3.6815 - val_accuracy: 0.3942 - val_loss: 3.2564 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 77s - 98ms/step - accuracy: 0.2745 - loss: 3.5958 - val_accuracy: 0.4079 - val_loss: 3.1039 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 77s - 98ms/step - accuracy: 0.2882 - loss: 3.5483 - val_accuracy: 0.3892 - val_loss: 3.1279 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 77s - 98ms/step - accuracy: 0.2893 - loss: 3.5112 - val_accuracy: 0.4150 - val_loss: 2.9970 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 79s - 100ms/step - accuracy: 0.2990 - loss: 3.4843 - val_accuracy: 0.4394 - val_loss: 2.9806 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 77s - 98ms/step - accuracy: 0.2960 - loss: 3.4500 - val_accuracy: 0.4416 - val_loss: 2.8853 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 76s - 97ms/step - accuracy: 0.3124 - loss: 3.4154 - val_accuracy: 0.4408 - val_loss: 2.8447 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 76s - 97ms/step - accuracy: 0.3076 - loss: 3.3776 - val_accuracy: 0.4217 - val_loss: 2.8333 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 76s - 97ms/step - accuracy: 0.3166 - loss: 3.3503 - val_accuracy: 0.4585 - val_loss: 2.7412 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 15.
Fold 1_2 Evaluation results: [2.7387642860412598, 0.4585125148296356]
              precision    recall  f1-score   support

        1820       0.51      0.66      0.58       321
        1821       0.87      0.58      0.69       285
        1822       0.00      0.00      0.00         8
        1823       0.00      0.00      0.00         4
        1824       0.00      0.00      0.00         4
        1825       0.00      0.00      0.00        10
        1826       0.00      0.00      0.00        15
        1827       0.64      0.28      0.39       132
        1828       0.00      0.00      0.00        11
        1829       0.00      0.00      0.00        23
        1830       0.32      0.65      0.43       278
        1831       0.60      0.91      0.72       671
        1832       0.62      0.70      0.65       341
        1833       0.85      0.62      0.72       102
        1834       0.35      0.17      0.23       149
        1835       0.00      0.00      0.00        13
        1836       0.00      0.00      0.00        17
        1837       0.00      0.00      0.00        26
        1838       0.00      0.00      0.00        15
        1839       0.00      0.00      0.00         5
        1840       0.35      0.47      0.40       207
        1841       0.50      0.30      0.37       523
        1842       0.00      0.00      0.00        34
        1843       0.00      0.00      0.00        21
        1844       0.00      0.00      0.00         2
        1845       0.00      0.00      0.00         7
        1846       0.00      0.00      0.00        30
        1847       0.00      0.00      0.00         9
        1848       0.00      0.00      0.00        32
        1849       0.00      0.00      0.00        27
        1850       0.42      0.27      0.33       250
        1851       0.46      0.71      0.56       390
        1852       0.00      0.00      0.00        34
        1853       0.00      0.00      0.00        38
        1854       0.00      0.00      0.00        10
        1855       0.00      0.00      0.00       113
        1856       0.00      0.00      0.00        61
        1857       0.33      0.63      0.43       158
        1858       0.00      0.00      0.00        10
        1859       0.00      0.00      0.00         9
        1860       0.19      0.34      0.24       318
        1861       0.50      0.62      0.55       414
        1862       0.00      0.00      0.00        91
        1863       0.50      0.07      0.13        94
        1864       0.21      0.05      0.08        84
        1865       0.00      0.00      0.00        35
        1866       0.00      0.00      0.00        24
        1867       0.00      0.00      0.00        48
        1868       0.00      0.00      0.00        36
        1869       0.00      0.00      0.00        26
        1870       0.24      0.41      0.30       145
        1871       0.47      0.73      0.57       252
        1872       0.00      0.00      0.00        35
        1873       0.00      0.00      0.00        55
        1874       0.00      0.00      0.00        30
        1875       0.33      0.45      0.38        62
        1876       0.57      0.08      0.14        51
        1877       0.00      0.00      0.00        27
        1878       0.00      0.00      0.00        50
        1879       0.00      0.00      0.00         7

    accuracy                           0.46      6279
   macro avg       0.16      0.16      0.15      6279
weighted avg       0.40      0.46      0.41      6279

Matthews Correlation Coefficient: 0.429
Macro avg F1: 0.148
Weighted avg F1: 0.407
Micro avg F1: 0.459
Top-3 Accuracy: 0.716
Top-5 Accuracy: 0.799
Micro ROC AUC  = 0.96
Macro ROC AUC (present classes) = 0.92
Classification MAE (in years): 5.47

Fold 1_2 Misclassification Analysis:
Near misses (within 2 years): 642 out of 3400 misclassifications (18.88%)
MAE with outliers: 5.47
MAE without outliers: 3.95 (improvement: 1.51)

5 Worst misclassifications:
Image: data/datasets/public/1870/1878_030met.jpg, True: 1878, Predicted: 1820, Error: 58
Image: data/datasets/public/1870/1877_003met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1876_415vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_398vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_70washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_432vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1873_044_001met.jpg, True: 1873, Predicted: 1820, Error: 53
Image: data/datasets/public/1820/1820_157wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1820/1820_115etsy.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_038met.jpg, True: 1820, Predicted: 1871, Error: 51

===== Iteration 3/5 =====
=== Training Base Model ===

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 97s - 123ms/step - accuracy: 0.1306 - loss: 4.5782 - val_accuracy: 0.2189 - val_loss: 4.1576 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 72s - 92ms/step - accuracy: 0.1761 - loss: 4.2324 - val_accuracy: 0.2634 - val_loss: 3.8516 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 72s - 92ms/step - accuracy: 0.2131 - loss: 4.0712 - val_accuracy: 0.2900 - val_loss: 3.7903 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 72s - 92ms/step - accuracy: 0.2242 - loss: 3.9949 - val_accuracy: 0.3377 - val_loss: 3.6443 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 71s - 91ms/step - accuracy: 0.2504 - loss: 3.8506 - val_accuracy: 0.3621 - val_loss: 3.4691 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 71s - 90ms/step - accuracy: 0.2613 - loss: 3.7429 - val_accuracy: 0.3748 - val_loss: 3.3578 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 70s - 89ms/step - accuracy: 0.2693 - loss: 3.6472 - val_accuracy: 0.3955 - val_loss: 3.2373 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 72s - 91ms/step - accuracy: 0.2746 - loss: 3.6406 - val_accuracy: 0.3955 - val_loss: 3.0445 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 70s - 89ms/step - accuracy: 0.2910 - loss: 3.5688 - val_accuracy: 0.4352 - val_loss: 3.0951 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 71s - 91ms/step - accuracy: 0.2996 - loss: 3.5227 - val_accuracy: 0.4417 - val_loss: 2.9460 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 71s - 90ms/step - accuracy: 0.3010 - loss: 3.4753 - val_accuracy: 0.4330 - val_loss: 2.9367 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 71s - 91ms/step - accuracy: 0.3071 - loss: 3.4065 - val_accuracy: 0.4527 - val_loss: 2.8390 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 71s - 91ms/step - accuracy: 0.3015 - loss: 3.4360 - val_accuracy: 0.4645 - val_loss: 2.7843 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 72s - 92ms/step - accuracy: 0.3147 - loss: 3.3505 - val_accuracy: 0.4653 - val_loss: 2.7640 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 71s - 90ms/step - accuracy: 0.3220 - loss: 3.3509 - val_accuracy: 0.4642 - val_loss: 2.7630 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 15.
Fold 2_1 Evaluation results: [2.7606043815612793, 0.46417197585105896]
              precision    recall  f1-score   support

        1820       0.44      0.77      0.56       291
        1821       0.88      0.49      0.63       311
        1822       0.00      0.00      0.00         4
        1823       0.00      0.00      0.00         7
        1824       0.00      0.00      0.00         6
        1825       0.00      0.00      0.00        13
        1826       0.00      0.00      0.00        13
        1827       0.84      0.13      0.23       123
        1828       0.00      0.00      0.00        11
        1829       0.00      0.00      0.00        28
        1830       0.37      0.46      0.41       269
        1831       0.57      0.89      0.70       662
        1832       0.59      0.67      0.63       340
        1833       0.93      0.70      0.80        97
        1834       0.47      0.04      0.08       169
        1835       0.00      0.00      0.00        10
        1836       0.00      0.00      0.00        17
        1837       0.00      0.00      0.00        31
        1838       0.00      0.00      0.00        15
        1839       0.00      0.00      0.00         5
        1840       0.39      0.46      0.42       223
        1841       0.49      0.46      0.47       578
        1842       0.00      0.00      0.00        22
        1843       0.00      0.00      0.00        26
        1844       0.00      0.00      0.00         1
        1845       0.00      0.00      0.00         7
        1846       0.00      0.00      0.00        35
        1847       0.00      0.00      0.00        10
        1848       0.00      0.00      0.00        24
        1849       0.00      0.00      0.00        27
        1850       0.31      0.39      0.35       244
        1851       0.42      0.80      0.55       365
        1852       0.00      0.00      0.00        39
        1853       0.00      0.00      0.00        32
        1854       0.00      0.00      0.00         7
        1855       0.50      0.07      0.12       116
        1856       0.00      0.00      0.00        66
        1857       0.37      0.50      0.42       151
        1858       0.00      0.00      0.00        14
        1859       0.00      0.00      0.00        12
        1860       0.28      0.34      0.31       299
        1861       0.45      0.71      0.55       418
        1862       0.00      0.00      0.00        93
        1863       1.00      0.06      0.12        93
        1864       0.00      0.00      0.00        92
        1865       0.00      0.00      0.00        36
        1866       0.00      0.00      0.00        25
        1867       1.00      0.02      0.04        53
        1868       0.00      0.00      0.00        37
        1869       0.00      0.00      0.00        26
        1870       0.30      0.37      0.33       147
        1871       0.38      0.74      0.50       240
        1872       0.00      0.00      0.00        40
        1873       0.00      0.00      0.00        57
        1874       0.00      0.00      0.00        17
        1875       0.47      0.24      0.32        71
        1876       0.94      0.38      0.54        42
        1877       0.00      0.00      0.00        27
        1878       0.00      0.00      0.00        39
        1879       0.00      0.00      0.00         7

    accuracy                           0.46      6280
   macro avg       0.21      0.16      0.15      6280
weighted avg       0.43      0.46      0.41      6280

Matthews Correlation Coefficient: 0.433
Macro avg F1: 0.151
Weighted avg F1: 0.405
Micro avg F1: 0.464
Top-3 Accuracy: 0.707
Top-5 Accuracy: 0.785
Micro ROC AUC  = 0.95
Macro ROC AUC (present classes) = 0.93
Classification MAE (in years): 5.22

Fold 2_1 Misclassification Analysis:
Near misses (within 2 years): 630 out of 3365 misclassifications (18.72%)
MAE with outliers: 5.22
MAE without outliers: 3.94 (improvement: 1.28)

5 Worst misclassifications:
Image: data/datasets/public/1870/1879_9wikimedia2.jpg, True: 1879, Predicted: 1820, Error: 59
Image: data/datasets/public/1870/1877_033met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1876_376vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_427vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_449vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1872_048met.jpg, True: 1872, Predicted: 1820, Error: 52
Image: data/datasets/private/1820/1820_034_Zrzut ekranu 2022-07-26 203655.png, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_12wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1870/1871_114etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/private/1870/1871_419etsy.jpg, True: 1871, Predicted: 1820, Error: 51

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 93s - 118ms/step - accuracy: 0.1131 - loss: 4.5180 - val_accuracy: 0.2113 - val_loss: 4.0683 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 71s - 91ms/step - accuracy: 0.1648 - loss: 4.2194 - val_accuracy: 0.2056 - val_loss: 3.6868 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 71s - 91ms/step - accuracy: 0.1949 - loss: 4.0985 - val_accuracy: 0.3080 - val_loss: 3.7381 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 72s - 91ms/step - accuracy: 0.2322 - loss: 3.9185 - val_accuracy: 0.3313 - val_loss: 3.7236 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 71s - 90ms/step - accuracy: 0.2350 - loss: 3.8276 - val_accuracy: 0.3849 - val_loss: 3.3782 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 71s - 90ms/step - accuracy: 0.2516 - loss: 3.7317 - val_accuracy: 0.4013 - val_loss: 3.2986 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 78s - 100ms/step - accuracy: 0.2745 - loss: 3.6662 - val_accuracy: 0.4109 - val_loss: 3.2608 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 78s - 100ms/step - accuracy: 0.2694 - loss: 3.6346 - val_accuracy: 0.4061 - val_loss: 3.2043 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 78s - 99ms/step - accuracy: 0.2826 - loss: 3.5891 - val_accuracy: 0.4276 - val_loss: 2.9474 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 76s - 96ms/step - accuracy: 0.2912 - loss: 3.5028 - val_accuracy: 0.4469 - val_loss: 2.9866 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 78s - 99ms/step - accuracy: 0.2959 - loss: 3.4916 - val_accuracy: 0.4474 - val_loss: 2.9284 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 78s - 100ms/step - accuracy: 0.3070 - loss: 3.4418 - val_accuracy: 0.4112 - val_loss: 2.8725 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 78s - 100ms/step - accuracy: 0.3078 - loss: 3.4272 - val_accuracy: 0.4657 - val_loss: 2.8388 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 77s - 99ms/step - accuracy: 0.3111 - loss: 3.4238 - val_accuracy: 0.4461 - val_loss: 2.7759 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 78s - 100ms/step - accuracy: 0.3239 - loss: 3.3532 - val_accuracy: 0.4889 - val_loss: 2.7653 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 15.
Fold 2_2 Evaluation results: [2.766430377960205, 0.4889313578605652]
              precision    recall  f1-score   support

        1820       0.56      0.67      0.61       326
        1821       0.86      0.58      0.70       263
        1822       0.00      0.00      0.00        10
        1823       0.00      0.00      0.00         6
        1824       0.00      0.00      0.00         4
        1825       0.00      0.00      0.00         8
        1826       0.00      0.00      0.00        10
        1827       0.79      0.48      0.60       127
        1828       0.00      0.00      0.00         6
        1829       0.00      0.00      0.00        16
        1830       0.40      0.52      0.45       291
        1831       0.62      0.91      0.73       682
        1832       0.77      0.58      0.66       339
        1833       0.62      0.92      0.74        93
        1834       0.34      0.41      0.38       123
        1835       0.00      0.00      0.00        11
        1836       0.00      0.00      0.00        18
        1837       0.00      0.00      0.00        33
        1838       0.00      0.00      0.00        23
        1839       0.00      0.00      0.00         3
        1840       0.37      0.50      0.43       204
        1841       0.44      0.46      0.45       497
        1842       0.00      0.00      0.00        33
        1843       0.00      0.00      0.00        32
        1844       0.00      0.00      0.00         2
        1845       0.00      0.00      0.00         4
        1846       0.00      0.00      0.00        22
        1847       0.00      0.00      0.00        10
        1848       0.00      0.00      0.00        31
        1849       0.00      0.00      0.00        26
        1850       0.32      0.39      0.35       233
        1851       0.71      0.59      0.65       405
        1852       0.00      0.00      0.00        35
        1853       0.00      0.00      0.00        31
        1854       0.00      0.00      0.00        17
        1855       0.14      0.01      0.02       116
        1856       0.00      0.00      0.00        54
        1857       0.34      0.66      0.45       156
        1858       0.00      0.00      0.00         9
        1859       0.00      0.00      0.00        13
        1860       0.22      0.46      0.29       347
        1861       0.70      0.69      0.69       432
        1862       0.00      0.00      0.00        96
        1863       0.53      0.10      0.17        91
        1864       0.00      0.00      0.00        78
        1865       0.00      0.00      0.00        30
        1866       0.00      0.00      0.00        32
        1867       0.00      0.00      0.00        51
        1868       0.00      0.00      0.00        36
        1869       0.00      0.00      0.00        27
        1870       0.30      0.43      0.35       160
        1871       0.42      0.76      0.54       251
        1872       0.00      0.00      0.00        31
        1873       0.00      0.00      0.00        49
        1874       0.00      0.00      0.00        35
        1875       0.31      0.46      0.37        68
        1876       1.00      0.16      0.27        58
        1877       0.00      0.00      0.00        27
        1878       0.00      0.00      0.00        51
        1879       0.00      0.00      0.00         7

    accuracy                           0.49      6279
   macro avg       0.18      0.18      0.16      6279
weighted avg       0.44      0.49      0.45      6279

Matthews Correlation Coefficient: 0.461
Macro avg F1: 0.165
Weighted avg F1: 0.447
Micro avg F1: 0.489
Top-3 Accuracy: 0.735
Top-5 Accuracy: 0.808
Micro ROC AUC  = 0.96
Macro ROC AUC (present classes) = 0.92
Classification MAE (in years): 5.01

Fold 2_2 Misclassification Analysis:
Near misses (within 2 years): 526 out of 3209 misclassifications (16.39%)
MAE with outliers: 5.01
MAE without outliers: 3.74 (improvement: 1.28)

5 Worst misclassifications:
Image: data/datasets/public/1870/1879_39wikimedia2.jpg, True: 1879, Predicted: 1820, Error: 59
Image: data/datasets/public/1870/1878_494vna.jpg, True: 1878, Predicted: 1820, Error: 58
Image: data/datasets/public/1870/1877_035met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1876_70washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_53washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_441vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/private/1870/1871_147etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/public/1820/1820_025_001met.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1870/1871_86etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/private/1870/1871_217etsy.jpg, True: 1871, Predicted: 1820, Error: 51
=== Training Alternative Model ===

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 129s - 164ms/step - accuracy: 0.1073 - loss: 4.6161 - val_accuracy: 0.2030 - val_loss: 4.1845 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 78s - 100ms/step - accuracy: 0.1613 - loss: 4.3002 - val_accuracy: 0.2462 - val_loss: 3.9903 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 79s - 100ms/step - accuracy: 0.1980 - loss: 4.1179 - val_accuracy: 0.3081 - val_loss: 3.6343 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 77s - 99ms/step - accuracy: 0.2211 - loss: 3.9232 - val_accuracy: 0.3444 - val_loss: 3.4450 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 79s - 100ms/step - accuracy: 0.2418 - loss: 3.7989 - val_accuracy: 0.3385 - val_loss: 3.3053 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 79s - 101ms/step - accuracy: 0.2655 - loss: 3.7237 - val_accuracy: 0.3600 - val_loss: 3.2247 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 79s - 100ms/step - accuracy: 0.2658 - loss: 3.6979 - val_accuracy: 0.3812 - val_loss: 3.1784 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 79s - 101ms/step - accuracy: 0.2776 - loss: 3.6089 - val_accuracy: 0.3779 - val_loss: 3.1073 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 79s - 100ms/step - accuracy: 0.2793 - loss: 3.5784 - val_accuracy: 0.4018 - val_loss: 3.1467 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 79s - 100ms/step - accuracy: 0.3013 - loss: 3.4933 - val_accuracy: 0.4220 - val_loss: 3.0096 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 78s - 99ms/step - accuracy: 0.2965 - loss: 3.4826 - val_accuracy: 0.4057 - val_loss: 3.0155 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 78s - 99ms/step - accuracy: 0.3091 - loss: 3.4278 - val_accuracy: 0.4315 - val_loss: 3.0005 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 78s - 99ms/step - accuracy: 0.3153 - loss: 3.3919 - val_accuracy: 0.4245 - val_loss: 2.9202 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 79s - 100ms/step - accuracy: 0.3050 - loss: 3.3778 - val_accuracy: 0.4164 - val_loss: 2.9275 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 77s - 98ms/step - accuracy: 0.3206 - loss: 3.3490 - val_accuracy: 0.4478 - val_loss: 2.8465 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 15.
Fold 2_1 Evaluation results: [2.8449642658233643, 0.44777071475982666]
              precision    recall  f1-score   support

        1820       0.47      0.61      0.53       291
        1821       0.78      0.64      0.71       311
        1822       0.00      0.00      0.00         4
        1823       0.00      0.00      0.00         7
        1824       0.00      0.00      0.00         6
        1825       0.00      0.00      0.00        13
        1826       0.00      0.00      0.00        13
        1827       0.49      0.35      0.41       123
        1828       0.00      0.00      0.00        11
        1829       0.00      0.00      0.00        28
        1830       0.42      0.46      0.44       269
        1831       0.61      0.88      0.72       662
        1832       0.51      0.72      0.60       340
        1833       0.92      0.70      0.80        97
        1834       0.46      0.34      0.39       169
        1835       0.00      0.00      0.00        10
        1836       0.00      0.00      0.00        17
        1837       0.00      0.00      0.00        31
        1838       0.00      0.00      0.00        15
        1839       0.00      0.00      0.00         5
        1840       0.54      0.29      0.38       223
        1841       0.57      0.26      0.35       578
        1842       0.00      0.00      0.00        22
        1843       0.00      0.00      0.00        26
        1844       0.00      0.00      0.00         1
        1845       0.00      0.00      0.00         7
        1846       0.00      0.00      0.00        35
        1847       0.00      0.00      0.00        10
        1848       0.00      0.00      0.00        24
        1849       0.00      0.00      0.00        27
        1850       0.41      0.30      0.34       244
        1851       0.43      0.70      0.53       365
        1852       0.00      0.00      0.00        39
        1853       0.00      0.00      0.00        32
        1854       0.00      0.00      0.00         7
        1855       0.26      0.22      0.23       116
        1856       0.00      0.00      0.00        66
        1857       0.30      0.44      0.35       151
        1858       0.00      0.00      0.00        14
        1859       0.00      0.00      0.00        12
        1860       0.17      0.33      0.22       299
        1861       0.39      0.68      0.50       418
        1862       0.23      0.08      0.11        93
        1863       0.48      0.17      0.25        93
        1864       0.12      0.11      0.11        92
        1865       0.00      0.00      0.00        36
        1866       0.00      0.00      0.00        25
        1867       0.00      0.00      0.00        53
        1868       0.00      0.00      0.00        37
        1869       0.00      0.00      0.00        26
        1870       0.31      0.26      0.28       147
        1871       0.42      0.77      0.55       240
        1872       0.00      0.00      0.00        40
        1873       0.00      0.00      0.00        57
        1874       0.00      0.00      0.00        17
        1875       0.27      0.56      0.36        71
        1876       0.60      0.21      0.32        42
        1877       0.00      0.00      0.00        27
        1878       0.00      0.00      0.00        39
        1879       0.00      0.00      0.00         7

    accuracy                           0.45      6280
   macro avg       0.17      0.17      0.16      6280
weighted avg       0.41      0.45      0.41      6280

Matthews Correlation Coefficient: 0.418
Macro avg F1: 0.158
Weighted avg F1: 0.405
Micro avg F1: 0.448
Top-3 Accuracy: 0.701
Top-5 Accuracy: 0.786
Micro ROC AUC  = 0.96
Macro ROC AUC (present classes) = 0.93
Classification MAE (in years): 5.62

Fold 2_1 Misclassification Analysis:
Near misses (within 2 years): 696 out of 3468 misclassifications (20.07%)
MAE with outliers: 5.62
MAE without outliers: 4.05 (improvement: 1.57)

5 Worst misclassifications:
Image: data/datasets/public/1870/1879_1476vna.jpg, True: 1879, Predicted: 1820, Error: 59
Image: data/datasets/public/1870/1877_033met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1820/1820_23wikimedia2.jpg, True: 1820, Predicted: 1876, Error: 56
Image: data/datasets/public/1870/1876_72washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_71washington.jpg, True: 1876, Predicted: 1821, Error: 55
Image: data/datasets/private/1820/1820_179etsy.jpg, True: 1820, Predicted: 1875, Error: 55
Image: data/datasets/public/1820/1820_008_001met.jpg, True: 1820, Predicted: 1875, Error: 55
Image: data/datasets/public/1820/1820_050_001met.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_198_001wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_22wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 120s - 153ms/step - accuracy: 0.1158 - loss: 4.5152 - val_accuracy: 0.1924 - val_loss: 4.2631 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 78s - 99ms/step - accuracy: 0.1497 - loss: 4.3624 - val_accuracy: 0.2383 - val_loss: 3.9699 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 77s - 98ms/step - accuracy: 0.1861 - loss: 4.1086 - val_accuracy: 0.2590 - val_loss: 3.8019 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 75s - 96ms/step - accuracy: 0.2210 - loss: 3.9332 - val_accuracy: 0.3204 - val_loss: 3.4994 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 77s - 98ms/step - accuracy: 0.2409 - loss: 3.7806 - val_accuracy: 0.3542 - val_loss: 3.5318 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 76s - 97ms/step - accuracy: 0.2506 - loss: 3.6967 - val_accuracy: 0.3657 - val_loss: 3.3071 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 76s - 97ms/step - accuracy: 0.2662 - loss: 3.6434 - val_accuracy: 0.4115 - val_loss: 3.1240 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 76s - 97ms/step - accuracy: 0.2771 - loss: 3.5665 - val_accuracy: 0.3985 - val_loss: 3.1278 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 76s - 97ms/step - accuracy: 0.2811 - loss: 3.5624 - val_accuracy: 0.4246 - val_loss: 3.0499 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 76s - 96ms/step - accuracy: 0.2939 - loss: 3.4645 - val_accuracy: 0.4236 - val_loss: 3.0151 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 76s - 97ms/step - accuracy: 0.3022 - loss: 3.4448 - val_accuracy: 0.4268 - val_loss: 2.9575 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 76s - 97ms/step - accuracy: 0.3041 - loss: 3.3926 - val_accuracy: 0.4445 - val_loss: 2.8968 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 73s - 93ms/step - accuracy: 0.3037 - loss: 3.3981 - val_accuracy: 0.4571 - val_loss: 2.8891 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 75s - 96ms/step - accuracy: 0.3043 - loss: 3.3744 - val_accuracy: 0.4509 - val_loss: 2.8064 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 75s - 95ms/step - accuracy: 0.3175 - loss: 3.3205 - val_accuracy: 0.4550 - val_loss: 2.8327 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 14.
Fold 2_2 Evaluation results: [2.807493209838867, 0.45086798071861267]
              precision    recall  f1-score   support

        1820       0.63      0.46      0.53       326
        1821       0.83      0.54      0.65       263
        1822       0.00      0.00      0.00        10
        1823       0.00      0.00      0.00         6
        1824       0.00      0.00      0.00         4
        1825       0.00      0.00      0.00         8
        1826       0.00      0.00      0.00        10
        1827       0.50      0.65      0.57       127
        1828       0.00      0.00      0.00         6
        1829       0.00      0.00      0.00        16
        1830       0.49      0.43      0.46       291
        1831       0.57      0.92      0.71       682
        1832       0.65      0.61      0.63       339
        1833       0.74      0.82      0.78        93
        1834       0.26      0.73      0.39       123
        1835       0.00      0.00      0.00        11
        1836       0.00      0.00      0.00        18
        1837       0.00      0.00      0.00        33
        1838       0.00      0.00      0.00        23
        1839       0.00      0.00      0.00         3
        1840       0.61      0.31      0.41       204
        1841       0.37      0.38      0.37       497
        1842       0.00      0.00      0.00        33
        1843       0.00      0.00      0.00        32
        1844       0.00      0.00      0.00         2
        1845       0.00      0.00      0.00         4
        1846       0.00      0.00      0.00        22
        1847       0.00      0.00      0.00        10
        1848       0.00      0.00      0.00        31
        1849       0.00      0.00      0.00        26
        1850       0.36      0.31      0.34       233
        1851       0.49      0.64      0.56       405
        1852       0.00      0.00      0.00        35
        1853       0.00      0.00      0.00        31
        1854       0.00      0.00      0.00        17
        1855       0.27      0.25      0.26       116
        1856       0.73      0.15      0.25        54
        1857       0.29      0.46      0.35       156
        1858       0.00      0.00      0.00         9
        1859       0.00      0.00      0.00        13
        1860       0.21      0.19      0.20       347
        1861       0.45      0.62      0.52       432
        1862       0.00      0.00      0.00        96
        1863       0.50      0.03      0.06        91
        1864       0.09      0.03      0.04        78
        1865       0.00      0.00      0.00        30
        1866       0.00      0.00      0.00        32
        1867       0.00      0.00      0.00        51
        1868       0.00      0.00      0.00        36
        1869       0.00      0.00      0.00        27
        1870       0.27      0.30      0.28       160
        1871       0.35      0.81      0.49       251
        1872       0.00      0.00      0.00        31
        1873       0.00      0.00      0.00        49
        1874       0.00      0.00      0.00        35
        1875       0.24      0.65      0.35        68
        1876       1.00      0.05      0.10        58
        1877       0.00      0.00      0.00        27
        1878       0.00      0.00      0.00        51
        1879       0.00      0.00      0.00         7

    accuracy                           0.45      6279
   macro avg       0.18      0.17      0.15      6279
weighted avg       0.41      0.45      0.40      6279

Matthews Correlation Coefficient: 0.420
Macro avg F1: 0.155
Weighted avg F1: 0.404
Micro avg F1: 0.451
Top-3 Accuracy: 0.692
Top-5 Accuracy: 0.788
Micro ROC AUC  = 0.95
Macro ROC AUC (present classes) = 0.92
Classification MAE (in years): 5.67

Fold 2_2 Misclassification Analysis:
Near misses (within 2 years): 630 out of 3448 misclassifications (18.27%)
MAE with outliers: 5.67
MAE without outliers: 4.06 (improvement: 1.61)

5 Worst misclassifications:
Image: data/datasets/public/1870/1878_498vna.jpg, True: 1878, Predicted: 1820, Error: 58
Image: data/datasets/public/1870/1876_70washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_53washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_410vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_415vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1820/1820_050met.jpg, True: 1820, Predicted: 1875, Error: 55
Image: data/datasets/public/1820/1820_035met.jpg, True: 1820, Predicted: 1875, Error: 55
Image: data/datasets/public/1870/1876_431vna.jpg, True: 1876, Predicted: 1821, Error: 55
Image: data/datasets/public/1870/1873_039met.jpg, True: 1873, Predicted: 1820, Error: 53
Image: data/datasets/public/1870/1873_012_001met.jpg, True: 1873, Predicted: 1820, Error: 53

===== Iteration 4/5 =====
=== Training Base Model ===

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 103s - 131ms/step - accuracy: 0.1268 - loss: 4.4795 - val_accuracy: 0.2218 - val_loss: 4.3083 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 77s - 98ms/step - accuracy: 0.1793 - loss: 4.2008 - val_accuracy: 0.2766 - val_loss: 4.0240 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 77s - 99ms/step - accuracy: 0.2069 - loss: 3.9999 - val_accuracy: 0.3108 - val_loss: 3.7598 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 76s - 97ms/step - accuracy: 0.2403 - loss: 3.8776 - val_accuracy: 0.3670 - val_loss: 3.5903 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 78s - 100ms/step - accuracy: 0.2523 - loss: 3.7844 - val_accuracy: 0.3694 - val_loss: 3.4860 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 78s - 99ms/step - accuracy: 0.2712 - loss: 3.7094 - val_accuracy: 0.3559 - val_loss: 3.2391 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 78s - 99ms/step - accuracy: 0.2848 - loss: 3.6111 - val_accuracy: 0.3920 - val_loss: 3.1625 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 77s - 97ms/step - accuracy: 0.2870 - loss: 3.5557 - val_accuracy: 0.4148 - val_loss: 3.1161 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 77s - 98ms/step - accuracy: 0.3008 - loss: 3.5082 - val_accuracy: 0.4296 - val_loss: 3.0146 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 77s - 98ms/step - accuracy: 0.3067 - loss: 3.4577 - val_accuracy: 0.4406 - val_loss: 2.9404 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 78s - 99ms/step - accuracy: 0.3069 - loss: 3.4393 - val_accuracy: 0.4400 - val_loss: 2.9246 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 78s - 99ms/step - accuracy: 0.3193 - loss: 3.3780 - val_accuracy: 0.4295 - val_loss: 2.9961 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 76s - 97ms/step - accuracy: 0.3215 - loss: 3.3766 - val_accuracy: 0.4561 - val_loss: 2.8386 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 75s - 96ms/step - accuracy: 0.3338 - loss: 3.3112 - val_accuracy: 0.4661 - val_loss: 2.7720 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 78s - 99ms/step - accuracy: 0.3325 - loss: 3.2994 - val_accuracy: 0.4568 - val_loss: 2.7815 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 14.
Fold 3_1 Evaluation results: [2.7752771377563477, 0.4660828113555908]
              precision    recall  f1-score   support

        1820       0.59      0.66      0.62       306
        1821       0.90      0.50      0.65       294
        1822       0.00      0.00      0.00         8
        1823       0.00      0.00      0.00         5
        1824       0.00      0.00      0.00         7
        1825       0.00      0.00      0.00        12
        1826       0.00      0.00      0.00         9
        1827       0.67      0.43      0.52       131
        1828       0.00      0.00      0.00        12
        1829       0.00      0.00      0.00        27
        1830       0.36      0.55      0.44       294
        1831       0.59      0.90      0.71       647
        1832       0.72      0.60      0.65       337
        1833       0.85      0.76      0.80       103
        1834       0.39      0.37      0.38       138
        1835       0.00      0.00      0.00        12
        1836       0.00      0.00      0.00        20
        1837       0.00      0.00      0.00        26
        1838       0.00      0.00      0.00        17
        1839       0.00      0.00      0.00         4
        1840       0.37      0.37      0.37       200
        1841       0.46      0.46      0.46       541
        1842       0.00      0.00      0.00        30
        1843       0.00      0.00      0.00        33
        1844       0.00      0.00      0.00         0
        1845       0.00      0.00      0.00         8
        1846       0.00      0.00      0.00        32
        1847       0.00      0.00      0.00        11
        1848       0.00      0.00      0.00        27
        1849       0.00      0.00      0.00        34
        1850       0.34      0.35      0.34       236
        1851       0.69      0.45      0.55       390
        1852       0.00      0.00      0.00        34
        1853       0.00      0.00      0.00        29
        1854       0.00      0.00      0.00        11
        1855       0.40      0.03      0.06       117
        1856       0.78      0.12      0.20        60
        1857       0.34      0.62      0.44       161
        1858       0.00      0.00      0.00        10
        1859       0.00      0.00      0.00        12
        1860       0.21      0.43      0.28       343
        1861       0.41      0.81      0.55       434
        1862       0.50      0.01      0.02        95
        1863       0.40      0.10      0.16       104
        1864       0.09      0.08      0.08        74
        1865       0.00      0.00      0.00        32
        1866       0.00      0.00      0.00        22
        1867       0.00      0.00      0.00        39
        1868       0.00      0.00      0.00        33
        1869       0.00      0.00      0.00        35
        1870       0.33      0.37      0.35       133
        1871       0.47      0.60      0.53       220
        1872       0.00      0.00      0.00        48
        1873       0.00      0.00      0.00        52
        1874       0.00      0.00      0.00        25
        1875       0.39      0.43      0.41        75
        1876       1.00      0.44      0.61        50
        1877       0.00      0.00      0.00        30
        1878       0.00      0.00      0.00        45
        1879       0.00      0.00      0.00         6

    accuracy                           0.47      6280
   macro avg       0.20      0.17      0.17      6280
weighted avg       0.45      0.47      0.43      6280

Matthews Correlation Coefficient: 0.436
Macro avg F1: 0.170
Weighted avg F1: 0.427
Micro avg F1: 0.466
Top-3 Accuracy: 0.724
Top-5 Accuracy: 0.798
Micro ROC AUC  = 0.95
Macro ROC AUC (present classes) = 0.93
Classification MAE (in years): 5.39

Fold 3_1 Misclassification Analysis:
Near misses (within 2 years): 545 out of 3353 misclassifications (16.25%)
MAE with outliers: 5.39
MAE without outliers: 4.08 (improvement: 1.31)

5 Worst misclassifications:
Image: data/datasets/public/1870/1877_033met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1877_034met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1876_70washington.jpg, True: 1876, Predicted: 1821, Error: 55
Image: data/datasets/private/1870/1871_259etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/private/1820/1820_031_Zrzut ekranu 2022-07-26 195637.png, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_29wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1820/1820_036_Zrzut ekranu 2022-07-26 205312.png, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_13wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1870/1871_415etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/private/1870/1871_154etsy.jpg, True: 1871, Predicted: 1820, Error: 51

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 100s - 127ms/step - accuracy: 0.1193 - loss: 4.6916 - val_accuracy: 0.2376 - val_loss: 4.1374 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 80s - 102ms/step - accuracy: 0.1799 - loss: 4.2359 - val_accuracy: 0.2908 - val_loss: 3.7502 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 79s - 101ms/step - accuracy: 0.1982 - loss: 4.0655 - val_accuracy: 0.3255 - val_loss: 3.6595 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 79s - 101ms/step - accuracy: 0.2261 - loss: 3.9253 - val_accuracy: 0.3681 - val_loss: 3.4002 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 79s - 100ms/step - accuracy: 0.2443 - loss: 3.8451 - val_accuracy: 0.3789 - val_loss: 3.2447 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 76s - 97ms/step - accuracy: 0.2500 - loss: 3.7480 - val_accuracy: 0.4082 - val_loss: 3.2396 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 69s - 88ms/step - accuracy: 0.2619 - loss: 3.6796 - val_accuracy: 0.4287 - val_loss: 3.1281 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 69s - 88ms/step - accuracy: 0.2803 - loss: 3.6036 - val_accuracy: 0.4389 - val_loss: 3.0795 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 69s - 88ms/step - accuracy: 0.2895 - loss: 3.5572 - val_accuracy: 0.4303 - val_loss: 3.0221 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 69s - 88ms/step - accuracy: 0.2896 - loss: 3.5186 - val_accuracy: 0.4642 - val_loss: 2.8503 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 69s - 88ms/step - accuracy: 0.2992 - loss: 3.4571 - val_accuracy: 0.4663 - val_loss: 2.8763 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 69s - 88ms/step - accuracy: 0.3037 - loss: 3.4257 - val_accuracy: 0.4615 - val_loss: 2.8510 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 82s - 105ms/step - accuracy: 0.3072 - loss: 3.3974 - val_accuracy: 0.4778 - val_loss: 2.8177 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 70s - 89ms/step - accuracy: 0.3132 - loss: 3.3725 - val_accuracy: 0.4684 - val_loss: 2.7010 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 69s - 88ms/step - accuracy: 0.3234 - loss: 3.3463 - val_accuracy: 0.4794 - val_loss: 2.6572 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 15.
Fold 3_2 Evaluation results: [2.659069776535034, 0.4793756902217865]
              precision    recall  f1-score   support

        1820       0.62      0.56      0.59       311
        1821       0.77      0.74      0.75       280
        1822       0.00      0.00      0.00         6
        1823       0.00      0.00      0.00         8
        1824       0.00      0.00      0.00         3
        1825       0.00      0.00      0.00         9
        1826       0.00      0.00      0.00        14
        1827       0.67      0.69      0.68       119
        1828       0.00      0.00      0.00         5
        1829       0.00      0.00      0.00        17
        1830       0.27      0.76      0.39       266
        1831       0.67      0.84      0.74       697
        1832       0.67      0.67      0.67       342
        1833       0.81      0.84      0.82        87
        1834       0.49      0.25      0.33       154
        1835       0.00      0.00      0.00         9
        1836       0.00      0.00      0.00        15
        1837       0.00      0.00      0.00        38
        1838       0.00      0.00      0.00        21
        1839       0.00      0.00      0.00         4
        1840       0.53      0.31      0.39       227
        1841       0.49      0.38      0.43       534
        1842       0.00      0.00      0.00        25
        1843       0.00      0.00      0.00        25
        1844       0.00      0.00      0.00         3
        1845       0.00      0.00      0.00         3
        1846       0.00      0.00      0.00        25
        1847       0.00      0.00      0.00         9
        1848       0.00      0.00      0.00        28
        1849       0.00      0.00      0.00        19
        1850       0.31      0.44      0.36       241
        1851       0.41      0.84      0.56       380
        1852       0.00      0.00      0.00        40
        1853       0.00      0.00      0.00        34
        1854       0.00      0.00      0.00        13
        1855       0.40      0.02      0.03       115
        1856       1.00      0.02      0.03        60
        1857       0.35      0.61      0.45       146
        1858       0.00      0.00      0.00        13
        1859       0.00      0.00      0.00        13
        1860       0.21      0.38      0.27       303
        1861       0.58      0.65      0.62       416
        1862       1.00      0.01      0.02        94
        1863       0.39      0.09      0.14        80
        1864       0.00      0.00      0.00        96
        1865       0.00      0.00      0.00        34
        1866       0.00      0.00      0.00        35
        1867       0.00      0.00      0.00        65
        1868       0.00      0.00      0.00        40
        1869       0.00      0.00      0.00        18
        1870       0.40      0.10      0.16       174
        1871       0.45      0.67      0.54       271
        1872       0.00      0.00      0.00        23
        1873       0.00      0.00      0.00        54
        1874       0.00      0.00      0.00        27
        1875       0.45      0.48      0.47        64
        1876       1.00      0.12      0.21        50
        1877       0.00      0.00      0.00        24
        1878       0.00      0.00      0.00        45
        1879       0.00      0.00      0.00         8

    accuracy                           0.48      6279
   macro avg       0.22      0.17      0.16      6279
weighted avg       0.46      0.48      0.43      6279

Matthews Correlation Coefficient: 0.452
Macro avg F1: 0.161
Weighted avg F1: 0.431
Micro avg F1: 0.479
Top-3 Accuracy: 0.724
Top-5 Accuracy: 0.800
Micro ROC AUC  = 0.96
Macro ROC AUC (present classes) = 0.92
Classification MAE (in years): 5.37

Fold 3_2 Misclassification Analysis:
Near misses (within 2 years): 531 out of 3269 misclassifications (16.24%)
MAE with outliers: 5.37
MAE without outliers: 3.98 (improvement: 1.39)

5 Worst misclassifications:
Image: data/datasets/public/1870/1879_9wikimedia2.jpg, True: 1879, Predicted: 1820, Error: 59
Image: data/datasets/public/1870/1879_26wikimedia2.jpg, True: 1879, Predicted: 1820, Error: 59
Image: data/datasets/public/1870/1879_39wikimedia2.jpg, True: 1879, Predicted: 1820, Error: 59
Image: data/datasets/public/1870/1877_035met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1873_034_001met.jpg, True: 1873, Predicted: 1820, Error: 53
Image: data/datasets/private/1870/1871_114etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/private/1820/1820_037_Zrzut ekranu 2022-07-26 210308.png, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1870/1870_79wikimedia2.jpg, True: 1870, Predicted: 1820, Error: 50
Image: data/datasets/public/1870/1870_165wikimedia2.jpg, True: 1870, Predicted: 1820, Error: 50
Image: data/datasets/private/1870/1871_411etsy.jpg, True: 1871, Predicted: 1821, Error: 50
=== Training Alternative Model ===

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 117s - 149ms/step - accuracy: 0.1118 - loss: 4.6216 - val_accuracy: 0.2148 - val_loss: 4.3076 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 78s - 99ms/step - accuracy: 0.1739 - loss: 4.2636 - val_accuracy: 0.2750 - val_loss: 3.9473 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 79s - 101ms/step - accuracy: 0.2050 - loss: 4.0528 - val_accuracy: 0.3046 - val_loss: 3.7013 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 79s - 101ms/step - accuracy: 0.2260 - loss: 3.9023 - val_accuracy: 0.3311 - val_loss: 3.5383 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 78s - 99ms/step - accuracy: 0.2473 - loss: 3.7736 - val_accuracy: 0.3541 - val_loss: 3.4509 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 77s - 98ms/step - accuracy: 0.2629 - loss: 3.7105 - val_accuracy: 0.3661 - val_loss: 3.3464 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 78s - 99ms/step - accuracy: 0.2703 - loss: 3.6326 - val_accuracy: 0.4006 - val_loss: 3.2021 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 77s - 98ms/step - accuracy: 0.2921 - loss: 3.5742 - val_accuracy: 0.4027 - val_loss: 3.1865 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 77s - 98ms/step - accuracy: 0.2886 - loss: 3.5343 - val_accuracy: 0.4064 - val_loss: 3.0420 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 76s - 97ms/step - accuracy: 0.2881 - loss: 3.4913 - val_accuracy: 0.4030 - val_loss: 2.9949 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 78s - 99ms/step - accuracy: 0.3059 - loss: 3.4399 - val_accuracy: 0.4263 - val_loss: 2.9763 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 77s - 98ms/step - accuracy: 0.3106 - loss: 3.4066 - val_accuracy: 0.4272 - val_loss: 2.9986 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 77s - 98ms/step - accuracy: 0.3077 - loss: 3.3945 - val_accuracy: 0.4358 - val_loss: 2.9465 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 77s - 98ms/step - accuracy: 0.3223 - loss: 3.3240 - val_accuracy: 0.4392 - val_loss: 2.9532 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 78s - 99ms/step - accuracy: 0.3223 - loss: 3.3207 - val_accuracy: 0.4309 - val_loss: 2.8986 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 15.
Fold 3_1 Evaluation results: [2.891408681869507, 0.4308917224407196]
              precision    recall  f1-score   support

        1820       0.59      0.43      0.50       306
        1821       0.85      0.37      0.52       294
        1822       0.00      0.00      0.00         8
        1823       0.00      0.00      0.00         5
        1824       0.00      0.00      0.00         7
        1825       0.00      0.00      0.00        12
        1826       0.00      0.00      0.00         9
        1827       0.48      0.40      0.44       131
        1828       0.00      0.00      0.00        12
        1829       0.00      0.00      0.00        27
        1830       0.55      0.33      0.41       294
        1831       0.55      0.84      0.66       647
        1832       0.76      0.58      0.66       337
        1833       0.92      0.52      0.67       103
        1834       0.34      0.67      0.45       138
        1835       0.00      0.00      0.00        12
        1836       0.00      0.00      0.00        20
        1837       0.00      0.00      0.00        26
        1838       0.00      0.00      0.00        17
        1839       0.00      0.00      0.00         4
        1840       0.49      0.36      0.42       200
        1841       0.35      0.47      0.40       541
        1842       0.00      0.00      0.00        30
        1843       0.00      0.00      0.00        33
        1844       0.00      0.00      0.00         0
        1845       0.00      0.00      0.00         8
        1846       0.00      0.00      0.00        32
        1847       0.00      0.00      0.00        11
        1848       0.00      0.00      0.00        27
        1849       0.00      0.00      0.00        34
        1850       0.37      0.36      0.37       236
        1851       0.52      0.64      0.57       390
        1852       0.00      0.00      0.00        34
        1853       0.00      0.00      0.00        29
        1854       0.00      0.00      0.00        11
        1855       0.04      0.01      0.01       117
        1856       1.00      0.02      0.03        60
        1857       0.28      0.64      0.39       161
        1858       0.00      0.00      0.00        10
        1859       0.00      0.00      0.00        12
        1860       0.19      0.30      0.23       343
        1861       0.43      0.66      0.52       434
        1862       0.33      0.01      0.02        95
        1863       0.28      0.05      0.08       104
        1864       0.18      0.14      0.15        74
        1865       0.00      0.00      0.00        32
        1866       0.00      0.00      0.00        22
        1867       0.00      0.00      0.00        39
        1868       0.00      0.00      0.00        33
        1869       0.00      0.00      0.00        35
        1870       0.22      0.49      0.30       133
        1871       0.40      0.68      0.50       220
        1872       0.00      0.00      0.00        48
        1873       0.00      0.00      0.00        52
        1874       0.00      0.00      0.00        25
        1875       0.36      0.53      0.43        75
        1876       0.33      0.10      0.15        50
        1877       0.00      0.00      0.00        30
        1878       0.00      0.00      0.00        45
        1879       0.00      0.00      0.00         6

    accuracy                           0.43      6280
   macro avg       0.18      0.16      0.15      6280
weighted avg       0.41      0.43      0.39      6280

Matthews Correlation Coefficient: 0.399
Macro avg F1: 0.148
Weighted avg F1: 0.392
Micro avg F1: 0.431
Top-3 Accuracy: 0.692
Top-5 Accuracy: 0.786
Micro ROC AUC  = 0.95
Macro ROC AUC (present classes) = 0.93
Classification MAE (in years): 5.87

Fold 3_1 Misclassification Analysis:
Near misses (within 2 years): 626 out of 3574 misclassifications (17.52%)
MAE with outliers: 5.87
MAE without outliers: 4.22 (improvement: 1.65)

5 Worst misclassifications:
Image: data/datasets/public/1870/1877_033met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1876_53washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_410vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/private/1820/1820_180etsy.jpg, True: 1820, Predicted: 1875, Error: 55
Image: data/datasets/public/1820/1820_050met.jpg, True: 1820, Predicted: 1875, Error: 55
Image: data/datasets/private/1820/1821_031_Zrzut ekranu 2022-07-26 200114.png, True: 1821, Predicted: 1876, Error: 55
Image: data/datasets/public/1870/1876_70washington.jpg, True: 1876, Predicted: 1821, Error: 55
Image: data/datasets/public/1820/1820_008_001met.jpg, True: 1820, Predicted: 1875, Error: 55
Image: data/datasets/private/1820/1824_032_Zrzut ekranu 2022-07-26 203037.png, True: 1824, Predicted: 1876, Error: 52
Image: data/datasets/public/1820/1820_40_001wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 120s - 153ms/step - accuracy: 0.0932 - loss: 4.6753 - val_accuracy: 0.1671 - val_loss: 4.4548 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 77s - 98ms/step - accuracy: 0.1516 - loss: 4.3254 - val_accuracy: 0.2480 - val_loss: 4.1379 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 79s - 101ms/step - accuracy: 0.1881 - loss: 4.1001 - val_accuracy: 0.3114 - val_loss: 3.8133 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 77s - 98ms/step - accuracy: 0.2186 - loss: 3.9331 - val_accuracy: 0.3141 - val_loss: 3.5891 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 77s - 98ms/step - accuracy: 0.2298 - loss: 3.8154 - val_accuracy: 0.3418 - val_loss: 3.3968 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 77s - 98ms/step - accuracy: 0.2529 - loss: 3.7314 - val_accuracy: 0.3803 - val_loss: 3.3241 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 71s - 91ms/step - accuracy: 0.2611 - loss: 3.6634 - val_accuracy: 0.3797 - val_loss: 3.2497 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 77s - 98ms/step - accuracy: 0.2737 - loss: 3.6164 - val_accuracy: 0.4171 - val_loss: 3.1523 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 77s - 98ms/step - accuracy: 0.2885 - loss: 3.5672 - val_accuracy: 0.4265 - val_loss: 3.0361 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 77s - 98ms/step - accuracy: 0.2818 - loss: 3.5114 - val_accuracy: 0.4386 - val_loss: 2.9590 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 78s - 100ms/step - accuracy: 0.2949 - loss: 3.4736 - val_accuracy: 0.4192 - val_loss: 2.9941 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 78s - 100ms/step - accuracy: 0.2976 - loss: 3.4397 - val_accuracy: 0.4563 - val_loss: 2.9121 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 78s - 99ms/step - accuracy: 0.3045 - loss: 3.4325 - val_accuracy: 0.4380 - val_loss: 2.9675 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 77s - 98ms/step - accuracy: 0.3204 - loss: 3.3438 - val_accuracy: 0.4572 - val_loss: 2.7424 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 76s - 97ms/step - accuracy: 0.3080 - loss: 3.3760 - val_accuracy: 0.4620 - val_loss: 2.7762 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 14.
Fold 3_2 Evaluation results: [2.7388315200805664, 0.45723840594291687]
              precision    recall  f1-score   support

        1820       0.47      0.54      0.50       311
        1821       0.66      0.62      0.64       280
        1822       0.00      0.00      0.00         6
        1823       0.00      0.00      0.00         8
        1824       0.00      0.00      0.00         3
        1825       0.00      0.00      0.00         9
        1826       0.00      0.00      0.00        14
        1827       0.61      0.32      0.42       119
        1828       0.00      0.00      0.00         5
        1829       0.00      0.00      0.00        17
        1830       0.28      0.70      0.40       266
        1831       0.61      0.90      0.73       697
        1832       0.68      0.59      0.63       342
        1833       0.82      0.72      0.77        87
        1834       0.51      0.29      0.37       154
        1835       0.00      0.00      0.00         9
        1836       0.00      0.00      0.00        15
        1837       0.00      0.00      0.00        38
        1838       0.00      0.00      0.00        21
        1839       0.00      0.00      0.00         4
        1840       0.56      0.36      0.44       227
        1841       0.40      0.38      0.39       534
        1842       0.00      0.00      0.00        25
        1843       0.00      0.00      0.00        25
        1844       0.00      0.00      0.00         3
        1845       0.00      0.00      0.00         3
        1846       0.00      0.00      0.00        25
        1847       0.00      0.00      0.00         9
        1848       0.00      0.00      0.00        28
        1849       0.00      0.00      0.00        19
        1850       0.31      0.49      0.38       241
        1851       0.58      0.63      0.60       380
        1852       0.00      0.00      0.00        40
        1853       0.00      0.00      0.00        34
        1854       0.00      0.00      0.00        13
        1855       0.27      0.05      0.09       115
        1856       0.00      0.00      0.00        60
        1857       0.36      0.56      0.44       146
        1858       0.00      0.00      0.00        13
        1859       0.00      0.00      0.00        13
        1860       0.17      0.39      0.23       303
        1861       0.60      0.54      0.57       416
        1862       1.00      0.01      0.02        94
        1863       0.43      0.12      0.19        80
        1864       0.20      0.01      0.02        96
        1865       0.00      0.00      0.00        34
        1866       0.00      0.00      0.00        35
        1867       0.00      0.00      0.00        65
        1868       0.00      0.00      0.00        40
        1869       0.00      0.00      0.00        18
        1870       0.32      0.40      0.35       174
        1871       0.57      0.65      0.61       271
        1872       0.00      0.00      0.00        23
        1873       0.00      0.00      0.00        54
        1874       0.00      0.00      0.00        27
        1875       0.33      0.47      0.39        64
        1876       0.00      0.00      0.00        50
        1877       0.00      0.00      0.00        24
        1878       0.00      0.00      0.00        45
        1879       0.00      0.00      0.00         8

    accuracy                           0.46      6279
   macro avg       0.18      0.16      0.15      6279
weighted avg       0.43      0.46      0.42      6279

Matthews Correlation Coefficient: 0.427
Macro avg F1: 0.153
Weighted avg F1: 0.418
Micro avg F1: 0.457
Top-3 Accuracy: 0.713
Top-5 Accuracy: 0.794
Micro ROC AUC  = 0.96
Macro ROC AUC (present classes) = 0.92
Classification MAE (in years): 5.73

Fold 3_2 Misclassification Analysis:
Near misses (within 2 years): 592 out of 3408 misclassifications (17.37%)
MAE with outliers: 5.73
MAE without outliers: 4.07 (improvement: 1.67)

5 Worst misclassifications:
Image: data/datasets/public/1870/1878_034met.jpg, True: 1878, Predicted: 1820, Error: 58
Image: data/datasets/public/1870/1877_035met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1876_399vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_71washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_72washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1874_029met.jpg, True: 1874, Predicted: 1820, Error: 54
Image: data/datasets/public/1870/1874_016met.jpg, True: 1874, Predicted: 1820, Error: 54
Image: data/datasets/public/1870/1873_041met.jpg, True: 1873, Predicted: 1820, Error: 53
Image: data/datasets/private/1870/1871_454etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/public/1820/1820_161_001wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51

===== Iteration 5/5 =====
=== Training Base Model ===

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 105s - 134ms/step - accuracy: 0.1094 - loss: 4.7846 - val_accuracy: 0.2228 - val_loss: 4.0223 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 79s - 101ms/step - accuracy: 0.1723 - loss: 4.2440 - val_accuracy: 0.2882 - val_loss: 3.8097 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 79s - 101ms/step - accuracy: 0.2016 - loss: 4.0719 - val_accuracy: 0.3326 - val_loss: 3.6287 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 78s - 100ms/step - accuracy: 0.2171 - loss: 3.9814 - val_accuracy: 0.3430 - val_loss: 3.4431 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 80s - 101ms/step - accuracy: 0.2317 - loss: 3.8552 - val_accuracy: 0.3847 - val_loss: 3.3783 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 79s - 100ms/step - accuracy: 0.2518 - loss: 3.7928 - val_accuracy: 0.3830 - val_loss: 3.2351 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 78s - 100ms/step - accuracy: 0.2658 - loss: 3.7188 - val_accuracy: 0.4215 - val_loss: 3.2271 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 79s - 101ms/step - accuracy: 0.2736 - loss: 3.6330 - val_accuracy: 0.4239 - val_loss: 3.1430 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 78s - 100ms/step - accuracy: 0.2742 - loss: 3.6285 - val_accuracy: 0.4360 - val_loss: 2.9337 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 78s - 100ms/step - accuracy: 0.2883 - loss: 3.5711 - val_accuracy: 0.4561 - val_loss: 2.9430 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 78s - 100ms/step - accuracy: 0.2956 - loss: 3.4967 - val_accuracy: 0.4465 - val_loss: 2.8012 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 77s - 99ms/step - accuracy: 0.2997 - loss: 3.4603 - val_accuracy: 0.4459 - val_loss: 2.7887 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 74s - 95ms/step - accuracy: 0.3059 - loss: 3.4178 - val_accuracy: 0.4799 - val_loss: 2.7551 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 73s - 93ms/step - accuracy: 0.3160 - loss: 3.3988 - val_accuracy: 0.4717 - val_loss: 2.7338 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 74s - 95ms/step - accuracy: 0.3243 - loss: 3.3552 - val_accuracy: 0.4729 - val_loss: 2.6686 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 15.
Fold 4_1 Evaluation results: [2.668154001235962, 0.4729299247264862]
              precision    recall  f1-score   support

        1820       0.54      0.59      0.56       312
        1821       0.79      0.62      0.70       277
        1822       0.00      0.00      0.00         9
        1823       0.00      0.00      0.00         5
        1824       0.00      0.00      0.00         2
        1825       0.00      0.00      0.00        10
        1826       0.00      0.00      0.00        11
        1827       0.64      0.07      0.13       129
        1828       0.00      0.00      0.00         3
        1829       0.00      0.00      0.00        24
        1830       0.31      0.71      0.43       270
        1831       0.66      0.82      0.73       705
        1832       0.82      0.61      0.70       355
        1833       0.83      0.70      0.76        86
        1834       0.83      0.07      0.13       141
        1835       0.00      0.00      0.00        14
        1836       0.00      0.00      0.00        15
        1837       0.00      0.00      0.00        32
        1838       0.00      0.00      0.00        20
        1839       0.00      0.00      0.00         4
        1840       0.38      0.45      0.41       199
        1841       0.40      0.60      0.48       530
        1842       0.00      0.00      0.00        26
        1843       0.00      0.00      0.00        27
        1844       0.00      0.00      0.00         0
        1845       0.00      0.00      0.00         4
        1846       0.00      0.00      0.00        22
        1847       0.00      0.00      0.00         6
        1848       0.00      0.00      0.00        31
        1849       0.00      0.00      0.00        28
        1850       0.33      0.31      0.32       258
        1851       0.55      0.67      0.60       395
        1852       0.00      0.00      0.00        39
        1853       0.00      0.00      0.00        34
        1854       0.00      0.00      0.00        11
        1855       0.00      0.00      0.00       124
        1856       1.00      0.02      0.03        59
        1857       0.38      0.57      0.45       167
        1858       0.00      0.00      0.00        13
        1859       0.00      0.00      0.00         8
        1860       0.20      0.48      0.28       307
        1861       0.54      0.69      0.61       441
        1862       0.00      0.00      0.00        91
        1863       0.53      0.22      0.31        89
        1864       0.17      0.05      0.08        82
        1865       0.00      0.00      0.00        38
        1866       0.00      0.00      0.00        24
        1867       0.00      0.00      0.00        47
        1868       0.00      0.00      0.00        36
        1869       0.00      0.00      0.00        24
        1870       0.43      0.15      0.22       158
        1871       0.41      0.60      0.49       231
        1872       0.00      0.00      0.00        35
        1873       0.00      0.00      0.00        41
        1874       0.00      0.00      0.00        25
        1875       0.42      0.51      0.46        69
        1876       0.93      0.50      0.65        50
        1877       0.00      0.00      0.00        33
        1878       0.00      0.00      0.00        49
        1879       0.00      0.00      0.00         5

    accuracy                           0.47      6280
   macro avg       0.20      0.17      0.16      6280
weighted avg       0.45      0.47      0.43      6280

Matthews Correlation Coefficient: 0.442
Macro avg F1: 0.159
Weighted avg F1: 0.428
Micro avg F1: 0.473
Top-3 Accuracy: 0.725
Top-5 Accuracy: 0.805
Micro ROC AUC  = 0.96
Macro ROC AUC (present classes) = 0.93
Classification MAE (in years): 5.10

Fold 4_1 Misclassification Analysis:
Near misses (within 2 years): 581 out of 3310 misclassifications (17.55%)
MAE with outliers: 5.10
MAE without outliers: 3.87 (improvement: 1.23)

5 Worst misclassifications:
Image: data/datasets/public/1870/1879_39wikimedia2.jpg, True: 1879, Predicted: 1820, Error: 59
Image: data/datasets/public/1870/1877_035met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1876_53washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_70washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/private/1870/1871_173etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/private/1820/1825_035_Zrzut ekranu 2022-07-26 204338.png, True: 1825, Predicted: 1876, Error: 51
Image: data/datasets/private/1870/1871_259etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/private/1870/1871_189etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/private/1870/1871_195etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/public/1820/1820_13wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 99s - 126ms/step - accuracy: 0.1291 - loss: 4.5028 - val_accuracy: 0.1918 - val_loss: 4.2014 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 77s - 98ms/step - accuracy: 0.1750 - loss: 4.1795 - val_accuracy: 0.2416 - val_loss: 3.9115 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 85s - 108ms/step - accuracy: 0.2005 - loss: 4.0424 - val_accuracy: 0.2873 - val_loss: 3.7564 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 70s - 89ms/step - accuracy: 0.2334 - loss: 3.9033 - val_accuracy: 0.3395 - val_loss: 3.6794 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 70s - 89ms/step - accuracy: 0.2439 - loss: 3.8094 - val_accuracy: 0.3295 - val_loss: 3.4028 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 70s - 89ms/step - accuracy: 0.2494 - loss: 3.7045 - val_accuracy: 0.3800 - val_loss: 3.3024 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 70s - 89ms/step - accuracy: 0.2734 - loss: 3.6314 - val_accuracy: 0.3876 - val_loss: 3.2415 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 69s - 88ms/step - accuracy: 0.2801 - loss: 3.5657 - val_accuracy: 0.3896 - val_loss: 3.1624 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 70s - 89ms/step - accuracy: 0.2839 - loss: 3.5165 - val_accuracy: 0.4212 - val_loss: 3.0853 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 84s - 107ms/step - accuracy: 0.2968 - loss: 3.4730 - val_accuracy: 0.4082 - val_loss: 3.0840 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 74s - 94ms/step - accuracy: 0.3030 - loss: 3.4369 - val_accuracy: 0.4329 - val_loss: 2.9712 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 78s - 100ms/step - accuracy: 0.3048 - loss: 3.4158 - val_accuracy: 0.4474 - val_loss: 2.8688 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 78s - 100ms/step - accuracy: 0.3127 - loss: 3.3562 - val_accuracy: 0.4349 - val_loss: 2.8466 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 78s - 99ms/step - accuracy: 0.3129 - loss: 3.3465 - val_accuracy: 0.4498 - val_loss: 2.8734 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 77s - 98ms/step - accuracy: 0.3228 - loss: 3.2818 - val_accuracy: 0.4588 - val_loss: 2.7703 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 15.
Fold 4_2 Evaluation results: [2.770965814590454, 0.4588310122489929]
              precision    recall  f1-score   support

        1820       0.52      0.72      0.60       305
        1821       0.85      0.66      0.75       297
        1822       0.00      0.00      0.00         5
        1823       0.00      0.00      0.00         8
        1824       0.00      0.00      0.00         8
        1825       0.00      0.00      0.00        11
        1826       0.00      0.00      0.00        12
        1827       0.65      0.25      0.36       121
        1828       0.00      0.00      0.00        14
        1829       0.00      0.00      0.00        20
        1830       0.31      0.61      0.41       290
        1831       0.53      0.93      0.68       639
        1832       0.50      0.63      0.56       324
        1833       0.90      0.70      0.79       104
        1834       0.42      0.13      0.19       151
        1835       0.00      0.00      0.00         7
        1836       0.00      0.00      0.00        20
        1837       0.00      0.00      0.00        32
        1838       0.00      0.00      0.00        18
        1839       0.00      0.00      0.00         4
        1840       0.45      0.46      0.45       228
        1841       0.55      0.23      0.33       545
        1842       0.00      0.00      0.00        29
        1843       0.00      0.00      0.00        31
        1844       0.00      0.00      0.00         3
        1845       0.00      0.00      0.00         7
        1846       0.00      0.00      0.00        35
        1847       0.00      0.00      0.00        14
        1848       0.00      0.00      0.00        24
        1849       0.00      0.00      0.00        25
        1850       0.32      0.35      0.33       219
        1851       0.60      0.64      0.62       375
        1852       0.00      0.00      0.00        35
        1853       0.00      0.00      0.00        29
        1854       0.00      0.00      0.00        13
        1855       0.50      0.03      0.05       108
        1856       0.00      0.00      0.00        61
        1857       0.31      0.59      0.40       140
        1858       0.00      0.00      0.00        10
        1859       0.00      0.00      0.00        17
        1860       0.24      0.44      0.31       339
        1861       0.64      0.70      0.67       409
        1862       0.00      0.00      0.00        98
        1863       0.39      0.07      0.12        95
        1864       0.00      0.00      0.00        88
        1865       0.00      0.00      0.00        28
        1866       0.00      0.00      0.00        33
        1867       0.00      0.00      0.00        57
        1868       0.00      0.00      0.00        37
        1869       0.00      0.00      0.00        29
        1870       0.22      0.34      0.27       149
        1871       0.35      0.85      0.50       260
        1872       0.00      0.00      0.00        36
        1873       0.00      0.00      0.00        65
        1874       0.00      0.00      0.00        27
        1875       0.31      0.23      0.26        70
        1876       1.00      0.12      0.21        50
        1877       0.00      0.00      0.00        21
        1878       0.00      0.00      0.00        41
        1879       0.00      0.00      0.00         9

    accuracy                           0.46      6279
   macro avg       0.18      0.16      0.15      6279
weighted avg       0.42      0.46      0.40      6279

Matthews Correlation Coefficient: 0.431
Macro avg F1: 0.148
Weighted avg F1: 0.404
Micro avg F1: 0.459
Top-3 Accuracy: 0.722
Top-5 Accuracy: 0.794
Micro ROC AUC  = 0.95
Macro ROC AUC (present classes) = 0.92
Classification MAE (in years): 5.53

Fold 4_2 Misclassification Analysis:
Near misses (within 2 years): 596 out of 3398 misclassifications (17.54%)
MAE with outliers: 5.53
MAE without outliers: 3.94 (improvement: 1.59)

5 Worst misclassifications:
Image: data/datasets/public/1870/1879_1476vna.jpg, True: 1879, Predicted: 1820, Error: 59
Image: data/datasets/public/1870/1879_9wikimedia2.jpg, True: 1879, Predicted: 1820, Error: 59
Image: data/datasets/public/1870/1879_26wikimedia2.jpg, True: 1879, Predicted: 1820, Error: 59
Image: data/datasets/public/1870/1876_377vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/private/1870/1871_154etsy.jpg, True: 1871, Predicted: 1820, Error: 51
Image: data/datasets/private/1820/1820_034_Zrzut ekranu 2022-07-26 203655.png, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1820/1820_038_Zrzut ekranu 2022-07-26 210339.png, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_12wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1820/1820_037_Zrzut ekranu 2022-07-26 210308.png, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1870/1871_114etsy.jpg, True: 1871, Predicted: 1820, Error: 51
=== Training Alternative Model ===

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 125s - 160ms/step - accuracy: 0.0965 - loss: 4.8068 - val_accuracy: 0.2038 - val_loss: 4.2235 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 77s - 98ms/step - accuracy: 0.1573 - loss: 4.3259 - val_accuracy: 0.2720 - val_loss: 4.0808 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 76s - 97ms/step - accuracy: 0.1892 - loss: 4.1334 - val_accuracy: 0.2906 - val_loss: 3.8096 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 76s - 96ms/step - accuracy: 0.2053 - loss: 3.9992 - val_accuracy: 0.3315 - val_loss: 3.5431 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 75s - 96ms/step - accuracy: 0.2360 - loss: 3.8907 - val_accuracy: 0.3573 - val_loss: 3.4536 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 75s - 96ms/step - accuracy: 0.2414 - loss: 3.7961 - val_accuracy: 0.3774 - val_loss: 3.2653 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 83s - 106ms/step - accuracy: 0.2529 - loss: 3.7199 - val_accuracy: 0.3912 - val_loss: 3.1495 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 74s - 95ms/step - accuracy: 0.2704 - loss: 3.6478 - val_accuracy: 0.4078 - val_loss: 3.0331 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 74s - 95ms/step - accuracy: 0.2698 - loss: 3.6265 - val_accuracy: 0.4032 - val_loss: 3.0309 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 76s - 97ms/step - accuracy: 0.2899 - loss: 3.5600 - val_accuracy: 0.4255 - val_loss: 3.0210 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 75s - 95ms/step - accuracy: 0.2894 - loss: 3.5055 - val_accuracy: 0.4288 - val_loss: 2.9597 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 74s - 95ms/step - accuracy: 0.3026 - loss: 3.4719 - val_accuracy: 0.4409 - val_loss: 2.8499 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 75s - 96ms/step - accuracy: 0.2943 - loss: 3.4667 - val_accuracy: 0.4567 - val_loss: 2.7758 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 74s - 94ms/step - accuracy: 0.3064 - loss: 3.4005 - val_accuracy: 0.4497 - val_loss: 2.8521 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 75s - 95ms/step - accuracy: 0.3150 - loss: 3.3852 - val_accuracy: 0.4596 - val_loss: 2.7559 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 15.
Fold 4_1 Evaluation results: [2.7596194744110107, 0.45955413579940796]
              precision    recall  f1-score   support

        1820       0.49      0.60      0.54       312
        1821       0.75      0.66      0.70       277
        1822       0.00      0.00      0.00         9
        1823       0.00      0.00      0.00         5
        1824       0.00      0.00      0.00         2
        1825       0.00      0.00      0.00        10
        1826       0.00      0.00      0.00        11
        1827       0.58      0.40      0.47       129
        1828       0.00      0.00      0.00         3
        1829       0.00      0.00      0.00        24
        1830       0.44      0.51      0.47       270
        1831       0.65      0.80      0.72       705
        1832       0.86      0.56      0.68       355
        1833       0.58      0.88      0.70        86
        1834       0.39      0.73      0.51       141
        1835       0.00      0.00      0.00        14
        1836       0.00      0.00      0.00        15
        1837       0.00      0.00      0.00        32
        1838       0.00      0.00      0.00        20
        1839       0.00      0.00      0.00         4
        1840       0.46      0.45      0.45       199
        1841       0.40      0.42      0.41       530
        1842       0.00      0.00      0.00        26
        1843       0.00      0.00      0.00        27
        1844       0.00      0.00      0.00         0
        1845       0.00      0.00      0.00         4
        1846       0.00      0.00      0.00        22
        1847       0.00      0.00      0.00         6
        1848       0.00      0.00      0.00        31
        1849       0.00      0.00      0.00        28
        1850       0.47      0.19      0.27       258
        1851       0.45      0.67      0.54       395
        1852       0.00      0.00      0.00        39
        1853       0.00      0.00      0.00        34
        1854       0.00      0.00      0.00        11
        1855       0.40      0.08      0.13       124
        1856       0.00      0.00      0.00        59
        1857       0.35      0.65      0.46       167
        1858       0.00      0.00      0.00        13
        1859       0.00      0.00      0.00         8
        1860       0.17      0.41      0.24       307
        1861       0.50      0.52      0.51       441
        1862       0.00      0.00      0.00        91
        1863       0.41      0.22      0.29        89
        1864       0.08      0.07      0.08        82
        1865       0.00      0.00      0.00        38
        1866       0.00      0.00      0.00        24
        1867       0.00      0.00      0.00        47
        1868       0.00      0.00      0.00        36
        1869       0.00      0.00      0.00        24
        1870       0.58      0.22      0.32       158
        1871       0.43      0.78      0.56       231
        1872       0.00      0.00      0.00        35
        1873       0.06      0.02      0.03        41
        1874       0.00      0.00      0.00        25
        1875       0.27      0.51      0.35        69
        1876       0.86      0.12      0.21        50
        1877       0.00      0.00      0.00        33
        1878       0.00      0.00      0.00        49
        1879       0.00      0.00      0.00         5

    accuracy                           0.46      6280
   macro avg       0.18      0.17      0.16      6280
weighted avg       0.43      0.46      0.42      6280

Matthews Correlation Coefficient: 0.430
Macro avg F1: 0.161
Weighted avg F1: 0.423
Micro avg F1: 0.460
Top-3 Accuracy: 0.711
Top-5 Accuracy: 0.794
Micro ROC AUC  = 0.96
Macro ROC AUC (present classes) = 0.93
Classification MAE (in years): 5.39

Fold 4_1 Misclassification Analysis:
Near misses (within 2 years): 636 out of 3394 misclassifications (18.74%)
MAE with outliers: 5.39
MAE without outliers: 4.07 (improvement: 1.33)

5 Worst misclassifications:
Image: data/datasets/public/1870/1877_033met.jpg, True: 1877, Predicted: 1820, Error: 57
Image: data/datasets/public/1870/1876_410vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_428vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_71washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_53washington.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_407vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_431vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_463vna.jpg, True: 1876, Predicted: 1820, Error: 56
Image: data/datasets/public/1870/1876_70washington.jpg, True: 1876, Predicted: 1821, Error: 55
Image: data/datasets/public/1820/1820_038_001met.jpg, True: 1820, Predicted: 1875, Error: 55

===== Fine Tuning 7 layers! =====
Epoch 1/15
785/785 - 118s - 151ms/step - accuracy: 0.1154 - loss: 4.6555 - val_accuracy: 0.1738 - val_loss: 4.0153 - learning_rate: 2.0479e-04
Epoch 2/15
785/785 - 74s - 94ms/step - accuracy: 0.1691 - loss: 4.2185 - val_accuracy: 0.2583 - val_loss: 3.9133 - learning_rate: 2.0479e-04
Epoch 3/15
785/785 - 74s - 95ms/step - accuracy: 0.2029 - loss: 3.9630 - val_accuracy: 0.2950 - val_loss: 3.6263 - learning_rate: 2.0479e-04
Epoch 4/15
785/785 - 67s - 86ms/step - accuracy: 0.2239 - loss: 3.8734 - val_accuracy: 0.3389 - val_loss: 3.5511 - learning_rate: 2.0479e-04
Epoch 5/15
785/785 - 62s - 79ms/step - accuracy: 0.2481 - loss: 3.7228 - val_accuracy: 0.3645 - val_loss: 3.4650 - learning_rate: 2.0479e-04
Epoch 6/15
785/785 - 62s - 79ms/step - accuracy: 0.2537 - loss: 3.6733 - val_accuracy: 0.3413 - val_loss: 3.3232 - learning_rate: 2.0479e-04
Epoch 7/15
785/785 - 83s - 105ms/step - accuracy: 0.2713 - loss: 3.5797 - val_accuracy: 0.3768 - val_loss: 3.2419 - learning_rate: 2.0479e-04
Epoch 8/15
785/785 - 62s - 79ms/step - accuracy: 0.2779 - loss: 3.5343 - val_accuracy: 0.3911 - val_loss: 3.1828 - learning_rate: 2.0479e-04
Epoch 9/15
785/785 - 62s - 79ms/step - accuracy: 0.2787 - loss: 3.5171 - val_accuracy: 0.4173 - val_loss: 3.2015 - learning_rate: 2.0479e-04
Epoch 10/15
785/785 - 79s - 100ms/step - accuracy: 0.2954 - loss: 3.4266 - val_accuracy: 0.3994 - val_loss: 3.1981 - learning_rate: 2.0479e-04
Epoch 11/15
785/785 - 66s - 85ms/step - accuracy: 0.3064 - loss: 3.4134 - val_accuracy: 0.4214 - val_loss: 3.1133 - learning_rate: 2.0479e-04
Epoch 12/15
785/785 - 68s - 86ms/step - accuracy: 0.3094 - loss: 3.3883 - val_accuracy: 0.4198 - val_loss: 3.0862 - learning_rate: 2.0479e-04
Epoch 13/15
785/785 - 68s - 86ms/step - accuracy: 0.3207 - loss: 3.3865 - val_accuracy: 0.4255 - val_loss: 2.9107 - learning_rate: 2.0479e-04
Epoch 14/15
785/785 - 65s - 82ms/step - accuracy: 0.3218 - loss: 3.3480 - val_accuracy: 0.4424 - val_loss: 2.9360 - learning_rate: 2.0479e-04
Epoch 15/15
785/785 - 66s - 84ms/step - accuracy: 0.3207 - loss: 3.3199 - val_accuracy: 0.4310 - val_loss: 2.9146 - learning_rate: 2.0479e-04
Restoring model weights from the end of the best epoch: 13.
Fold 4_2 Evaluation results: [2.9142005443573, 0.42554548382759094]
              precision    recall  f1-score   support

        1820       0.51      0.55      0.53       305
        1821       0.91      0.41      0.56       297
        1822       0.00      0.00      0.00         5
        1823       0.00      0.00      0.00         8
        1824       0.00      0.00      0.00         8
        1825       0.00      0.00      0.00        11
        1826       0.00      0.00      0.00        12
        1827       0.60      0.33      0.43       121
        1828       0.00      0.00      0.00        14
        1829       0.00      0.00      0.00        20
        1830       0.35      0.51      0.41       290
        1831       0.57      0.85      0.68       639
        1832       0.53      0.69      0.60       324
        1833       0.86      0.62      0.72       104
        1834       0.39      0.34      0.36       151
        1835       0.00      0.00      0.00         7
        1836       0.00      0.00      0.00        20
        1837       0.00      0.00      0.00        32
        1838       0.00      0.00      0.00        18
        1839       0.00      0.00      0.00         4
        1840       0.44      0.33      0.38       228
        1841       0.36      0.44      0.40       545
        1842       0.00      0.00      0.00        29
        1843       0.00      0.00      0.00        31
        1844       0.00      0.00      0.00         3
        1845       0.00      0.00      0.00         7
        1846       0.00      0.00      0.00        35
        1847       0.00      0.00      0.00        14
        1848       0.00      0.00      0.00        24
        1849       0.00      0.00      0.00        25
        1850       0.31      0.37      0.33       219
        1851       0.48      0.75      0.58       375
        1852       0.00      0.00      0.00        35
        1853       0.00      0.00      0.00        29
        1854       0.00      0.00      0.00        13
        1855       0.00      0.00      0.00       108
        1856       1.00      0.02      0.03        61
        1857       0.19      0.74      0.30       140
        1858       0.00      0.00      0.00        10
        1859       0.00      0.00      0.00        17
        1860       0.19      0.11      0.14       339
        1861       0.51      0.54      0.52       409
        1862       0.11      0.01      0.02        98
        1863       0.21      0.03      0.06        95
        1864       0.12      0.02      0.04        88
        1865       0.00      0.00      0.00        28
        1866       0.00      0.00      0.00        33
        1867       0.00      0.00      0.00        57
        1868       0.00      0.00      0.00        37
        1869       0.00      0.00      0.00        29
        1870       0.22      0.19      0.20       149
        1871       0.36      0.79      0.50       260
        1872       0.00      0.00      0.00        36
        1873       0.00      0.00      0.00        65
        1874       0.00      0.00      0.00        27
        1875       0.27      0.44      0.34        70
        1876       0.33      0.02      0.04        50
        1877       0.00      0.00      0.00        21
        1878       0.00      0.00      0.00        41
        1879       0.00      0.00      0.00         9

    accuracy                           0.43      6279
   macro avg       0.16      0.15      0.14      6279
weighted avg       0.38      0.43      0.37      6279

Matthews Correlation Coefficient: 0.394
Macro avg F1: 0.136
Weighted avg F1: 0.375
Micro avg F1: 0.426
Top-3 Accuracy: 0.686
Top-5 Accuracy: 0.771
Micro ROC AUC  = 0.95
Macro ROC AUC (present classes) = 0.91
Classification MAE (in years): 6.12

Fold 4_2 Misclassification Analysis:
Near misses (within 2 years): 599 out of 3607 misclassifications (16.61%)
MAE with outliers: 6.12
MAE without outliers: 4.41 (improvement: 1.71)

5 Worst misclassifications:
Image: data/datasets/public/1820/1820_050met.jpg, True: 1820, Predicted: 1875, Error: 55
Image: data/datasets/public/1870/1874_029met.jpg, True: 1874, Predicted: 1820, Error: 54
Image: data/datasets/private/1820/1820_109etsy.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/private/1820/1820_032_Zrzut ekranu 2022-07-26 195754.png, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_22wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_12wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_041met.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_198_001wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_157wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51
Image: data/datasets/public/1820/1820_6_001wikimedia2.jpg, True: 1820, Predicted: 1871, Error: 51

===== 5x2 Cross-Validation Results =====
Base model average accuracy: 0.4706
Alternative model average accuracy: 0.4457
Mean difference: 0.0235

Specialized 5x2cv t-test (Dietterich 1998):
t-statistic: 2.9213
p-value: 0.0330
Statistically significant difference: Yes (alpha=0.05)

Standard paired t-test:
t-statistic: 6.4493
p-value: 0.0001
Statistically significant difference: Yes (alpha=0.05)

=== Total running time: 7 hours, 51 minutes, 43 seconds ===

