Using model: NASNetMobile.
TensorFlow Version: 2.19.0
Num GPUs Available: 1
Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')

=== Fold 0 ===
model_name: NASNetMobile
input_shape: (224, 224, 3)
Epoch 1/100
707/707 - 179s - 253ms/step - loss: 0.0593 - mae: 0.2007 - mse: 0.0593 - val_loss: 0.0520 - val_mae: 0.1894 - val_mse: 0.0520
Epoch 2/100
707/707 - 118s - 166ms/step - loss: 0.0448 - mae: 0.1699 - mse: 0.0448 - val_loss: 0.0441 - val_mae: 0.1706 - val_mse: 0.0441
Epoch 3/100
707/707 - 125s - 176ms/step - loss: 0.0405 - mae: 0.1597 - mse: 0.0405 - val_loss: 0.0400 - val_mae: 0.1591 - val_mse: 0.0400
Epoch 4/100
707/707 - 111s - 158ms/step - loss: 0.0377 - mae: 0.1521 - mse: 0.0377 - val_loss: 0.0377 - val_mae: 0.1535 - val_mse: 0.0377
Epoch 5/100
707/707 - 111s - 157ms/step - loss: 0.0358 - mae: 0.1469 - mse: 0.0358 - val_loss: 0.0365 - val_mae: 0.1513 - val_mse: 0.0365
Epoch 6/100
707/707 - 110s - 156ms/step - loss: 0.0352 - mae: 0.1463 - mse: 0.0352 - val_loss: 0.0352 - val_mae: 0.1459 - val_mse: 0.0352
Epoch 7/100
707/707 - 120s - 169ms/step - loss: 0.0341 - mae: 0.1429 - mse: 0.0341 - val_loss: 0.0342 - val_mae: 0.1446 - val_mse: 0.0342
Epoch 8/100
707/707 - 120s - 169ms/step - loss: 0.0335 - mae: 0.1417 - mse: 0.0335 - val_loss: 0.0330 - val_mae: 0.1413 - val_mse: 0.0330
Epoch 9/100
707/707 - 119s - 168ms/step - loss: 0.0317 - mae: 0.1377 - mse: 0.0317 - val_loss: 0.0328 - val_mae: 0.1416 - val_mse: 0.0328
Epoch 10/100
707/707 - 121s - 171ms/step - loss: 0.0311 - mae: 0.1365 - mse: 0.0311 - val_loss: 0.0319 - val_mae: 0.1388 - val_mse: 0.0319
Epoch 11/100
707/707 - 121s - 172ms/step - loss: 0.0306 - mae: 0.1349 - mse: 0.0306 - val_loss: 0.0314 - val_mae: 0.1373 - val_mse: 0.0314
Epoch 12/100
707/707 - 123s - 174ms/step - loss: 0.0302 - mae: 0.1337 - mse: 0.0302 - val_loss: 0.0310 - val_mae: 0.1366 - val_mse: 0.0310
Epoch 13/100
707/707 - 121s - 171ms/step - loss: 0.0303 - mae: 0.1346 - mse: 0.0303 - val_loss: 0.0323 - val_mae: 0.1384 - val_mse: 0.0323
Epoch 14/100
707/707 - 121s - 171ms/step - loss: 0.0292 - mae: 0.1318 - mse: 0.0292 - val_loss: 0.0309 - val_mae: 0.1358 - val_mse: 0.0309
Epoch 15/100
707/707 - 121s - 172ms/step - loss: 0.0296 - mae: 0.1325 - mse: 0.0296 - val_loss: 0.0305 - val_mae: 0.1353 - val_mse: 0.0305
Epoch 16/100
707/707 - 120s - 170ms/step - loss: 0.0288 - mae: 0.1307 - mse: 0.0288 - val_loss: 0.0320 - val_mae: 0.1373 - val_mse: 0.0320
Epoch 17/100
707/707 - 121s - 171ms/step - loss: 0.0292 - mae: 0.1313 - mse: 0.0292 - val_loss: 0.0305 - val_mae: 0.1357 - val_mse: 0.0305
Epoch 18/100
707/707 - 121s - 171ms/step - loss: 0.0291 - mae: 0.1319 - mse: 0.0291 - val_loss: 0.0299 - val_mae: 0.1335 - val_mse: 0.0299
Epoch 19/100
707/707 - 120s - 170ms/step - loss: 0.0286 - mae: 0.1298 - mse: 0.0286 - val_loss: 0.0313 - val_mae: 0.1361 - val_mse: 0.0313
Epoch 20/100
707/707 - 121s - 172ms/step - loss: 0.0288 - mae: 0.1312 - mse: 0.0288 - val_loss: 0.0295 - val_mae: 0.1326 - val_mse: 0.0295
Epoch 21/100
707/707 - 121s - 172ms/step - loss: 0.0282 - mae: 0.1291 - mse: 0.0282 - val_loss: 0.0295 - val_mae: 0.1321 - val_mse: 0.0295
Epoch 22/100
707/707 - 120s - 169ms/step - loss: 0.0283 - mae: 0.1292 - mse: 0.0283 - val_loss: 0.0295 - val_mae: 0.1329 - val_mse: 0.0295
Epoch 23/100
707/707 - 120s - 170ms/step - loss: 0.0282 - mae: 0.1290 - mse: 0.0282 - val_loss: 0.0296 - val_mae: 0.1334 - val_mse: 0.0296
Epoch 24/100
707/707 - 128s - 181ms/step - loss: 0.0287 - mae: 0.1298 - mse: 0.0287 - val_loss: 0.0289 - val_mae: 0.1313 - val_mse: 0.0289
Epoch 25/100
707/707 - 130s - 183ms/step - loss: 0.0277 - mae: 0.1277 - mse: 0.0277 - val_loss: 0.0289 - val_mae: 0.1311 - val_mse: 0.0289
Epoch 26/100
707/707 - 128s - 182ms/step - loss: 0.0272 - mae: 0.1268 - mse: 0.0272 - val_loss: 0.0293 - val_mae: 0.1321 - val_mse: 0.0293
Epoch 27/100
707/707 - 125s - 176ms/step - loss: 0.0274 - mae: 0.1276 - mse: 0.0274 - val_loss: 0.0294 - val_mae: 0.1317 - val_mse: 0.0294
Epoch 28/100
707/707 - 125s - 177ms/step - loss: 0.0280 - mae: 0.1279 - mse: 0.0280 - val_loss: 0.0293 - val_mae: 0.1320 - val_mse: 0.0293
Epoch 29/100
707/707 - 127s - 179ms/step - loss: 0.0273 - mae: 0.1270 - mse: 0.0273 - val_loss: 0.0289 - val_mae: 0.1306 - val_mse: 0.0289
Epoch 30/100
707/707 - 128s - 181ms/step - loss: 0.0271 - mae: 0.1258 - mse: 0.0271 - val_loss: 0.0301 - val_mae: 0.1342 - val_mse: 0.0301
Epoch 31/100
707/707 - 127s - 179ms/step - loss: 0.0271 - mae: 0.1266 - mse: 0.0271 - val_loss: 0.0291 - val_mae: 0.1309 - val_mse: 0.0291
Epoch 32/100
707/707 - 128s - 181ms/step - loss: 0.0274 - mae: 0.1273 - mse: 0.0274 - val_loss: 0.0291 - val_mae: 0.1317 - val_mse: 0.0291
Epoch 33/100
707/707 - 128s - 181ms/step - loss: 0.0269 - mae: 0.1261 - mse: 0.0269 - val_loss: 0.0291 - val_mae: 0.1318 - val_mse: 0.0291
Epoch 33: early stopping
Restoring model weights from the end of the best epoch: 29.
79/79 - 12s - 146ms/step - loss: 0.0289 - mae: 0.1306 - mse: 0.0289
Fold 0 Evaluation results: [0.028916025534272194, 0.13055245578289032, 0.028916025534272194]
Fold 0 Exactly correct year predictions: 24 out of 1256
Fold 0 Final MAE (rounded to years): 16.66

=== Fold 1 ===
model_name: NASNetMobile
input_shape: (224, 224, 3)
Epoch 1/100
707/707 - 173s - 244ms/step - loss: 0.0724 - mae: 0.2239 - mse: 0.0724 - val_loss: 0.0612 - val_mae: 0.2039 - val_mse: 0.0612
Epoch 2/100
707/707 - 126s - 178ms/step - loss: 0.0486 - mae: 0.1779 - mse: 0.0486 - val_loss: 0.0508 - val_mae: 0.1827 - val_mse: 0.0508
Epoch 3/100
707/707 - 129s - 182ms/step - loss: 0.0428 - mae: 0.1643 - mse: 0.0428 - val_loss: 0.0459 - val_mae: 0.1737 - val_mse: 0.0459
Epoch 4/100
707/707 - 130s - 184ms/step - loss: 0.0396 - mae: 0.1570 - mse: 0.0396 - val_loss: 0.0420 - val_mae: 0.1641 - val_mse: 0.0420
Epoch 5/100
707/707 - 129s - 183ms/step - loss: 0.0372 - mae: 0.1513 - mse: 0.0372 - val_loss: 0.0407 - val_mae: 0.1612 - val_mse: 0.0407
Epoch 6/100
707/707 - 127s - 179ms/step - loss: 0.0352 - mae: 0.1464 - mse: 0.0352 - val_loss: 0.0393 - val_mae: 0.1568 - val_mse: 0.0393
Epoch 7/100
707/707 - 127s - 180ms/step - loss: 0.0342 - mae: 0.1441 - mse: 0.0342 - val_loss: 0.0382 - val_mae: 0.1551 - val_mse: 0.0382
Epoch 8/100
707/707 - 130s - 183ms/step - loss: 0.0332 - mae: 0.1413 - mse: 0.0332 - val_loss: 0.0374 - val_mae: 0.1526 - val_mse: 0.0374
Epoch 9/100
707/707 - 128s - 181ms/step - loss: 0.0325 - mae: 0.1403 - mse: 0.0325 - val_loss: 0.0364 - val_mae: 0.1489 - val_mse: 0.0364
Epoch 10/100
707/707 - 129s - 183ms/step - loss: 0.0319 - mae: 0.1377 - mse: 0.0319 - val_loss: 0.0364 - val_mae: 0.1496 - val_mse: 0.0364
Epoch 11/100
707/707 - 126s - 179ms/step - loss: 0.0312 - mae: 0.1366 - mse: 0.0312 - val_loss: 0.0363 - val_mae: 0.1490 - val_mse: 0.0363
Epoch 12/100
707/707 - 129s - 182ms/step - loss: 0.0306 - mae: 0.1354 - mse: 0.0306 - val_loss: 0.0359 - val_mae: 0.1473 - val_mse: 0.0359
Epoch 13/100
707/707 - 129s - 183ms/step - loss: 0.0302 - mae: 0.1340 - mse: 0.0302 - val_loss: 0.0346 - val_mae: 0.1444 - val_mse: 0.0346
Epoch 14/100
707/707 - 127s - 179ms/step - loss: 0.0296 - mae: 0.1324 - mse: 0.0296 - val_loss: 0.0347 - val_mae: 0.1446 - val_mse: 0.0347
Epoch 15/100
707/707 - 127s - 180ms/step - loss: 0.0294 - mae: 0.1325 - mse: 0.0294 - val_loss: 0.0343 - val_mae: 0.1432 - val_mse: 0.0343
Epoch 16/100
707/707 - 127s - 180ms/step - loss: 0.0290 - mae: 0.1312 - mse: 0.0290 - val_loss: 0.0340 - val_mae: 0.1427 - val_mse: 0.0340
Epoch 17/100
707/707 - 128s - 181ms/step - loss: 0.0292 - mae: 0.1316 - mse: 0.0292 - val_loss: 0.0337 - val_mae: 0.1417 - val_mse: 0.0337
Epoch 18/100
707/707 - 128s - 181ms/step - loss: 0.0287 - mae: 0.1308 - mse: 0.0287 - val_loss: 0.0347 - val_mae: 0.1441 - val_mse: 0.0347
Epoch 19/100
707/707 - 128s - 182ms/step - loss: 0.0291 - mae: 0.1310 - mse: 0.0291 - val_loss: 0.0330 - val_mae: 0.1404 - val_mse: 0.0330
Epoch 20/100
707/707 - 128s - 181ms/step - loss: 0.0284 - mae: 0.1295 - mse: 0.0284 - val_loss: 0.0328 - val_mae: 0.1393 - val_mse: 0.0328
Epoch 21/100
707/707 - 126s - 179ms/step - loss: 0.0284 - mae: 0.1296 - mse: 0.0284 - val_loss: 0.0332 - val_mae: 0.1406 - val_mse: 0.0332
Epoch 22/100
707/707 - 128s - 182ms/step - loss: 0.0281 - mae: 0.1288 - mse: 0.0281 - val_loss: 0.0334 - val_mae: 0.1418 - val_mse: 0.0334
Epoch 23/100
707/707 - 128s - 181ms/step - loss: 0.0279 - mae: 0.1285 - mse: 0.0279 - val_loss: 0.0336 - val_mae: 0.1420 - val_mse: 0.0336
Epoch 24/100
707/707 - 127s - 180ms/step - loss: 0.0280 - mae: 0.1285 - mse: 0.0280 - val_loss: 0.0329 - val_mae: 0.1404 - val_mse: 0.0329
Epoch 24: early stopping
Restoring model weights from the end of the best epoch: 20.
79/79 - 12s - 152ms/step - loss: 0.0328 - mae: 0.1393 - mse: 0.0328
Fold 1 Evaluation results: [0.03275202587246895, 0.13933053612709045, 0.03275202587246895]
Fold 1 Exactly correct year predictions: 31 out of 1256
Fold 1 Final MAE (rounded to years): 16.78

=== Fold 2 ===
model_name: NASNetMobile
input_shape: (224, 224, 3)
Epoch 1/100
707/707 - 172s - 243ms/step - loss: 0.0706 - mae: 0.2183 - mse: 0.0706 - val_loss: 0.0515 - val_mae: 0.1844 - val_mse: 0.0515
Epoch 2/100
707/707 - 127s - 180ms/step - loss: 0.0495 - mae: 0.1791 - mse: 0.0495 - val_loss: 0.0454 - val_mae: 0.1719 - val_mse: 0.0454
Epoch 3/100
707/707 - 128s - 181ms/step - loss: 0.0417 - mae: 0.1616 - mse: 0.0417 - val_loss: 0.0415 - val_mae: 0.1623 - val_mse: 0.0415
Epoch 4/100
707/707 - 130s - 184ms/step - loss: 0.0386 - mae: 0.1547 - mse: 0.0386 - val_loss: 0.0388 - val_mae: 0.1567 - val_mse: 0.0388
Epoch 5/100
707/707 - 126s - 179ms/step - loss: 0.0358 - mae: 0.1478 - mse: 0.0358 - val_loss: 0.0373 - val_mae: 0.1533 - val_mse: 0.0373
Epoch 6/100
707/707 - 130s - 183ms/step - loss: 0.0345 - mae: 0.1448 - mse: 0.0345 - val_loss: 0.0374 - val_mae: 0.1534 - val_mse: 0.0374
Epoch 7/100
707/707 - 127s - 180ms/step - loss: 0.0337 - mae: 0.1425 - mse: 0.0337 - val_loss: 0.0364 - val_mae: 0.1510 - val_mse: 0.0364
Epoch 8/100
707/707 - 128s - 181ms/step - loss: 0.0318 - mae: 0.1381 - mse: 0.0318 - val_loss: 0.0355 - val_mae: 0.1487 - val_mse: 0.0355
Epoch 9/100
707/707 - 129s - 182ms/step - loss: 0.0320 - mae: 0.1383 - mse: 0.0320 - val_loss: 0.0351 - val_mae: 0.1473 - val_mse: 0.0351
Epoch 10/100
707/707 - 128s - 181ms/step - loss: 0.0311 - mae: 0.1368 - mse: 0.0311 - val_loss: 0.0338 - val_mae: 0.1442 - val_mse: 0.0338
Epoch 11/100
707/707 - 128s - 181ms/step - loss: 0.0311 - mae: 0.1367 - mse: 0.0311 - val_loss: 0.0340 - val_mae: 0.1445 - val_mse: 0.0340
Epoch 12/100
707/707 - 128s - 181ms/step - loss: 0.0305 - mae: 0.1341 - mse: 0.0305 - val_loss: 0.0344 - val_mae: 0.1458 - val_mse: 0.0344
Epoch 13/100
707/707 - 128s - 181ms/step - loss: 0.0294 - mae: 0.1319 - mse: 0.0294 - val_loss: 0.0332 - val_mae: 0.1423 - val_mse: 0.0332
Epoch 14/100
707/707 - 128s - 181ms/step - loss: 0.0303 - mae: 0.1343 - mse: 0.0303 - val_loss: 0.0331 - val_mae: 0.1423 - val_mse: 0.0331
Epoch 15/100
707/707 - 128s - 181ms/step - loss: 0.0297 - mae: 0.1327 - mse: 0.0297 - val_loss: 0.0328 - val_mae: 0.1415 - val_mse: 0.0328
Epoch 16/100
707/707 - 128s - 181ms/step - loss: 0.0297 - mae: 0.1328 - mse: 0.0297 - val_loss: 0.0327 - val_mae: 0.1410 - val_mse: 0.0327
Epoch 17/100
707/707 - 126s - 179ms/step - loss: 0.0289 - mae: 0.1309 - mse: 0.0289 - val_loss: 0.0329 - val_mae: 0.1413 - val_mse: 0.0329
Epoch 18/100
707/707 - 126s - 179ms/step - loss: 0.0285 - mae: 0.1299 - mse: 0.0285 - val_loss: 0.0332 - val_mae: 0.1420 - val_mse: 0.0332
Epoch 19/100
707/707 - 126s - 178ms/step - loss: 0.0289 - mae: 0.1305 - mse: 0.0289 - val_loss: 0.0326 - val_mae: 0.1407 - val_mse: 0.0326
Epoch 20/100
707/707 - 126s - 179ms/step - loss: 0.0288 - mae: 0.1304 - mse: 0.0288 - val_loss: 0.0320 - val_mae: 0.1391 - val_mse: 0.0320
Epoch 21/100
707/707 - 128s - 180ms/step - loss: 0.0287 - mae: 0.1296 - mse: 0.0287 - val_loss: 0.0319 - val_mae: 0.1388 - val_mse: 0.0319
Epoch 22/100
707/707 - 125s - 177ms/step - loss: 0.0276 - mae: 0.1281 - mse: 0.0276 - val_loss: 0.0318 - val_mae: 0.1385 - val_mse: 0.0318
Epoch 23/100
707/707 - 124s - 175ms/step - loss: 0.0281 - mae: 0.1289 - mse: 0.0281 - val_loss: 0.0316 - val_mae: 0.1380 - val_mse: 0.0316
Epoch 24/100
707/707 - 128s - 181ms/step - loss: 0.0281 - mae: 0.1289 - mse: 0.0281 - val_loss: 0.0324 - val_mae: 0.1407 - val_mse: 0.0324
Epoch 25/100
707/707 - 127s - 180ms/step - loss: 0.0271 - mae: 0.1267 - mse: 0.0271 - val_loss: 0.0322 - val_mae: 0.1402 - val_mse: 0.0322
Epoch 26/100
707/707 - 127s - 180ms/step - loss: 0.0276 - mae: 0.1274 - mse: 0.0276 - val_loss: 0.0317 - val_mae: 0.1386 - val_mse: 0.0317
Epoch 27/100
707/707 - 124s - 176ms/step - loss: 0.0279 - mae: 0.1287 - mse: 0.0279 - val_loss: 0.0317 - val_mae: 0.1382 - val_mse: 0.0317
Epoch 27: early stopping
Restoring model weights from the end of the best epoch: 23.
79/79 - 13s - 159ms/step - loss: 0.0316 - mae: 0.1380 - mse: 0.0316
Fold 2 Evaluation results: [0.03157830238342285, 0.13800078630447388, 0.03157830238342285]
Fold 2 Exactly correct year predictions: 23 out of 1256
Fold 2 Final MAE (rounded to years): 17.32

=== Fold 3 ===
model_name: NASNetMobile
input_shape: (224, 224, 3)
Epoch 1/100
707/707 - 194s - 275ms/step - loss: 0.0634 - mae: 0.2073 - mse: 0.0634 - val_loss: 0.0497 - val_mae: 0.1835 - val_mse: 0.0497
Epoch 2/100
707/707 - 125s - 176ms/step - loss: 0.0469 - mae: 0.1740 - mse: 0.0469 - val_loss: 0.0421 - val_mae: 0.1679 - val_mse: 0.0421
Epoch 3/100
707/707 - 125s - 177ms/step - loss: 0.0405 - mae: 0.1592 - mse: 0.0405 - val_loss: 0.0392 - val_mae: 0.1603 - val_mse: 0.0392
Epoch 4/100
707/707 - 129s - 183ms/step - loss: 0.0383 - mae: 0.1543 - mse: 0.0383 - val_loss: 0.0370 - val_mae: 0.1547 - val_mse: 0.0370
Epoch 5/100
707/707 - 126s - 178ms/step - loss: 0.0367 - mae: 0.1494 - mse: 0.0367 - val_loss: 0.0365 - val_mae: 0.1509 - val_mse: 0.0365
Epoch 6/100
707/707 - 127s - 179ms/step - loss: 0.0348 - mae: 0.1459 - mse: 0.0348 - val_loss: 0.0347 - val_mae: 0.1478 - val_mse: 0.0347
Epoch 7/100
707/707 - 126s - 179ms/step - loss: 0.0330 - mae: 0.1412 - mse: 0.0330 - val_loss: 0.0340 - val_mae: 0.1458 - val_mse: 0.0340
Epoch 8/100
707/707 - 128s - 181ms/step - loss: 0.0322 - mae: 0.1390 - mse: 0.0322 - val_loss: 0.0336 - val_mae: 0.1443 - val_mse: 0.0336
Epoch 9/100
707/707 - 130s - 183ms/step - loss: 0.0323 - mae: 0.1391 - mse: 0.0323 - val_loss: 0.0334 - val_mae: 0.1436 - val_mse: 0.0334
Epoch 10/100
707/707 - 130s - 184ms/step - loss: 0.0310 - mae: 0.1365 - mse: 0.0310 - val_loss: 0.0331 - val_mae: 0.1426 - val_mse: 0.0331
Epoch 11/100
707/707 - 129s - 183ms/step - loss: 0.0306 - mae: 0.1351 - mse: 0.0306 - val_loss: 0.0319 - val_mae: 0.1407 - val_mse: 0.0319
Epoch 12/100
707/707 - 127s - 179ms/step - loss: 0.0308 - mae: 0.1355 - mse: 0.0308 - val_loss: 0.0314 - val_mae: 0.1389 - val_mse: 0.0314
Epoch 13/100
707/707 - 128s - 181ms/step - loss: 0.0296 - mae: 0.1332 - mse: 0.0296 - val_loss: 0.0326 - val_mae: 0.1423 - val_mse: 0.0326
Epoch 14/100
707/707 - 126s - 179ms/step - loss: 0.0292 - mae: 0.1316 - mse: 0.0292 - val_loss: 0.0325 - val_mae: 0.1406 - val_mse: 0.0325
Epoch 15/100
707/707 - 128s - 181ms/step - loss: 0.0297 - mae: 0.1329 - mse: 0.0297 - val_loss: 0.0318 - val_mae: 0.1396 - val_mse: 0.0318
Epoch 16/100
707/707 - 130s - 184ms/step - loss: 0.0293 - mae: 0.1324 - mse: 0.0293 - val_loss: 0.0312 - val_mae: 0.1381 - val_mse: 0.0312
Epoch 17/100
707/707 - 126s - 179ms/step - loss: 0.0288 - mae: 0.1310 - mse: 0.0288 - val_loss: 0.0320 - val_mae: 0.1402 - val_mse: 0.0320
Epoch 18/100
707/707 - 128s - 181ms/step - loss: 0.0291 - mae: 0.1315 - mse: 0.0291 - val_loss: 0.0324 - val_mae: 0.1410 - val_mse: 0.0324
Epoch 19/100
707/707 - 125s - 177ms/step - loss: 0.0291 - mae: 0.1318 - mse: 0.0291 - val_loss: 0.0312 - val_mae: 0.1375 - val_mse: 0.0312
Epoch 20/100
707/707 - 127s - 179ms/step - loss: 0.0281 - mae: 0.1290 - mse: 0.0281 - val_loss: 0.0313 - val_mae: 0.1384 - val_mse: 0.0313
Epoch 21/100
707/707 - 127s - 180ms/step - loss: 0.0282 - mae: 0.1294 - mse: 0.0282 - val_loss: 0.0310 - val_mae: 0.1371 - val_mse: 0.0310
Epoch 22/100
707/707 - 127s - 179ms/step - loss: 0.0286 - mae: 0.1300 - mse: 0.0286 - val_loss: 0.0310 - val_mae: 0.1370 - val_mse: 0.0310
Epoch 23/100
707/707 - 126s - 179ms/step - loss: 0.0280 - mae: 0.1284 - mse: 0.0280 - val_loss: 0.0299 - val_mae: 0.1341 - val_mse: 0.0299
Epoch 24/100
707/707 - 128s - 181ms/step - loss: 0.0276 - mae: 0.1273 - mse: 0.0276 - val_loss: 0.0307 - val_mae: 0.1354 - val_mse: 0.0307
Epoch 25/100
707/707 - 124s - 175ms/step - loss: 0.0285 - mae: 0.1304 - mse: 0.0285 - val_loss: 0.0297 - val_mae: 0.1338 - val_mse: 0.0297
Epoch 26/100
707/707 - 127s - 179ms/step - loss: 0.0281 - mae: 0.1287 - mse: 0.0281 - val_loss: 0.0302 - val_mae: 0.1347 - val_mse: 0.0302
Epoch 27/100
707/707 - 126s - 178ms/step - loss: 0.0277 - mae: 0.1281 - mse: 0.0277 - val_loss: 0.0305 - val_mae: 0.1352 - val_mse: 0.0305
Epoch 28/100
707/707 - 122s - 173ms/step - loss: 0.0275 - mae: 0.1271 - mse: 0.0275 - val_loss: 0.0304 - val_mae: 0.1356 - val_mse: 0.0304
Epoch 29/100
707/707 - 123s - 173ms/step - loss: 0.0276 - mae: 0.1277 - mse: 0.0276 - val_loss: 0.0303 - val_mae: 0.1356 - val_mse: 0.0303
Epoch 29: early stopping
Restoring model weights from the end of the best epoch: 25.
79/79 - 13s - 166ms/step - loss: 0.0297 - mae: 0.1338 - mse: 0.0297
Fold 3 Evaluation results: [0.029695916920900345, 0.133767768740654, 0.029695916920900345]
Fold 3 Exactly correct year predictions: 26 out of 1256
Fold 3 Final MAE (rounded to years): 17.24

=== Fold 4 ===
model_name: NASNetMobile
input_shape: (224, 224, 3)
Epoch 1/100
707/707 - 198s - 280ms/step - loss: 0.0671 - mae: 0.2145 - mse: 0.0671 - val_loss: 0.0547 - val_mae: 0.1915 - val_mse: 0.0547
Epoch 2/100
707/707 - 124s - 175ms/step - loss: 0.0461 - mae: 0.1724 - mse: 0.0461 - val_loss: 0.0474 - val_mae: 0.1769 - val_mse: 0.0474
Epoch 3/100
707/707 - 126s - 178ms/step - loss: 0.0413 - mae: 0.1607 - mse: 0.0413 - val_loss: 0.0455 - val_mae: 0.1716 - val_mse: 0.0455
Epoch 4/100
707/707 - 127s - 180ms/step - loss: 0.0375 - mae: 0.1518 - mse: 0.0375 - val_loss: 0.0419 - val_mae: 0.1644 - val_mse: 0.0419
Epoch 5/100
707/707 - 123s - 175ms/step - loss: 0.0362 - mae: 0.1485 - mse: 0.0362 - val_loss: 0.0396 - val_mae: 0.1585 - val_mse: 0.0396
Epoch 6/100
707/707 - 126s - 178ms/step - loss: 0.0333 - mae: 0.1416 - mse: 0.0333 - val_loss: 0.0392 - val_mae: 0.1573 - val_mse: 0.0392
Epoch 7/100
707/707 - 126s - 179ms/step - loss: 0.0337 - mae: 0.1421 - mse: 0.0337 - val_loss: 0.0379 - val_mae: 0.1539 - val_mse: 0.0379
Epoch 8/100
707/707 - 127s - 180ms/step - loss: 0.0325 - mae: 0.1398 - mse: 0.0325 - val_loss: 0.0374 - val_mae: 0.1521 - val_mse: 0.0374
Epoch 9/100
707/707 - 128s - 181ms/step - loss: 0.0312 - mae: 0.1363 - mse: 0.0312 - val_loss: 0.0359 - val_mae: 0.1484 - val_mse: 0.0359
Epoch 10/100
707/707 - 128s - 181ms/step - loss: 0.0312 - mae: 0.1356 - mse: 0.0312 - val_loss: 0.0356 - val_mae: 0.1476 - val_mse: 0.0356
Epoch 11/100
707/707 - 127s - 180ms/step - loss: 0.0307 - mae: 0.1354 - mse: 0.0307 - val_loss: 0.0359 - val_mae: 0.1487 - val_mse: 0.0359
Epoch 12/100
707/707 - 128s - 181ms/step - loss: 0.0299 - mae: 0.1336 - mse: 0.0299 - val_loss: 0.0353 - val_mae: 0.1465 - val_mse: 0.0353
Epoch 13/100
707/707 - 126s - 178ms/step - loss: 0.0299 - mae: 0.1331 - mse: 0.0299 - val_loss: 0.0341 - val_mae: 0.1443 - val_mse: 0.0341
Epoch 14/100
707/707 - 129s - 182ms/step - loss: 0.0293 - mae: 0.1308 - mse: 0.0293 - val_loss: 0.0353 - val_mae: 0.1465 - val_mse: 0.0353
Epoch 15/100
707/707 - 128s - 181ms/step - loss: 0.0290 - mae: 0.1313 - mse: 0.0290 - val_loss: 0.0340 - val_mae: 0.1433 - val_mse: 0.0340
Epoch 16/100
707/707 - 128s - 182ms/step - loss: 0.0293 - mae: 0.1314 - mse: 0.0293 - val_loss: 0.0335 - val_mae: 0.1426 - val_mse: 0.0335
Epoch 17/100
707/707 - 128s - 181ms/step - loss: 0.0294 - mae: 0.1324 - mse: 0.0294 - val_loss: 0.0339 - val_mae: 0.1431 - val_mse: 0.0339
Epoch 18/100
707/707 - 128s - 181ms/step - loss: 0.0286 - mae: 0.1301 - mse: 0.0286 - val_loss: 0.0329 - val_mae: 0.1408 - val_mse: 0.0329
Epoch 19/100
707/707 - 128s - 181ms/step - loss: 0.0283 - mae: 0.1295 - mse: 0.0283 - val_loss: 0.0327 - val_mae: 0.1402 - val_mse: 0.0327
Epoch 20/100
707/707 - 126s - 179ms/step - loss: 0.0280 - mae: 0.1281 - mse: 0.0280 - val_loss: 0.0327 - val_mae: 0.1405 - val_mse: 0.0327
Epoch 21/100
707/707 - 128s - 180ms/step - loss: 0.0281 - mae: 0.1286 - mse: 0.0281 - val_loss: 0.0326 - val_mae: 0.1401 - val_mse: 0.0326
Epoch 22/100
707/707 - 126s - 178ms/step - loss: 0.0275 - mae: 0.1274 - mse: 0.0275 - val_loss: 0.0334 - val_mae: 0.1423 - val_mse: 0.0334
Epoch 23/100
707/707 - 128s - 181ms/step - loss: 0.0273 - mae: 0.1265 - mse: 0.0273 - val_loss: 0.0324 - val_mae: 0.1398 - val_mse: 0.0324
Epoch 24/100
707/707 - 126s - 178ms/step - loss: 0.0280 - mae: 0.1282 - mse: 0.0280 - val_loss: 0.0325 - val_mae: 0.1403 - val_mse: 0.0325
Epoch 25/100
707/707 - 130s - 184ms/step - loss: 0.0276 - mae: 0.1276 - mse: 0.0276 - val_loss: 0.0331 - val_mae: 0.1417 - val_mse: 0.0331
Epoch 26/100
707/707 - 125s - 177ms/step - loss: 0.0273 - mae: 0.1269 - mse: 0.0273 - val_loss: 0.0321 - val_mae: 0.1391 - val_mse: 0.0321
Epoch 27/100
707/707 - 127s - 180ms/step - loss: 0.0277 - mae: 0.1274 - mse: 0.0277 - val_loss: 0.0317 - val_mae: 0.1382 - val_mse: 0.0317
Epoch 28/100
707/707 - 125s - 177ms/step - loss: 0.0274 - mae: 0.1262 - mse: 0.0274 - val_loss: 0.0320 - val_mae: 0.1390 - val_mse: 0.0320
Epoch 29/100
707/707 - 126s - 179ms/step - loss: 0.0270 - mae: 0.1262 - mse: 0.0270 - val_loss: 0.0322 - val_mae: 0.1393 - val_mse: 0.0322
Epoch 30/100
707/707 - 129s - 183ms/step - loss: 0.0275 - mae: 0.1273 - mse: 0.0275 - val_loss: 0.0318 - val_mae: 0.1379 - val_mse: 0.0318
Epoch 31/100
707/707 - 124s - 176ms/step - loss: 0.0274 - mae: 0.1272 - mse: 0.0274 - val_loss: 0.0319 - val_mae: 0.1380 - val_mse: 0.0319
Epoch 31: early stopping
Restoring model weights from the end of the best epoch: 27.
79/79 - 11s - 139ms/step - loss: 0.0317 - mae: 0.1382 - mse: 0.0317
Fold 4 Evaluation results: [0.03174157813191414, 0.13817812502384186, 0.03174157813191414]
Fold 4 Exactly correct year predictions: 28 out of 1256
Fold 4 Final MAE (rounded to years): 16.75

=== Fold 5 ===
model_name: NASNetMobile
input_shape: (224, 224, 3)
Epoch 1/100
707/707 - 185s - 262ms/step - loss: 0.0649 - mae: 0.2119 - mse: 0.0649 - val_loss: 0.0520 - val_mae: 0.1892 - val_mse: 0.0520
Epoch 2/100
707/707 - 126s - 179ms/step - loss: 0.0484 - mae: 0.1774 - mse: 0.0484 - val_loss: 0.0442 - val_mae: 0.1723 - val_mse: 0.0442
Epoch 3/100
707/707 - 123s - 175ms/step - loss: 0.0418 - mae: 0.1635 - mse: 0.0418 - val_loss: 0.0400 - val_mae: 0.1621 - val_mse: 0.0400
Epoch 4/100
707/707 - 120s - 170ms/step - loss: 0.0385 - mae: 0.1541 - mse: 0.0385 - val_loss: 0.0383 - val_mae: 0.1577 - val_mse: 0.0383
Epoch 5/100
707/707 - 137s - 194ms/step - loss: 0.0361 - mae: 0.1492 - mse: 0.0361 - val_loss: 0.0362 - val_mae: 0.1518 - val_mse: 0.0362
Epoch 6/100
707/707 - 132s - 187ms/step - loss: 0.0344 - mae: 0.1448 - mse: 0.0344 - val_loss: 0.0345 - val_mae: 0.1475 - val_mse: 0.0345
Epoch 7/100
707/707 - 129s - 183ms/step - loss: 0.0338 - mae: 0.1425 - mse: 0.0338 - val_loss: 0.0354 - val_mae: 0.1486 - val_mse: 0.0354
Epoch 8/100
707/707 - 133s - 188ms/step - loss: 0.0326 - mae: 0.1391 - mse: 0.0326 - val_loss: 0.0335 - val_mae: 0.1443 - val_mse: 0.0335
Epoch 9/100
707/707 - 129s - 183ms/step - loss: 0.0320 - mae: 0.1385 - mse: 0.0320 - val_loss: 0.0336 - val_mae: 0.1446 - val_mse: 0.0336
Epoch 10/100
707/707 - 143s - 202ms/step - loss: 0.0320 - mae: 0.1388 - mse: 0.0320 - val_loss: 0.0331 - val_mae: 0.1436 - val_mse: 0.0331
Epoch 11/100
707/707 - 134s - 190ms/step - loss: 0.0308 - mae: 0.1358 - mse: 0.0308 - val_loss: 0.0324 - val_mae: 0.1415 - val_mse: 0.0324
Epoch 12/100
707/707 - 131s - 186ms/step - loss: 0.0306 - mae: 0.1355 - mse: 0.0306 - val_loss: 0.0327 - val_mae: 0.1427 - val_mse: 0.0327
Epoch 13/100
707/707 - 129s - 182ms/step - loss: 0.0300 - mae: 0.1331 - mse: 0.0300 - val_loss: 0.0325 - val_mae: 0.1411 - val_mse: 0.0325
Epoch 14/100
707/707 - 125s - 177ms/step - loss: 0.0301 - mae: 0.1339 - mse: 0.0301 - val_loss: 0.0321 - val_mae: 0.1410 - val_mse: 0.0321
Epoch 15/100
707/707 - 126s - 178ms/step - loss: 0.0292 - mae: 0.1321 - mse: 0.0292 - val_loss: 0.0312 - val_mae: 0.1383 - val_mse: 0.0312
Epoch 16/100
707/707 - 126s - 178ms/step - loss: 0.0294 - mae: 0.1317 - mse: 0.0294 - val_loss: 0.0308 - val_mae: 0.1373 - val_mse: 0.0308
Epoch 17/100
707/707 - 128s - 181ms/step - loss: 0.0293 - mae: 0.1311 - mse: 0.0293 - val_loss: 0.0315 - val_mae: 0.1387 - val_mse: 0.0315
Epoch 18/100
707/707 - 125s - 176ms/step - loss: 0.0286 - mae: 0.1302 - mse: 0.0286 - val_loss: 0.0307 - val_mae: 0.1372 - val_mse: 0.0307
Epoch 19/100
707/707 - 127s - 180ms/step - loss: 0.0286 - mae: 0.1301 - mse: 0.0286 - val_loss: 0.0305 - val_mae: 0.1365 - val_mse: 0.0305
Epoch 20/100
707/707 - 122s - 172ms/step - loss: 0.0284 - mae: 0.1292 - mse: 0.0284 - val_loss: 0.0304 - val_mae: 0.1363 - val_mse: 0.0304
Epoch 21/100
707/707 - 128s - 181ms/step - loss: 0.0287 - mae: 0.1296 - mse: 0.0287 - val_loss: 0.0302 - val_mae: 0.1355 - val_mse: 0.0302
Epoch 22/100
707/707 - 127s - 180ms/step - loss: 0.0288 - mae: 0.1302 - mse: 0.0288 - val_loss: 0.0319 - val_mae: 0.1388 - val_mse: 0.0319
Epoch 23/100
707/707 - 125s - 177ms/step - loss: 0.0279 - mae: 0.1281 - mse: 0.0279 - val_loss: 0.0297 - val_mae: 0.1341 - val_mse: 0.0297
Epoch 24/100
707/707 - 125s - 176ms/step - loss: 0.0278 - mae: 0.1285 - mse: 0.0278 - val_loss: 0.0299 - val_mae: 0.1348 - val_mse: 0.0299
Epoch 25/100
707/707 - 122s - 173ms/step - loss: 0.0277 - mae: 0.1283 - mse: 0.0277 - val_loss: 0.0295 - val_mae: 0.1335 - val_mse: 0.0295
Epoch 26/100
707/707 - 126s - 178ms/step - loss: 0.0283 - mae: 0.1291 - mse: 0.0283 - val_loss: 0.0297 - val_mae: 0.1340 - val_mse: 0.0297
Epoch 27/100
707/707 - 123s - 174ms/step - loss: 0.0281 - mae: 0.1284 - mse: 0.0281 - val_loss: 0.0295 - val_mae: 0.1333 - val_mse: 0.0295
Epoch 28/100
707/707 - 124s - 175ms/step - loss: 0.0278 - mae: 0.1273 - mse: 0.0278 - val_loss: 0.0300 - val_mae: 0.1351 - val_mse: 0.0300
Epoch 29/100
707/707 - 125s - 177ms/step - loss: 0.0278 - mae: 0.1276 - mse: 0.0278 - val_loss: 0.0297 - val_mae: 0.1345 - val_mse: 0.0297
Epoch 29: early stopping
Restoring model weights from the end of the best epoch: 25.
79/79 - 12s - 148ms/step - loss: 0.0295 - mae: 0.1335 - mse: 0.0295
Fold 5 Evaluation results: [0.02946598269045353, 0.1335083693265915, 0.02946598269045353]
Fold 5 Exactly correct year predictions: 19 out of 1256
Fold 5 Final MAE (rounded to years): 16.79

=== Fold 6 ===
model_name: NASNetMobile
input_shape: (224, 224, 3)
Epoch 1/100
707/707 - 173s - 245ms/step - loss: 0.0677 - mae: 0.2156 - mse: 0.0677 - val_loss: 0.0559 - val_mae: 0.1969 - val_mse: 0.0559
Epoch 2/100
707/707 - 126s - 178ms/step - loss: 0.0481 - mae: 0.1765 - mse: 0.0481 - val_loss: 0.0475 - val_mae: 0.1785 - val_mse: 0.0475
Epoch 3/100
707/707 - 126s - 178ms/step - loss: 0.0421 - mae: 0.1631 - mse: 0.0421 - val_loss: 0.0429 - val_mae: 0.1668 - val_mse: 0.0429
Epoch 4/100
707/707 - 126s - 178ms/step - loss: 0.0388 - mae: 0.1544 - mse: 0.0388 - val_loss: 0.0422 - val_mae: 0.1644 - val_mse: 0.0422
Epoch 5/100
707/707 - 127s - 180ms/step - loss: 0.0364 - mae: 0.1480 - mse: 0.0364 - val_loss: 0.0402 - val_mae: 0.1593 - val_mse: 0.0402
Epoch 6/100
707/707 - 124s - 176ms/step - loss: 0.0358 - mae: 0.1472 - mse: 0.0358 - val_loss: 0.0393 - val_mae: 0.1567 - val_mse: 0.0393
Epoch 7/100
707/707 - 127s - 179ms/step - loss: 0.0337 - mae: 0.1430 - mse: 0.0337 - val_loss: 0.0393 - val_mae: 0.1560 - val_mse: 0.0393
Epoch 8/100
707/707 - 130s - 184ms/step - loss: 0.0336 - mae: 0.1414 - mse: 0.0336 - val_loss: 0.0385 - val_mae: 0.1546 - val_mse: 0.0385
Epoch 9/100
707/707 - 124s - 175ms/step - loss: 0.0329 - mae: 0.1407 - mse: 0.0329 - val_loss: 0.0367 - val_mae: 0.1502 - val_mse: 0.0367
Epoch 10/100
707/707 - 128s - 181ms/step - loss: 0.0319 - mae: 0.1382 - mse: 0.0319 - val_loss: 0.0369 - val_mae: 0.1506 - val_mse: 0.0369
Epoch 11/100
707/707 - 127s - 179ms/step - loss: 0.0312 - mae: 0.1369 - mse: 0.0312 - val_loss: 0.0357 - val_mae: 0.1475 - val_mse: 0.0357
Epoch 12/100
707/707 - 126s - 178ms/step - loss: 0.0307 - mae: 0.1351 - mse: 0.0307 - val_loss: 0.0357 - val_mae: 0.1483 - val_mse: 0.0357
Epoch 13/100
707/707 - 126s - 179ms/step - loss: 0.0303 - mae: 0.1335 - mse: 0.0303 - val_loss: 0.0356 - val_mae: 0.1472 - val_mse: 0.0356
Epoch 14/100
707/707 - 122s - 173ms/step - loss: 0.0305 - mae: 0.1348 - mse: 0.0305 - val_loss: 0.0351 - val_mae: 0.1447 - val_mse: 0.0351
Epoch 15/100
707/707 - 128s - 180ms/step - loss: 0.0298 - mae: 0.1329 - mse: 0.0298 - val_loss: 0.0347 - val_mae: 0.1449 - val_mse: 0.0347
Epoch 16/100
707/707 - 126s - 178ms/step - loss: 0.0296 - mae: 0.1326 - mse: 0.0296 - val_loss: 0.0346 - val_mae: 0.1451 - val_mse: 0.0346
Epoch 17/100
707/707 - 126s - 178ms/step - loss: 0.0295 - mae: 0.1324 - mse: 0.0295 - val_loss: 0.0342 - val_mae: 0.1437 - val_mse: 0.0342
Epoch 18/100
707/707 - 126s - 179ms/step - loss: 0.0296 - mae: 0.1322 - mse: 0.0296 - val_loss: 0.0354 - val_mae: 0.1443 - val_mse: 0.0354
Epoch 19/100
707/707 - 124s - 176ms/step - loss: 0.0288 - mae: 0.1303 - mse: 0.0288 - val_loss: 0.0349 - val_mae: 0.1447 - val_mse: 0.0349
Epoch 20/100
707/707 - 124s - 175ms/step - loss: 0.0285 - mae: 0.1302 - mse: 0.0285 - val_loss: 0.0336 - val_mae: 0.1423 - val_mse: 0.0336
Epoch 21/100
707/707 - 125s - 177ms/step - loss: 0.0282 - mae: 0.1296 - mse: 0.0282 - val_loss: 0.0341 - val_mae: 0.1424 - val_mse: 0.0341
Epoch 22/100
707/707 - 126s - 178ms/step - loss: 0.0282 - mae: 0.1290 - mse: 0.0282 - val_loss: 0.0333 - val_mae: 0.1407 - val_mse: 0.0333
Epoch 23/100
707/707 - 122s - 172ms/step - loss: 0.0279 - mae: 0.1282 - mse: 0.0279 - val_loss: 0.0334 - val_mae: 0.1418 - val_mse: 0.0334
Epoch 24/100
707/707 - 125s - 177ms/step - loss: 0.0278 - mae: 0.1281 - mse: 0.0278 - val_loss: 0.0333 - val_mae: 0.1421 - val_mse: 0.0333
Epoch 25/100
707/707 - 124s - 176ms/step - loss: 0.0279 - mae: 0.1284 - mse: 0.0279 - val_loss: 0.0329 - val_mae: 0.1400 - val_mse: 0.0329
Epoch 26/100
707/707 - 125s - 177ms/step - loss: 0.0276 - mae: 0.1275 - mse: 0.0276 - val_loss: 0.0331 - val_mae: 0.1399 - val_mse: 0.0331
Epoch 27/100
707/707 - 124s - 175ms/step - loss: 0.0278 - mae: 0.1283 - mse: 0.0278 - val_loss: 0.0330 - val_mae: 0.1399 - val_mse: 0.0330
Epoch 28/100
707/707 - 125s - 177ms/step - loss: 0.0277 - mae: 0.1278 - mse: 0.0277 - val_loss: 0.0329 - val_mae: 0.1390 - val_mse: 0.0329
Epoch 29/100
707/707 - 126s - 178ms/step - loss: 0.0272 - mae: 0.1269 - mse: 0.0272 - val_loss: 0.0326 - val_mae: 0.1388 - val_mse: 0.0326
Epoch 30/100
707/707 - 125s - 178ms/step - loss: 0.0273 - mae: 0.1273 - mse: 0.0273 - val_loss: 0.0324 - val_mae: 0.1394 - val_mse: 0.0324
Epoch 31/100
707/707 - 125s - 176ms/step - loss: 0.0275 - mae: 0.1277 - mse: 0.0275 - val_loss: 0.0328 - val_mae: 0.1398 - val_mse: 0.0328
Epoch 32/100
707/707 - 128s - 181ms/step - loss: 0.0279 - mae: 0.1275 - mse: 0.0279 - val_loss: 0.0327 - val_mae: 0.1383 - val_mse: 0.0327
Epoch 33/100
707/707 - 124s - 176ms/step - loss: 0.0278 - mae: 0.1271 - mse: 0.0278 - val_loss: 0.0325 - val_mae: 0.1394 - val_mse: 0.0325
Epoch 34/100
707/707 - 126s - 178ms/step - loss: 0.0268 - mae: 0.1251 - mse: 0.0268 - val_loss: 0.0320 - val_mae: 0.1373 - val_mse: 0.0320
Epoch 35/100
707/707 - 124s - 176ms/step - loss: 0.0265 - mae: 0.1246 - mse: 0.0265 - val_loss: 0.0323 - val_mae: 0.1373 - val_mse: 0.0323
Epoch 36/100
707/707 - 125s - 177ms/step - loss: 0.0269 - mae: 0.1257 - mse: 0.0269 - val_loss: 0.0316 - val_mae: 0.1359 - val_mse: 0.0316
Epoch 37/100
707/707 - 126s - 178ms/step - loss: 0.0268 - mae: 0.1258 - mse: 0.0268 - val_loss: 0.0315 - val_mae: 0.1372 - val_mse: 0.0315
Epoch 38/100
707/707 - 127s - 179ms/step - loss: 0.0272 - mae: 0.1261 - mse: 0.0272 - val_loss: 0.0313 - val_mae: 0.1358 - val_mse: 0.0313
Epoch 39/100
707/707 - 126s - 178ms/step - loss: 0.0266 - mae: 0.1248 - mse: 0.0266 - val_loss: 0.0319 - val_mae: 0.1391 - val_mse: 0.0319
Epoch 40/100
707/707 - 125s - 177ms/step - loss: 0.0267 - mae: 0.1258 - mse: 0.0267 - val_loss: 0.0318 - val_mae: 0.1376 - val_mse: 0.0318
Epoch 41/100
707/707 - 128s - 181ms/step - loss: 0.0264 - mae: 0.1246 - mse: 0.0264 - val_loss: 0.0317 - val_mae: 0.1375 - val_mse: 0.0317
Epoch 42/100
707/707 - 124s - 176ms/step - loss: 0.0272 - mae: 0.1263 - mse: 0.0272 - val_loss: 0.0316 - val_mae: 0.1368 - val_mse: 0.0316
Epoch 42: early stopping
Restoring model weights from the end of the best epoch: 38.
79/79 - 11s - 145ms/step - loss: 0.0313 - mae: 0.1358 - mse: 0.0313
Fold 6 Evaluation results: [0.03125676512718201, 0.13575786352157593, 0.03125676512718201]
Fold 6 Exactly correct year predictions: 24 out of 1256
Fold 6 Final MAE (rounded to years): 17.54

=== Fold 7 ===
model_name: NASNetMobile
input_shape: (224, 224, 3)
Epoch 1/100
707/707 - 168s - 237ms/step - loss: 0.0658 - mae: 0.2149 - mse: 0.0658 - val_loss: 0.0537 - val_mae: 0.1931 - val_mse: 0.0537
Epoch 2/100
707/707 - 128s - 181ms/step - loss: 0.0471 - mae: 0.1753 - mse: 0.0471 - val_loss: 0.0452 - val_mae: 0.1744 - val_mse: 0.0452
Epoch 3/100
707/707 - 126s - 179ms/step - loss: 0.0414 - mae: 0.1613 - mse: 0.0414 - val_loss: 0.0406 - val_mae: 0.1625 - val_mse: 0.0406
Epoch 4/100
707/707 - 123s - 175ms/step - loss: 0.0385 - mae: 0.1535 - mse: 0.0385 - val_loss: 0.0390 - val_mae: 0.1579 - val_mse: 0.0390
Epoch 5/100
707/707 - 124s - 176ms/step - loss: 0.0358 - mae: 0.1480 - mse: 0.0358 - val_loss: 0.0382 - val_mae: 0.1558 - val_mse: 0.0382
Epoch 6/100
707/707 - 126s - 178ms/step - loss: 0.0348 - mae: 0.1455 - mse: 0.0348 - val_loss: 0.0370 - val_mae: 0.1523 - val_mse: 0.0370
Epoch 7/100
707/707 - 123s - 175ms/step - loss: 0.0335 - mae: 0.1428 - mse: 0.0335 - val_loss: 0.0368 - val_mae: 0.1514 - val_mse: 0.0368
Epoch 8/100
707/707 - 124s - 175ms/step - loss: 0.0329 - mae: 0.1404 - mse: 0.0329 - val_loss: 0.0361 - val_mae: 0.1493 - val_mse: 0.0361
Epoch 9/100
707/707 - 124s - 176ms/step - loss: 0.0315 - mae: 0.1377 - mse: 0.0315 - val_loss: 0.0362 - val_mae: 0.1495 - val_mse: 0.0362
Epoch 10/100
707/707 - 124s - 175ms/step - loss: 0.0308 - mae: 0.1360 - mse: 0.0308 - val_loss: 0.0348 - val_mae: 0.1460 - val_mse: 0.0348
Epoch 11/100
707/707 - 122s - 173ms/step - loss: 0.0299 - mae: 0.1335 - mse: 0.0299 - val_loss: 0.0352 - val_mae: 0.1462 - val_mse: 0.0352
Epoch 12/100
707/707 - 125s - 177ms/step - loss: 0.0306 - mae: 0.1353 - mse: 0.0306 - val_loss: 0.0339 - val_mae: 0.1440 - val_mse: 0.0339
Epoch 13/100
707/707 - 123s - 174ms/step - loss: 0.0301 - mae: 0.1337 - mse: 0.0301 - val_loss: 0.0333 - val_mae: 0.1430 - val_mse: 0.0333
Epoch 14/100
707/707 - 123s - 174ms/step - loss: 0.0294 - mae: 0.1316 - mse: 0.0294 - val_loss: 0.0333 - val_mae: 0.1426 - val_mse: 0.0333
Epoch 15/100
707/707 - 124s - 176ms/step - loss: 0.0293 - mae: 0.1320 - mse: 0.0293 - val_loss: 0.0351 - val_mae: 0.1459 - val_mse: 0.0351
Epoch 16/100
707/707 - 124s - 176ms/step - loss: 0.0293 - mae: 0.1313 - mse: 0.0293 - val_loss: 0.0327 - val_mae: 0.1412 - val_mse: 0.0327
Epoch 17/100
707/707 - 125s - 177ms/step - loss: 0.0287 - mae: 0.1305 - mse: 0.0287 - val_loss: 0.0324 - val_mae: 0.1406 - val_mse: 0.0324
Epoch 18/100
707/707 - 123s - 173ms/step - loss: 0.0285 - mae: 0.1302 - mse: 0.0285 - val_loss: 0.0325 - val_mae: 0.1408 - val_mse: 0.0325
Epoch 19/100
707/707 - 124s - 176ms/step - loss: 0.0286 - mae: 0.1306 - mse: 0.0286 - val_loss: 0.0321 - val_mae: 0.1397 - val_mse: 0.0321
Epoch 20/100
707/707 - 127s - 180ms/step - loss: 0.0280 - mae: 0.1288 - mse: 0.0280 - val_loss: 0.0320 - val_mae: 0.1390 - val_mse: 0.0320
Epoch 21/100
707/707 - 123s - 174ms/step - loss: 0.0283 - mae: 0.1289 - mse: 0.0283 - val_loss: 0.0321 - val_mae: 0.1396 - val_mse: 0.0321
Epoch 22/100
707/707 - 122s - 172ms/step - loss: 0.0290 - mae: 0.1312 - mse: 0.0290 - val_loss: 0.0320 - val_mae: 0.1392 - val_mse: 0.0320
Epoch 23/100
707/707 - 123s - 174ms/step - loss: 0.0286 - mae: 0.1300 - mse: 0.0286 - val_loss: 0.0323 - val_mae: 0.1398 - val_mse: 0.0323
Epoch 24/100
707/707 - 125s - 177ms/step - loss: 0.0276 - mae: 0.1286 - mse: 0.0276 - val_loss: 0.0316 - val_mae: 0.1387 - val_mse: 0.0316
Epoch 25/100
707/707 - 124s - 175ms/step - loss: 0.0283 - mae: 0.1296 - mse: 0.0283 - val_loss: 0.0320 - val_mae: 0.1391 - val_mse: 0.0320
Epoch 26/100
707/707 - 124s - 176ms/step - loss: 0.0275 - mae: 0.1272 - mse: 0.0275 - val_loss: 0.0322 - val_mae: 0.1395 - val_mse: 0.0322
Epoch 27/100
707/707 - 125s - 177ms/step - loss: 0.0274 - mae: 0.1273 - mse: 0.0274 - val_loss: 0.0320 - val_mae: 0.1393 - val_mse: 0.0320
Epoch 28/100
707/707 - 125s - 177ms/step - loss: 0.0276 - mae: 0.1277 - mse: 0.0276 - val_loss: 0.0324 - val_mae: 0.1395 - val_mse: 0.0324
Epoch 28: early stopping
Restoring model weights from the end of the best epoch: 24.
79/79 - 12s - 150ms/step - loss: 0.0316 - mae: 0.1387 - mse: 0.0316
Fold 7 Evaluation results: [0.03163624554872513, 0.13865616917610168, 0.03163624554872513]
Fold 7 Exactly correct year predictions: 33 out of 1256
Fold 7 Final MAE (rounded to years): 16.53

=== Fold 8 ===
model_name: NASNetMobile
input_shape: (224, 224, 3)
Epoch 1/100
707/707 - 170s - 241ms/step - loss: 0.0715 - mae: 0.2188 - mse: 0.0715 - val_loss: 0.0634 - val_mae: 0.2033 - val_mse: 0.0634
Epoch 2/100
707/707 - 124s - 176ms/step - loss: 0.0484 - mae: 0.1770 - mse: 0.0484 - val_loss: 0.0529 - val_mae: 0.1843 - val_mse: 0.0529
Epoch 3/100
707/707 - 120s - 170ms/step - loss: 0.0418 - mae: 0.1618 - mse: 0.0418 - val_loss: 0.0484 - val_mae: 0.1747 - val_mse: 0.0484
Epoch 4/100
707/707 - 124s - 175ms/step - loss: 0.0387 - mae: 0.1542 - mse: 0.0387 - val_loss: 0.0454 - val_mae: 0.1696 - val_mse: 0.0454
Epoch 5/100
707/707 - 123s - 174ms/step - loss: 0.0361 - mae: 0.1481 - mse: 0.0361 - val_loss: 0.0469 - val_mae: 0.1716 - val_mse: 0.0469
Epoch 6/100
707/707 - 123s - 173ms/step - loss: 0.0352 - mae: 0.1458 - mse: 0.0352 - val_loss: 0.0400 - val_mae: 0.1578 - val_mse: 0.0400
Epoch 7/100
707/707 - 123s - 174ms/step - loss: 0.0336 - mae: 0.1426 - mse: 0.0336 - val_loss: 0.0392 - val_mae: 0.1562 - val_mse: 0.0392
Epoch 8/100
707/707 - 123s - 175ms/step - loss: 0.0330 - mae: 0.1412 - mse: 0.0330 - val_loss: 0.0381 - val_mae: 0.1542 - val_mse: 0.0381
Epoch 9/100
707/707 - 124s - 176ms/step - loss: 0.0325 - mae: 0.1393 - mse: 0.0325 - val_loss: 0.0380 - val_mae: 0.1535 - val_mse: 0.0380
Epoch 10/100
707/707 - 126s - 178ms/step - loss: 0.0309 - mae: 0.1363 - mse: 0.0309 - val_loss: 0.0368 - val_mae: 0.1513 - val_mse: 0.0368
Epoch 11/100
707/707 - 124s - 175ms/step - loss: 0.0309 - mae: 0.1353 - mse: 0.0309 - val_loss: 0.0375 - val_mae: 0.1523 - val_mse: 0.0375
Epoch 12/100
707/707 - 125s - 176ms/step - loss: 0.0307 - mae: 0.1349 - mse: 0.0307 - val_loss: 0.0368 - val_mae: 0.1505 - val_mse: 0.0368
Epoch 13/100
707/707 - 125s - 177ms/step - loss: 0.0301 - mae: 0.1333 - mse: 0.0301 - val_loss: 0.0376 - val_mae: 0.1519 - val_mse: 0.0376
Epoch 14/100
707/707 - 123s - 173ms/step - loss: 0.0297 - mae: 0.1327 - mse: 0.0297 - val_loss: 0.0354 - val_mae: 0.1475 - val_mse: 0.0354
Epoch 15/100
707/707 - 124s - 175ms/step - loss: 0.0296 - mae: 0.1324 - mse: 0.0296 - val_loss: 0.0350 - val_mae: 0.1466 - val_mse: 0.0350
Epoch 16/100
707/707 - 125s - 177ms/step - loss: 0.0294 - mae: 0.1319 - mse: 0.0294 - val_loss: 0.0349 - val_mae: 0.1463 - val_mse: 0.0349
Epoch 17/100
707/707 - 122s - 173ms/step - loss: 0.0294 - mae: 0.1318 - mse: 0.0294 - val_loss: 0.0343 - val_mae: 0.1453 - val_mse: 0.0343
Epoch 18/100
707/707 - 118s - 167ms/step - loss: 0.0293 - mae: 0.1320 - mse: 0.0293 - val_loss: 0.0343 - val_mae: 0.1455 - val_mse: 0.0343
Epoch 19/100
707/707 - 127s - 179ms/step - loss: 0.0289 - mae: 0.1307 - mse: 0.0289 - val_loss: 0.0350 - val_mae: 0.1473 - val_mse: 0.0350
Epoch 20/100
707/707 - 125s - 177ms/step - loss: 0.0283 - mae: 0.1290 - mse: 0.0283 - val_loss: 0.0348 - val_mae: 0.1461 - val_mse: 0.0348
Epoch 21/100
707/707 - 122s - 172ms/step - loss: 0.0287 - mae: 0.1302 - mse: 0.0287 - val_loss: 0.0343 - val_mae: 0.1455 - val_mse: 0.0343
Epoch 22/100
707/707 - 123s - 174ms/step - loss: 0.0277 - mae: 0.1286 - mse: 0.0277 - val_loss: 0.0347 - val_mae: 0.1457 - val_mse: 0.0347
Epoch 23/100
707/707 - 127s - 180ms/step - loss: 0.0280 - mae: 0.1284 - mse: 0.0280 - val_loss: 0.0337 - val_mae: 0.1440 - val_mse: 0.0337
Epoch 24/100
707/707 - 126s - 179ms/step - loss: 0.0282 - mae: 0.1290 - mse: 0.0282 - val_loss: 0.0335 - val_mae: 0.1435 - val_mse: 0.0335
Epoch 25/100
707/707 - 127s - 180ms/step - loss: 0.0289 - mae: 0.1302 - mse: 0.0289 - val_loss: 0.0342 - val_mae: 0.1448 - val_mse: 0.0342
Epoch 26/100
707/707 - 125s - 176ms/step - loss: 0.0282 - mae: 0.1283 - mse: 0.0282 - val_loss: 0.0338 - val_mae: 0.1436 - val_mse: 0.0338
Epoch 27/100
707/707 - 124s - 175ms/step - loss: 0.0277 - mae: 0.1285 - mse: 0.0277 - val_loss: 0.0342 - val_mae: 0.1444 - val_mse: 0.0342
Epoch 28/100
707/707 - 114s - 161ms/step - loss: 0.0277 - mae: 0.1278 - mse: 0.0277 - val_loss: 0.0340 - val_mae: 0.1439 - val_mse: 0.0340
Epoch 28: early stopping
Restoring model weights from the end of the best epoch: 24.
79/79 - 11s - 146ms/step - loss: 0.0335 - mae: 0.1435 - mse: 0.0335
Fold 8 Evaluation results: [0.03353239595890045, 0.1435132920742035, 0.03353239595890045]
Fold 8 Exactly correct year predictions: 25 out of 1256
Fold 8 Final MAE (rounded to years): 16.78

=== Fold 9 ===
model_name: NASNetMobile
input_shape: (224, 224, 3)
Epoch 1/100
707/707 - 159s - 225ms/step - loss: 0.0663 - mae: 0.2156 - mse: 0.0663 - val_loss: 0.0613 - val_mae: 0.2068 - val_mse: 0.0613
Epoch 2/100
707/707 - 112s - 159ms/step - loss: 0.0475 - mae: 0.1763 - mse: 0.0475 - val_loss: 0.0535 - val_mae: 0.1911 - val_mse: 0.0535
Epoch 3/100
707/707 - 113s - 160ms/step - loss: 0.0410 - mae: 0.1604 - mse: 0.0410 - val_loss: 0.0468 - val_mae: 0.1751 - val_mse: 0.0468
Epoch 4/100
707/707 - 115s - 162ms/step - loss: 0.0391 - mae: 0.1559 - mse: 0.0391 - val_loss: 0.0457 - val_mae: 0.1732 - val_mse: 0.0457
Epoch 5/100
707/707 - 115s - 162ms/step - loss: 0.0360 - mae: 0.1483 - mse: 0.0360 - val_loss: 0.0417 - val_mae: 0.1633 - val_mse: 0.0417
Epoch 6/100
707/707 - 115s - 162ms/step - loss: 0.0342 - mae: 0.1440 - mse: 0.0342 - val_loss: 0.0409 - val_mae: 0.1604 - val_mse: 0.0409
Epoch 7/100
707/707 - 118s - 166ms/step - loss: 0.0339 - mae: 0.1426 - mse: 0.0339 - val_loss: 0.0409 - val_mae: 0.1621 - val_mse: 0.0409
Epoch 8/100
707/707 - 114s - 161ms/step - loss: 0.0325 - mae: 0.1390 - mse: 0.0325 - val_loss: 0.0392 - val_mae: 0.1573 - val_mse: 0.0392
Epoch 9/100
707/707 - 113s - 160ms/step - loss: 0.0312 - mae: 0.1369 - mse: 0.0312 - val_loss: 0.0386 - val_mae: 0.1553 - val_mse: 0.0386
Epoch 10/100
707/707 - 116s - 164ms/step - loss: 0.0315 - mae: 0.1369 - mse: 0.0315 - val_loss: 0.0380 - val_mae: 0.1539 - val_mse: 0.0380
Epoch 11/100
707/707 - 117s - 165ms/step - loss: 0.0306 - mae: 0.1344 - mse: 0.0306 - val_loss: 0.0380 - val_mae: 0.1536 - val_mse: 0.0380
Epoch 12/100
707/707 - 116s - 165ms/step - loss: 0.0307 - mae: 0.1356 - mse: 0.0307 - val_loss: 0.0367 - val_mae: 0.1504 - val_mse: 0.0367
Epoch 13/100
707/707 - 122s - 173ms/step - loss: 0.0299 - mae: 0.1336 - mse: 0.0299 - val_loss: 0.0364 - val_mae: 0.1496 - val_mse: 0.0364
Epoch 14/100
707/707 - 113s - 160ms/step - loss: 0.0288 - mae: 0.1308 - mse: 0.0288 - val_loss: 0.0363 - val_mae: 0.1493 - val_mse: 0.0363
Epoch 15/100
707/707 - 113s - 159ms/step - loss: 0.0302 - mae: 0.1346 - mse: 0.0302 - val_loss: 0.0357 - val_mae: 0.1474 - val_mse: 0.0357
Epoch 16/100
707/707 - 114s - 161ms/step - loss: 0.0287 - mae: 0.1303 - mse: 0.0287 - val_loss: 0.0356 - val_mae: 0.1476 - val_mse: 0.0356
Epoch 17/100
707/707 - 112s - 159ms/step - loss: 0.0291 - mae: 0.1309 - mse: 0.0291 - val_loss: 0.0350 - val_mae: 0.1457 - val_mse: 0.0350
Epoch 18/100
707/707 - 115s - 163ms/step - loss: 0.0289 - mae: 0.1308 - mse: 0.0289 - val_loss: 0.0355 - val_mae: 0.1474 - val_mse: 0.0355
Epoch 19/100
707/707 - 113s - 160ms/step - loss: 0.0279 - mae: 0.1281 - mse: 0.0279 - val_loss: 0.0351 - val_mae: 0.1463 - val_mse: 0.0351
Epoch 20/100
707/707 - 113s - 160ms/step - loss: 0.0284 - mae: 0.1298 - mse: 0.0284 - val_loss: 0.0346 - val_mae: 0.1452 - val_mse: 0.0346
Epoch 21/100
707/707 - 110s - 155ms/step - loss: 0.0282 - mae: 0.1288 - mse: 0.0282 - val_loss: 0.0348 - val_mae: 0.1449 - val_mse: 0.0348
Epoch 22/100
707/707 - 114s - 161ms/step - loss: 0.0284 - mae: 0.1300 - mse: 0.0284 - val_loss: 0.0341 - val_mae: 0.1435 - val_mse: 0.0341
Epoch 23/100
707/707 - 113s - 160ms/step - loss: 0.0283 - mae: 0.1290 - mse: 0.0283 - val_loss: 0.0341 - val_mae: 0.1438 - val_mse: 0.0341
Epoch 24/100
707/707 - 110s - 156ms/step - loss: 0.0274 - mae: 0.1273 - mse: 0.0274 - val_loss: 0.0341 - val_mae: 0.1436 - val_mse: 0.0341
Epoch 25/100
707/707 - 112s - 159ms/step - loss: 0.0274 - mae: 0.1272 - mse: 0.0274 - val_loss: 0.0349 - val_mae: 0.1459 - val_mse: 0.0349
Epoch 26/100
707/707 - 112s - 158ms/step - loss: 0.0278 - mae: 0.1282 - mse: 0.0278 - val_loss: 0.0344 - val_mae: 0.1442 - val_mse: 0.0344
Epoch 27/100
707/707 - 112s - 159ms/step - loss: 0.0273 - mae: 0.1269 - mse: 0.0273 - val_loss: 0.0352 - val_mae: 0.1455 - val_mse: 0.0352
Epoch 28/100
707/707 - 112s - 159ms/step - loss: 0.0278 - mae: 0.1282 - mse: 0.0278 - val_loss: 0.0343 - val_mae: 0.1442 - val_mse: 0.0343
Epoch 28: early stopping
Restoring model weights from the end of the best epoch: 24.
79/79 - 10s - 128ms/step - loss: 0.0341 - mae: 0.1436 - mse: 0.0341
Fold 9 Evaluation results: [0.034099020063877106, 0.1435803472995758, 0.034099020063877106]
Fold 9 Exactly correct year predictions: 20 out of 1255
Fold 9 Final MAE (rounded to years): 17.08

Mean MAE over 10 folds: 16.95
Mean exact matches: 25.30 / 1255.90
Total running time: 10 hours, 38 minutes, 55 seconds
